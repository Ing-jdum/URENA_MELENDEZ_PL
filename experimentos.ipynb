{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExploraciÃ³n del uso de la libreria, replicando el ejemplo de la compuerta XOR pero transformandolo para aproximar una compuerta AND."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from logic_explained_networks import lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for repeatibility purposes\n",
    "lens.utils.base.set_seed(0)\n",
    "\n",
    "# create a dataset, a tensor of 100 rows and 4 columns with data uniformly between 0 and 1\n",
    "x = torch.rand([100, 4])\n",
    "\n",
    "# create target vector\n",
    "y = (x[:, 0] >= 0.5) & (x[:, 1] >= 0.5)\n",
    "\n",
    "data = torch.utils.data.TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50, Loss: 0.579, Tr_acc: 77.50, Val_acc: 60.00, best_e: -1\n",
      "Epoch: 2/50, Loss: 0.550, Tr_acc: 77.50, Val_acc: 60.00, best_e: -1\n",
      "Epoch: 3/50, Loss: 0.536, Tr_acc: 77.50, Val_acc: 60.00, best_e: -1\n",
      "Epoch: 4/50, Loss: 0.510, Tr_acc: 77.50, Val_acc: 60.00, best_e: -1\n",
      "Epoch: 5/50, Loss: 0.492, Tr_acc: 77.50, Val_acc: 60.00, best_e: -1\n",
      "Epoch: 6/50, Loss: 0.477, Tr_acc: 77.50, Val_acc: 60.00, best_e: -1\n",
      "Epoch: 7/50, Loss: 0.447, Tr_acc: 77.50, Val_acc: 60.00, best_e: -1\n",
      "Epoch: 8/50, Loss: 0.411, Tr_acc: 77.50, Val_acc: 60.00, best_e: -1\n",
      "Epoch: 9/50, Loss: 0.379, Tr_acc: 77.50, Val_acc: 70.00, best_e: -1\n",
      "Epoch: 10/50, Loss: 0.345, Tr_acc: 80.00, Val_acc: 90.00, best_e: -1\n",
      "Epoch: 11/50, Loss: 0.311, Tr_acc: 87.50, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 12/50, Loss: 0.283, Tr_acc: 96.25, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 13/50, Loss: 0.256, Tr_acc: 93.75, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 14/50, Loss: 0.230, Tr_acc: 93.75, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 15/50, Loss: 0.212, Tr_acc: 95.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 16/50, Loss: 0.195, Tr_acc: 95.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 17/50, Loss: 0.181, Tr_acc: 92.50, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 18/50, Loss: 0.172, Tr_acc: 92.50, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 19/50, Loss: 0.163, Tr_acc: 92.50, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 20/50, Loss: 0.157, Tr_acc: 92.50, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 21/50, Loss: 0.152, Tr_acc: 92.50, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 22/50, Loss: 0.147, Tr_acc: 92.50, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 23/50, Loss: 0.143, Tr_acc: 92.50, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 24/50, Loss: 0.140, Tr_acc: 92.50, Val_acc: 100.00, best_e: -1\n",
      "Pruned 2/4 features\n",
      "Pruned 2/4 features\n",
      "Pruned features\n",
      "Epoch: 25/50, Loss: 0.136, Tr_acc: 92.50, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 26/50, Loss: 0.224, Tr_acc: 92.50, Val_acc: 100.00, best_e: 26\n",
      "Epoch: 27/50, Loss: 0.152, Tr_acc: 92.50, Val_acc: 100.00, best_e: 27\n",
      "Epoch: 28/50, Loss: 0.203, Tr_acc: 90.00, Val_acc: 100.00, best_e: 28\n",
      "Epoch: 29/50, Loss: 0.178, Tr_acc: 91.25, Val_acc: 100.00, best_e: 29\n",
      "Epoch: 30/50, Loss: 0.144, Tr_acc: 93.75, Val_acc: 100.00, best_e: 30\n",
      "Epoch: 31/50, Loss: 0.149, Tr_acc: 91.25, Val_acc: 100.00, best_e: 31\n",
      "Epoch: 32/50, Loss: 0.169, Tr_acc: 93.75, Val_acc: 100.00, best_e: 32\n",
      "Epoch: 33/50, Loss: 0.166, Tr_acc: 93.75, Val_acc: 100.00, best_e: 33\n",
      "Epoch: 34/50, Loss: 0.146, Tr_acc: 92.50, Val_acc: 100.00, best_e: 34\n",
      "Epoch: 35/50, Loss: 0.136, Tr_acc: 95.00, Val_acc: 100.00, best_e: 35\n",
      "Epoch: 36/50, Loss: 0.145, Tr_acc: 92.50, Val_acc: 100.00, best_e: 36\n",
      "Epoch: 37/50, Loss: 0.155, Tr_acc: 92.50, Val_acc: 100.00, best_e: 37\n",
      "Epoch: 38/50, Loss: 0.148, Tr_acc: 92.50, Val_acc: 100.00, best_e: 38\n",
      "Epoch: 39/50, Loss: 0.137, Tr_acc: 95.00, Val_acc: 100.00, best_e: 39\n",
      "Epoch: 40/50, Loss: 0.135, Tr_acc: 93.75, Val_acc: 100.00, best_e: 40\n",
      "Epoch: 41/50, Loss: 0.141, Tr_acc: 92.50, Val_acc: 100.00, best_e: 41\n",
      "Epoch: 42/50, Loss: 0.145, Tr_acc: 91.25, Val_acc: 100.00, best_e: 42\n",
      "Epoch: 43/50, Loss: 0.141, Tr_acc: 93.75, Val_acc: 100.00, best_e: 43\n",
      "Epoch: 44/50, Loss: 0.134, Tr_acc: 92.50, Val_acc: 100.00, best_e: 44\n",
      "Epoch: 45/50, Loss: 0.133, Tr_acc: 95.00, Val_acc: 100.00, best_e: 45\n",
      "Epoch: 46/50, Loss: 0.136, Tr_acc: 93.75, Val_acc: 100.00, best_e: 46\n",
      "Epoch: 47/50, Loss: 0.138, Tr_acc: 93.75, Val_acc: 100.00, best_e: 47\n",
      "Epoch: 48/50, Loss: 0.135, Tr_acc: 93.75, Val_acc: 100.00, best_e: 48\n",
      "Epoch: 49/50, Loss: 0.131, Tr_acc: 95.00, Val_acc: 100.00, best_e: 49\n",
      "Epoch: 50/50, Loss: 0.130, Tr_acc: 92.50, Val_acc: 100.00, best_e: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdum/PycharmProjects/URENA_MELENDEZ_PL/logic_explained_networks/lens/models/base.py:348: UserWarning: Loaded model does not have time or explanations. They need to be recalculated but time will only consider rule extraction time.\n",
      "  warnings.warn(\"Loaded model does not have time or explanations. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tot losses</th>\n",
       "      <th>Train accs</th>\n",
       "      <th>Val accs</th>\n",
       "      <th>Best epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.578810</td>\n",
       "      <td>77.50</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.550012</td>\n",
       "      <td>77.50</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.536012</td>\n",
       "      <td>77.50</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.510130</td>\n",
       "      <td>77.50</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.492443</td>\n",
       "      <td>77.50</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.476678</td>\n",
       "      <td>77.50</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.446680</td>\n",
       "      <td>77.50</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.410562</td>\n",
       "      <td>77.50</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.378703</td>\n",
       "      <td>77.50</td>\n",
       "      <td>70.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.345119</td>\n",
       "      <td>80.00</td>\n",
       "      <td>90.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.310761</td>\n",
       "      <td>87.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.283375</td>\n",
       "      <td>96.25</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.256374</td>\n",
       "      <td>93.75</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.230378</td>\n",
       "      <td>93.75</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.211911</td>\n",
       "      <td>95.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.195359</td>\n",
       "      <td>95.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.181388</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.172413</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.163158</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.156698</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.151952</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.146543</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.143101</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.139642</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.136178</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.224358</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.151971</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.202669</td>\n",
       "      <td>90.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.178115</td>\n",
       "      <td>91.25</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.144417</td>\n",
       "      <td>93.75</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.148786</td>\n",
       "      <td>91.25</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.168746</td>\n",
       "      <td>93.75</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.165898</td>\n",
       "      <td>93.75</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.145949</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.136351</td>\n",
       "      <td>95.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.145309</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.154548</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.148085</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.137172</td>\n",
       "      <td>95.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.135143</td>\n",
       "      <td>93.75</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.141253</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.145215</td>\n",
       "      <td>91.25</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.141043</td>\n",
       "      <td>93.75</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.134375</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.132920</td>\n",
       "      <td>95.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.136403</td>\n",
       "      <td>93.75</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.138018</td>\n",
       "      <td>93.75</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.134892</td>\n",
       "      <td>93.75</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.130912</td>\n",
       "      <td>95.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.130022</td>\n",
       "      <td>92.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tot losses  Train accs  Val accs  Best epoch\n",
       "0     0.578810       77.50      60.0          50\n",
       "1     0.550012       77.50      60.0          50\n",
       "2     0.536012       77.50      60.0          50\n",
       "3     0.510130       77.50      60.0          50\n",
       "4     0.492443       77.50      60.0          50\n",
       "5     0.476678       77.50      60.0          50\n",
       "6     0.446680       77.50      60.0          50\n",
       "7     0.410562       77.50      60.0          50\n",
       "8     0.378703       77.50      70.0          50\n",
       "9     0.345119       80.00      90.0          50\n",
       "10    0.310761       87.50     100.0          50\n",
       "11    0.283375       96.25     100.0          50\n",
       "12    0.256374       93.75     100.0          50\n",
       "13    0.230378       93.75     100.0          50\n",
       "14    0.211911       95.00     100.0          50\n",
       "15    0.195359       95.00     100.0          50\n",
       "16    0.181388       92.50     100.0          50\n",
       "17    0.172413       92.50     100.0          50\n",
       "18    0.163158       92.50     100.0          50\n",
       "19    0.156698       92.50     100.0          50\n",
       "20    0.151952       92.50     100.0          50\n",
       "21    0.146543       92.50     100.0          50\n",
       "22    0.143101       92.50     100.0          50\n",
       "23    0.139642       92.50     100.0          50\n",
       "24    0.136178       92.50     100.0          50\n",
       "25    0.224358       92.50     100.0          50\n",
       "26    0.151971       92.50     100.0          50\n",
       "27    0.202669       90.00     100.0          50\n",
       "28    0.178115       91.25     100.0          50\n",
       "29    0.144417       93.75     100.0          50\n",
       "30    0.148786       91.25     100.0          50\n",
       "31    0.168746       93.75     100.0          50\n",
       "32    0.165898       93.75     100.0          50\n",
       "33    0.145949       92.50     100.0          50\n",
       "34    0.136351       95.00     100.0          50\n",
       "35    0.145309       92.50     100.0          50\n",
       "36    0.154548       92.50     100.0          50\n",
       "37    0.148085       92.50     100.0          50\n",
       "38    0.137172       95.00     100.0          50\n",
       "39    0.135143       93.75     100.0          50\n",
       "40    0.141253       92.50     100.0          50\n",
       "41    0.145215       91.25     100.0          50\n",
       "42    0.141043       93.75     100.0          50\n",
       "43    0.134375       92.50     100.0          50\n",
       "44    0.132920       95.00     100.0          50\n",
       "45    0.136403       93.75     100.0          50\n",
       "46    0.138018       93.75     100.0          50\n",
       "47    0.134892       93.75     100.0          50\n",
       "48    0.130912       95.00     100.0          50\n",
       "49    0.130022       92.50     100.0          50"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset splitting into train, validation and testing.\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(data, [80, 10, 10])\n",
    "x_train, y_train = data[train_data.indices]\n",
    "x_val, y_val = data[val_data.indices]\n",
    "x_test, y_test = data[test_data.indices]\n",
    "\n",
    "# model instantiation\n",
    "model = lens.models.XMuNN(n_classes=2, n_features=4,\n",
    "                          hidden_neurons=[3], loss=torch.nn.CrossEntropyLoss())\n",
    "\n",
    "# training\n",
    "model.fit(train_data, val_data, epochs=50, l_r=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 90.0\n",
      "x1 & x2 <-> f1\n",
      "Logic Test Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "# get accuracy on test samples\n",
    "test_acc = model.evaluate(test_data)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "\n",
    "# get first order 1ogic explanations for a specific target class\n",
    "target_class = 1\n",
    "concept_names = ['x1', 'x2', 'x3', 'x4']\n",
    "formula = model.get_global_explanation(x_train, y_train, target_class,\n",
    "                                       top_k_explanations=2, concept_names=concept_names)\n",
    "print(f\"{formula} <-> f{target_class}\")\n",
    "\n",
    "# compute explanation accuracy\n",
    "exp_accuracy, _ = lens.logic.test_explanation(formula, target_class, x_test, y_test,\n",
    "                                              concept_names=concept_names)\n",
    "print(\"Logic Test Accuracy:\", exp_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por curiosidad, vamos a mover el threshold de la funciÃ³n que obtiene los valores de y, para ver si los resultados son similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500, Loss: 0.728, Tr_acc: 49.71, Val_acc: 45.33, best_e: -1\n",
      "Epoch: 2/500, Loss: 0.661, Tr_acc: 50.29, Val_acc: 52.67, best_e: -1\n",
      "Epoch: 3/500, Loss: 0.458, Tr_acc: 57.86, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 4/500, Loss: 0.285, Tr_acc: 100.00, Val_acc: 99.33, best_e: -1\n",
      "Epoch: 5/500, Loss: 0.192, Tr_acc: 99.14, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 6/500, Loss: 0.098, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 7/500, Loss: 0.059, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 8/500, Loss: 0.037, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 9/500, Loss: 0.024, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 10/500, Loss: 0.018, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 11/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 12/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 13/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 14/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 15/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 16/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 17/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 18/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 19/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 20/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 21/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 22/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 23/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 24/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 25/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 26/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 27/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 28/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 29/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 30/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 31/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 32/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 33/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 34/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 35/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 36/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 37/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 38/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 39/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 40/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 41/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 42/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 43/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 44/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 45/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 46/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 47/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 48/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 49/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 50/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 51/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 52/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 53/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 54/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 55/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 56/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 57/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 58/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 59/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 60/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 61/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 62/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 63/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 64/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 65/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 66/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 67/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 68/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 69/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 70/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 71/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 72/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 73/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 74/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 75/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 76/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 77/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 78/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 79/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 80/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 81/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 82/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 83/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 84/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 85/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 86/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 87/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 88/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 89/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 90/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 91/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 92/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 93/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 94/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 95/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 96/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 97/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 98/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 99/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 100/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 101/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 102/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 103/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 104/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 105/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 106/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 107/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 108/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 109/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 110/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 111/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 112/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 113/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 114/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 115/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 116/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 117/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 118/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 119/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 120/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 121/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 122/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 123/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 124/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 125/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 126/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 127/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 128/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 129/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 130/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 131/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 132/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 133/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 134/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 135/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 136/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 137/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 138/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 139/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 140/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 141/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 142/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 143/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 144/500, Loss: 0.008, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 145/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 146/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 147/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 148/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 149/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 150/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 151/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 152/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 153/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 154/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 155/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 156/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 157/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 158/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 159/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 160/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 161/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 162/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 163/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 164/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 165/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 166/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 167/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 168/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 169/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 170/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 171/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 172/500, Loss: 0.007, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 173/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 174/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 175/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 176/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 177/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 178/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 179/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 180/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 181/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 182/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 183/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 184/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 185/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 186/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 187/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 188/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 189/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 190/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 191/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 192/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 193/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 194/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 195/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 196/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 197/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 198/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 199/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 200/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 201/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 202/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 203/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 204/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 205/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 206/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 207/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 208/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 209/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 210/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 211/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 212/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 213/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 214/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 215/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 216/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 217/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 218/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 219/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 220/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 221/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 222/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 223/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 224/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 225/500, Loss: 0.006, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 226/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 227/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 228/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 229/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 230/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 231/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 232/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 233/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 234/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 235/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 236/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 237/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 238/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 239/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 240/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 241/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 242/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 243/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 244/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 245/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 246/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 247/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 248/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 249/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Pruned 0/4 features\n",
      "Pruned 0/4 features\n",
      "Pruned features\n",
      "Epoch: 250/500, Loss: 0.005, Tr_acc: 100.00, Val_acc: 100.00, best_e: -1\n",
      "Epoch: 251/500, Loss: 0.001, Tr_acc: 100.00, Val_acc: 100.00, best_e: 251\n",
      "Epoch: 252/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 252\n",
      "Epoch: 253/500, Loss: 0.009, Tr_acc: 99.71, Val_acc: 100.00, best_e: 253\n",
      "Epoch: 254/500, Loss: 0.001, Tr_acc: 100.00, Val_acc: 98.00, best_e: 253\n",
      "Epoch: 255/500, Loss: 0.021, Tr_acc: 99.29, Val_acc: 100.00, best_e: 255\n",
      "Epoch: 256/500, Loss: 0.003, Tr_acc: 100.00, Val_acc: 100.00, best_e: 256\n",
      "Epoch: 257/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 257\n",
      "Epoch: 258/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 258\n",
      "Epoch: 259/500, Loss: 0.005, Tr_acc: 99.71, Val_acc: 100.00, best_e: 259\n",
      "Epoch: 260/500, Loss: 0.006, Tr_acc: 99.71, Val_acc: 100.00, best_e: 260\n",
      "Epoch: 261/500, Loss: 0.001, Tr_acc: 100.00, Val_acc: 100.00, best_e: 261\n",
      "Epoch: 262/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 262\n",
      "Epoch: 263/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 263\n",
      "Epoch: 264/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 264\n",
      "Epoch: 265/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 265\n",
      "Epoch: 266/500, Loss: 0.001, Tr_acc: 100.00, Val_acc: 100.00, best_e: 266\n",
      "Epoch: 267/500, Loss: 0.002, Tr_acc: 100.00, Val_acc: 100.00, best_e: 267\n",
      "Epoch: 268/500, Loss: 0.002, Tr_acc: 100.00, Val_acc: 100.00, best_e: 268\n",
      "Epoch: 269/500, Loss: 0.001, Tr_acc: 100.00, Val_acc: 100.00, best_e: 269\n",
      "Epoch: 270/500, Loss: 0.001, Tr_acc: 100.00, Val_acc: 100.00, best_e: 270\n",
      "Epoch: 271/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 271\n",
      "Epoch: 272/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 272\n",
      "Epoch: 273/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 273\n",
      "Epoch: 274/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 274\n",
      "Epoch: 275/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 275\n",
      "Epoch: 276/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 276\n",
      "Epoch: 277/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 277\n",
      "Epoch: 278/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 278\n",
      "Epoch: 279/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 279\n",
      "Epoch: 280/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 280\n",
      "Epoch: 281/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 281\n",
      "Epoch: 282/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 282\n",
      "Epoch: 283/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 283\n",
      "Epoch: 284/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 284\n",
      "Epoch: 285/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 285\n",
      "Epoch: 286/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 286\n",
      "Epoch: 287/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 287\n",
      "Epoch: 288/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 288\n",
      "Epoch: 289/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 289\n",
      "Epoch: 290/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 290\n",
      "Epoch: 291/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 291\n",
      "Epoch: 292/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 292\n",
      "Epoch: 293/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 293\n",
      "Epoch: 294/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 294\n",
      "Epoch: 295/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 295\n",
      "Epoch: 296/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 296\n",
      "Epoch: 297/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 297\n",
      "Epoch: 298/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 298\n",
      "Epoch: 299/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 299\n",
      "Epoch: 300/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 300\n",
      "Epoch: 301/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 301\n",
      "Epoch: 302/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 302\n",
      "Epoch: 303/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 303\n",
      "Epoch: 304/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 304\n",
      "Epoch: 305/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 305\n",
      "Epoch: 306/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 306\n",
      "Epoch: 307/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 307\n",
      "Epoch: 308/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 308\n",
      "Epoch: 309/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 309\n",
      "Epoch: 310/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 310\n",
      "Epoch: 311/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 311\n",
      "Epoch: 312/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 312\n",
      "Epoch: 313/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 313\n",
      "Epoch: 314/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 314\n",
      "Epoch: 315/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 315\n",
      "Epoch: 316/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 316\n",
      "Epoch: 317/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 317\n",
      "Epoch: 318/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 318\n",
      "Epoch: 319/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 319\n",
      "Epoch: 320/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 320\n",
      "Epoch: 321/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 321\n",
      "Epoch: 322/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 322\n",
      "Epoch: 323/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 323\n",
      "Epoch: 324/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 324\n",
      "Epoch: 325/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 325\n",
      "Epoch: 326/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 326\n",
      "Epoch: 327/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 327\n",
      "Epoch: 328/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 328\n",
      "Epoch: 329/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 329\n",
      "Epoch: 330/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 330\n",
      "Epoch: 331/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 331\n",
      "Epoch: 332/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 332\n",
      "Epoch: 333/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 333\n",
      "Epoch: 334/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 334\n",
      "Epoch: 335/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 335\n",
      "Epoch: 336/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 336\n",
      "Epoch: 337/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 337\n",
      "Epoch: 338/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 338\n",
      "Epoch: 339/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 339\n",
      "Epoch: 340/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 340\n",
      "Epoch: 341/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 341\n",
      "Epoch: 342/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 342\n",
      "Epoch: 343/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 343\n",
      "Epoch: 344/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 344\n",
      "Epoch: 345/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 345\n",
      "Epoch: 346/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 346\n",
      "Epoch: 347/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 347\n",
      "Epoch: 348/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 348\n",
      "Epoch: 349/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 349\n",
      "Epoch: 350/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 350\n",
      "Epoch: 351/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 351\n",
      "Epoch: 352/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 352\n",
      "Epoch: 353/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 353\n",
      "Epoch: 354/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 354\n",
      "Epoch: 355/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 355\n",
      "Epoch: 356/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 356\n",
      "Epoch: 357/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 357\n",
      "Epoch: 358/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 358\n",
      "Epoch: 359/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 359\n",
      "Epoch: 360/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 360\n",
      "Epoch: 361/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 361\n",
      "Epoch: 362/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 362\n",
      "Epoch: 363/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 363\n",
      "Epoch: 364/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 364\n",
      "Epoch: 365/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 365\n",
      "Epoch: 366/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 366\n",
      "Epoch: 367/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 367\n",
      "Epoch: 368/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 368\n",
      "Epoch: 369/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 369\n",
      "Epoch: 370/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 370\n",
      "Epoch: 371/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 371\n",
      "Epoch: 372/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 372\n",
      "Epoch: 373/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 373\n",
      "Epoch: 374/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 374\n",
      "Epoch: 375/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 375\n",
      "Epoch: 376/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 376\n",
      "Epoch: 377/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 377\n",
      "Epoch: 378/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 378\n",
      "Epoch: 379/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 379\n",
      "Epoch: 380/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 380\n",
      "Epoch: 381/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 381\n",
      "Epoch: 382/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 382\n",
      "Epoch: 383/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 383\n",
      "Epoch: 384/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 384\n",
      "Epoch: 385/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 385\n",
      "Epoch: 386/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 386\n",
      "Epoch: 387/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 387\n",
      "Epoch: 388/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 388\n",
      "Epoch: 389/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 389\n",
      "Epoch: 390/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 390\n",
      "Epoch: 391/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 391\n",
      "Epoch: 392/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 392\n",
      "Epoch: 393/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 393\n",
      "Epoch: 394/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 394\n",
      "Epoch: 395/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 395\n",
      "Epoch: 396/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 396\n",
      "Epoch: 397/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 397\n",
      "Epoch: 398/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 398\n",
      "Epoch: 399/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 399\n",
      "Epoch: 400/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 400\n",
      "Epoch: 401/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 401\n",
      "Epoch: 402/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 402\n",
      "Epoch: 403/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 403\n",
      "Epoch: 404/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 404\n",
      "Epoch: 405/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 405\n",
      "Epoch: 406/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 406\n",
      "Epoch: 407/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 407\n",
      "Epoch: 408/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 408\n",
      "Epoch: 409/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 409\n",
      "Epoch: 410/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 410\n",
      "Epoch: 411/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 411\n",
      "Epoch: 412/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 412\n",
      "Epoch: 413/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 413\n",
      "Epoch: 414/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 414\n",
      "Epoch: 415/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 415\n",
      "Epoch: 416/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 416\n",
      "Epoch: 417/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 417\n",
      "Epoch: 418/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 418\n",
      "Epoch: 419/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 419\n",
      "Epoch: 420/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 420\n",
      "Epoch: 421/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 421\n",
      "Epoch: 422/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 422\n",
      "Epoch: 423/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 423\n",
      "Epoch: 424/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 424\n",
      "Epoch: 425/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 425\n",
      "Epoch: 426/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 426\n",
      "Epoch: 427/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 427\n",
      "Epoch: 428/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 428\n",
      "Epoch: 429/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 429\n",
      "Epoch: 430/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 430\n",
      "Epoch: 431/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 431\n",
      "Epoch: 432/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 432\n",
      "Epoch: 433/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 433\n",
      "Epoch: 434/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 434\n",
      "Epoch: 435/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 435\n",
      "Epoch: 436/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 436\n",
      "Epoch: 437/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 437\n",
      "Epoch: 438/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 438\n",
      "Epoch: 439/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 439\n",
      "Epoch: 440/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 440\n",
      "Epoch: 441/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 441\n",
      "Epoch: 442/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 442\n",
      "Epoch: 443/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 443\n",
      "Epoch: 444/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 444\n",
      "Epoch: 445/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 445\n",
      "Epoch: 446/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 446\n",
      "Epoch: 447/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 447\n",
      "Epoch: 448/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 448\n",
      "Epoch: 449/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 449\n",
      "Epoch: 450/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 450\n",
      "Epoch: 451/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 451\n",
      "Epoch: 452/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 452\n",
      "Epoch: 453/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 453\n",
      "Epoch: 454/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 454\n",
      "Epoch: 455/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 455\n",
      "Epoch: 456/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 456\n",
      "Epoch: 457/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 457\n",
      "Epoch: 458/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 458\n",
      "Epoch: 459/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 459\n",
      "Epoch: 460/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 460\n",
      "Epoch: 461/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 461\n",
      "Epoch: 462/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 462\n",
      "Epoch: 463/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 463\n",
      "Epoch: 464/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 464\n",
      "Epoch: 465/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 465\n",
      "Epoch: 466/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 466\n",
      "Epoch: 467/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 467\n",
      "Epoch: 468/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 468\n",
      "Epoch: 469/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 469\n",
      "Epoch: 470/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 470\n",
      "Epoch: 471/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 471\n",
      "Epoch: 472/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 472\n",
      "Epoch: 473/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 473\n",
      "Epoch: 474/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 474\n",
      "Epoch: 475/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 475\n",
      "Epoch: 476/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 476\n",
      "Epoch: 477/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 477\n",
      "Epoch: 478/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 478\n",
      "Epoch: 479/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 479\n",
      "Epoch: 480/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 480\n",
      "Epoch: 481/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 481\n",
      "Epoch: 482/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 482\n",
      "Epoch: 483/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 483\n",
      "Epoch: 484/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 484\n",
      "Epoch: 485/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 485\n",
      "Epoch: 486/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 486\n",
      "Epoch: 487/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 487\n",
      "Epoch: 488/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 488\n",
      "Epoch: 489/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 489\n",
      "Epoch: 490/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 490\n",
      "Epoch: 491/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 491\n",
      "Epoch: 492/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 492\n",
      "Epoch: 493/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 493\n",
      "Epoch: 494/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 494\n",
      "Epoch: 495/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 495\n",
      "Epoch: 496/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 496\n",
      "Epoch: 497/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 497\n",
      "Epoch: 498/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 498\n",
      "Epoch: 499/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 499\n",
      "Epoch: 500/500, Loss: 0.000, Tr_acc: 100.00, Val_acc: 100.00, best_e: 500\n",
      "Test accuracy: 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdum/PycharmProjects/URENA_MELENDEZ_PL/logic_explained_networks/lens/models/base.py:348: UserWarning: Loaded model does not have time or explanations. They need to be recalculated but time will only consider rule extraction time.\n",
      "  warnings.warn(\"Loaded model does not have time or explanations. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 & x2 & x4 <-> f1\n",
      "Logic Test Accuracy: 62.0\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for repeatibility purposes\n",
    "lens.utils.base.set_seed(0)\n",
    "\n",
    "# Define the number of values below and above 0.3\n",
    "num_values = 1000\n",
    "num_below = num_values // 2\n",
    "num_above = num_values - num_below\n",
    "\n",
    "# Generate random values\n",
    "below_threshold = torch.rand(num_below, 4) * 0.3\n",
    "above_threshold = 0.3 + torch.rand(num_above, 4) * 0.7\n",
    "\n",
    "# Concatenate below and above threshold values\n",
    "x = torch.cat((below_threshold, above_threshold), dim=0)\n",
    "\n",
    "# create target vector\n",
    "y = (x[:, 0] >= 0.3) & (x[:, 1] >= 0.3)\n",
    "\n",
    "data = torch.utils.data.TensorDataset(x, y)\n",
    "\n",
    "# Dataset splitting into train, validation and testing.\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(data, [700, 150, 150])\n",
    "x_train, y_train = data[train_data.indices]\n",
    "x_val, y_val = data[val_data.indices]\n",
    "x_test, y_test = data[test_data.indices]\n",
    "\n",
    "# model instantiation\n",
    "model = lens.models.XMuNN(n_classes=2, n_features=4,\n",
    "                          hidden_neurons=[30], loss=torch.nn.CrossEntropyLoss())\n",
    "\n",
    "# training\n",
    "model.fit(train_data, val_data, epochs=500, l_r=0.1)\n",
    "\n",
    "# get accuracy on test samples\n",
    "test_acc = model.evaluate(test_data)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "\n",
    "# get first order 1ogic explanations for a specific target class\n",
    "target_class = 1\n",
    "concept_names = ['x1', 'x2', 'x3', 'x4']\n",
    "formula = model.get_global_explanation(x_train, y_train, target_class,\n",
    "                                       top_k_explanations=2, concept_names=concept_names)\n",
    "print(f\"{formula} <-> f{target_class}\")\n",
    "\n",
    "# compute explanation accuracy\n",
    "exp_accuracy, _ = lens.logic.test_explanation(formula, target_class, x_test, y_test,\n",
    "                                              concept_names=concept_names)\n",
    "print(\"Logic Test Accuracy:\", exp_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperarse, la explicaciÃ³n es mucho peor. AquÃ­ es donde viene de utilidad el conocimiento del funcionamiento de las LENs. Sabemos que para el entrenamiento se construye una tabla de verdad empÃ­rica $T^i$ booleanizando los valores de los atributos utilizando un umbral, en este caso 0.5. Para la tabla de verdad ningÃºn valor de nuestros datos es verdadero, mientras que en la escala de nuestra funciÃ³n, por encima de 0.1 se considera como verdadero. Esto es importante porque nos da una pista de la importancia del tipo de normalizaciÃ³n que utilicemos. Puede ser que nuestros atributos tengan valores entre 0 y 100, pero la salida de la funciÃ³n booleana que intentamos aproximar considerarÃ¡ que serÃ¡ verdadera cuando el valor sea mayor que 40. Si normalizamos simplemente utilizando min max, nuestro umbral quedarÃ­a en 0.4, mientras que la tabla que booleaniza lo considera en 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quize experimentar moviendo el treshold en el codigo fuente de la libreria pero, en muchos lugares estÃ¡ el valor 0.5 estÃ¡ colocado \"Hard coded\" y hace dificil el cambio. Creo que una posible mejora en el rendimiento de estos algoritmos puede ser aÃ±adir un array de variables libres, tantas variables como atributos tengamos, para determinar el mejor valor de umbral de cada atributo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a explorar un dataset tÃ­pico que es el del iris con este modelo para ver como el conocimiento del problema influye directamente en los resultados obtenidos. Para fines de reuzabilidad, vamos a definir una funciÃ³n que contenga los pasos de separaciÃ³n de dataset, entrenamiento y extracciÃ³n de reglas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(targets: torch.Tensor, predictions_np: np.array):\n",
    "    # Convert tensors to numpy arrays\n",
    "    targets_np = targets.cpu().numpy()\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(targets_np, predictions_np)\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from logic_explained_networks import lens\n",
    "from logic_explained_networks.lens.utils.datasets import StructuredDataset\n",
    "from logic_explained_networks.lens.utils.metrics import F1Score, Accuracy, Metric\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "##############################################################\n",
    "def train_len(data: StructuredDataset, print_conf_matrix=0, metric: Metric = F1Score(), target_class=0, epoch=500,\n",
    "              l_r=0.1, hidden_neurons=20):\n",
    "    # Dataset splitting into train, validation, and testing with stratification\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data.x, data.y, test_size=0.3, stratify=data.y, random_state=42)\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, stratify=y_test, random_state=42)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    train_data = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
    "    val_data = TensorDataset(torch.tensor(x_val), torch.tensor(y_val))\n",
    "    test_data = TensorDataset(torch.tensor(x_test), torch.tensor(y_test))\n",
    "\n",
    "    # Model instantiation\n",
    "    model = lens.models.XMuNN(n_classes=len(torch.unique(data.y)), n_features=data.x.shape[1],\n",
    "                              hidden_neurons=[hidden_neurons], loss=torch.nn.CrossEntropyLoss())\n",
    "\n",
    "    # Training\n",
    "    model.fit(train_data, val_data, epochs=epoch, l_r=l_r)\n",
    "\n",
    "    # Get accuracy on test samples\n",
    "    test_acc = model.evaluate(test_data, metric=metric)\n",
    "    print(\"Test accuracy:\", test_acc)\n",
    "\n",
    "    concept_names = data.feature_names\n",
    "\n",
    "    # Create a DataFrame to store explanations and their metrics\n",
    "    explanations_dict = {\"Explanation\": [], \"Metric Result\": []}\n",
    "\n",
    "    for target in range(target_class):\n",
    "        formula = model.get_global_explanation(x_train, y_train, target,\n",
    "                                               top_k_explanations=2, metric=metric, concept_names=concept_names)\n",
    "\n",
    "        # Compute explanation accuracy\n",
    "        exp_accuracy, predictions = lens.logic.test_explanation(formula, target, x_test, y_test, metric=metric,\n",
    "                                                                concept_names=concept_names)\n",
    "\n",
    "        # Store the explanation and its metric result in the dictionary\n",
    "        explanations_dict[\"Explanation\"].append(formula)\n",
    "        explanations_dict[\"Metric Result\"].append(exp_accuracy)\n",
    "\n",
    "        if print_conf_matrix:\n",
    "            print_confusion_matrix(targets=y_test, predictions_np=predictions)\n",
    "    explanations_df = pd.DataFrame(explanations_dict)\n",
    "    return explanations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por curiosidad, veamos un entrenamiento sin ningÃºn preprocesado mÃ¡s que normalizar la data para que se encuentre en un rango entre 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tensor shape: torch.Size([150, 4])\n",
      "y_tensor shape: torch.Size([150])\n",
      "X_tensor: tensor([[0.2222, 0.6250, 0.0678, 0.0417],\n",
      "        [0.1667, 0.4167, 0.0678, 0.0417],\n",
      "        [0.1111, 0.5000, 0.0508, 0.0417],\n",
      "        [0.0833, 0.4583, 0.0847, 0.0417],\n",
      "        [0.1944, 0.6667, 0.0678, 0.0417],\n",
      "        [0.3056, 0.7917, 0.1186, 0.1250],\n",
      "        [0.0833, 0.5833, 0.0678, 0.0833],\n",
      "        [0.1944, 0.5833, 0.0847, 0.0417],\n",
      "        [0.0278, 0.3750, 0.0678, 0.0417],\n",
      "        [0.1667, 0.4583, 0.0847, 0.0000],\n",
      "        [0.3056, 0.7083, 0.0847, 0.0417],\n",
      "        [0.1389, 0.5833, 0.1017, 0.0417],\n",
      "        [0.1389, 0.4167, 0.0678, 0.0000],\n",
      "        [0.0000, 0.4167, 0.0169, 0.0000],\n",
      "        [0.4167, 0.8333, 0.0339, 0.0417],\n",
      "        [0.3889, 1.0000, 0.0847, 0.1250],\n",
      "        [0.3056, 0.7917, 0.0508, 0.1250],\n",
      "        [0.2222, 0.6250, 0.0678, 0.0833],\n",
      "        [0.3889, 0.7500, 0.1186, 0.0833],\n",
      "        [0.2222, 0.7500, 0.0847, 0.0833],\n",
      "        [0.3056, 0.5833, 0.1186, 0.0417],\n",
      "        [0.2222, 0.7083, 0.0847, 0.1250],\n",
      "        [0.0833, 0.6667, 0.0000, 0.0417],\n",
      "        [0.2222, 0.5417, 0.1186, 0.1667],\n",
      "        [0.1389, 0.5833, 0.1525, 0.0417],\n",
      "        [0.1944, 0.4167, 0.1017, 0.0417],\n",
      "        [0.1944, 0.5833, 0.1017, 0.1250],\n",
      "        [0.2500, 0.6250, 0.0847, 0.0417],\n",
      "        [0.2500, 0.5833, 0.0678, 0.0417],\n",
      "        [0.1111, 0.5000, 0.1017, 0.0417],\n",
      "        [0.1389, 0.4583, 0.1017, 0.0417],\n",
      "        [0.3056, 0.5833, 0.0847, 0.1250],\n",
      "        [0.2500, 0.8750, 0.0847, 0.0000],\n",
      "        [0.3333, 0.9167, 0.0678, 0.0417],\n",
      "        [0.1667, 0.4583, 0.0847, 0.0417],\n",
      "        [0.1944, 0.5000, 0.0339, 0.0417],\n",
      "        [0.3333, 0.6250, 0.0508, 0.0417],\n",
      "        [0.1667, 0.6667, 0.0678, 0.0000],\n",
      "        [0.0278, 0.4167, 0.0508, 0.0417],\n",
      "        [0.2222, 0.5833, 0.0847, 0.0417],\n",
      "        [0.1944, 0.6250, 0.0508, 0.0833],\n",
      "        [0.0556, 0.1250, 0.0508, 0.0833],\n",
      "        [0.0278, 0.5000, 0.0508, 0.0417],\n",
      "        [0.1944, 0.6250, 0.1017, 0.2083],\n",
      "        [0.2222, 0.7500, 0.1525, 0.1250],\n",
      "        [0.1389, 0.4167, 0.0678, 0.0833],\n",
      "        [0.2222, 0.7500, 0.1017, 0.0417],\n",
      "        [0.0833, 0.5000, 0.0678, 0.0417],\n",
      "        [0.2778, 0.7083, 0.0847, 0.0417],\n",
      "        [0.1944, 0.5417, 0.0678, 0.0417],\n",
      "        [0.7500, 0.5000, 0.6271, 0.5417],\n",
      "        [0.5833, 0.5000, 0.5932, 0.5833],\n",
      "        [0.7222, 0.4583, 0.6610, 0.5833],\n",
      "        [0.3333, 0.1250, 0.5085, 0.5000],\n",
      "        [0.6111, 0.3333, 0.6102, 0.5833],\n",
      "        [0.3889, 0.3333, 0.5932, 0.5000],\n",
      "        [0.5556, 0.5417, 0.6271, 0.6250],\n",
      "        [0.1667, 0.1667, 0.3898, 0.3750],\n",
      "        [0.6389, 0.3750, 0.6102, 0.5000],\n",
      "        [0.2500, 0.2917, 0.4915, 0.5417],\n",
      "        [0.1944, 0.0000, 0.4237, 0.3750],\n",
      "        [0.4444, 0.4167, 0.5424, 0.5833],\n",
      "        [0.4722, 0.0833, 0.5085, 0.3750],\n",
      "        [0.5000, 0.3750, 0.6271, 0.5417],\n",
      "        [0.3611, 0.3750, 0.4407, 0.5000],\n",
      "        [0.6667, 0.4583, 0.5763, 0.5417],\n",
      "        [0.3611, 0.4167, 0.5932, 0.5833],\n",
      "        [0.4167, 0.2917, 0.5254, 0.3750],\n",
      "        [0.5278, 0.0833, 0.5932, 0.5833],\n",
      "        [0.3611, 0.2083, 0.4915, 0.4167],\n",
      "        [0.4444, 0.5000, 0.6441, 0.7083],\n",
      "        [0.5000, 0.3333, 0.5085, 0.5000],\n",
      "        [0.5556, 0.2083, 0.6610, 0.5833],\n",
      "        [0.5000, 0.3333, 0.6271, 0.4583],\n",
      "        [0.5833, 0.3750, 0.5593, 0.5000],\n",
      "        [0.6389, 0.4167, 0.5763, 0.5417],\n",
      "        [0.6944, 0.3333, 0.6441, 0.5417],\n",
      "        [0.6667, 0.4167, 0.6780, 0.6667],\n",
      "        [0.4722, 0.3750, 0.5932, 0.5833],\n",
      "        [0.3889, 0.2500, 0.4237, 0.3750],\n",
      "        [0.3333, 0.1667, 0.4746, 0.4167],\n",
      "        [0.3333, 0.1667, 0.4576, 0.3750],\n",
      "        [0.4167, 0.2917, 0.4915, 0.4583],\n",
      "        [0.4722, 0.2917, 0.6949, 0.6250],\n",
      "        [0.3056, 0.4167, 0.5932, 0.5833],\n",
      "        [0.4722, 0.5833, 0.5932, 0.6250],\n",
      "        [0.6667, 0.4583, 0.6271, 0.5833],\n",
      "        [0.5556, 0.1250, 0.5763, 0.5000],\n",
      "        [0.3611, 0.4167, 0.5254, 0.5000],\n",
      "        [0.3333, 0.2083, 0.5085, 0.5000],\n",
      "        [0.3333, 0.2500, 0.5763, 0.4583],\n",
      "        [0.5000, 0.4167, 0.6102, 0.5417],\n",
      "        [0.4167, 0.2500, 0.5085, 0.4583],\n",
      "        [0.1944, 0.1250, 0.3898, 0.3750],\n",
      "        [0.3611, 0.2917, 0.5424, 0.5000],\n",
      "        [0.3889, 0.4167, 0.5424, 0.4583],\n",
      "        [0.3889, 0.3750, 0.5424, 0.5000],\n",
      "        [0.5278, 0.3750, 0.5593, 0.5000],\n",
      "        [0.2222, 0.2083, 0.3390, 0.4167],\n",
      "        [0.3889, 0.3333, 0.5254, 0.5000],\n",
      "        [0.5556, 0.5417, 0.8475, 1.0000],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.7778, 0.4167, 0.8305, 0.8333],\n",
      "        [0.5556, 0.3750, 0.7797, 0.7083],\n",
      "        [0.6111, 0.4167, 0.8136, 0.8750],\n",
      "        [0.9167, 0.4167, 0.9492, 0.8333],\n",
      "        [0.1667, 0.2083, 0.5932, 0.6667],\n",
      "        [0.8333, 0.3750, 0.8983, 0.7083],\n",
      "        [0.6667, 0.2083, 0.8136, 0.7083],\n",
      "        [0.8056, 0.6667, 0.8644, 1.0000],\n",
      "        [0.6111, 0.5000, 0.6949, 0.7917],\n",
      "        [0.5833, 0.2917, 0.7288, 0.7500],\n",
      "        [0.6944, 0.4167, 0.7627, 0.8333],\n",
      "        [0.3889, 0.2083, 0.6780, 0.7917],\n",
      "        [0.4167, 0.3333, 0.6949, 0.9583],\n",
      "        [0.5833, 0.5000, 0.7288, 0.9167],\n",
      "        [0.6111, 0.4167, 0.7627, 0.7083],\n",
      "        [0.9444, 0.7500, 0.9661, 0.8750],\n",
      "        [0.9444, 0.2500, 1.0000, 0.9167],\n",
      "        [0.4722, 0.0833, 0.6780, 0.5833],\n",
      "        [0.7222, 0.5000, 0.7966, 0.9167],\n",
      "        [0.3611, 0.3333, 0.6610, 0.7917],\n",
      "        [0.9444, 0.3333, 0.9661, 0.7917],\n",
      "        [0.5556, 0.2917, 0.6610, 0.7083],\n",
      "        [0.6667, 0.5417, 0.7966, 0.8333],\n",
      "        [0.8056, 0.5000, 0.8475, 0.7083],\n",
      "        [0.5278, 0.3333, 0.6441, 0.7083],\n",
      "        [0.5000, 0.4167, 0.6610, 0.7083],\n",
      "        [0.5833, 0.3333, 0.7797, 0.8333],\n",
      "        [0.8056, 0.4167, 0.8136, 0.6250],\n",
      "        [0.8611, 0.3333, 0.8644, 0.7500],\n",
      "        [1.0000, 0.7500, 0.9153, 0.7917],\n",
      "        [0.5833, 0.3333, 0.7797, 0.8750],\n",
      "        [0.5556, 0.3333, 0.6949, 0.5833],\n",
      "        [0.5000, 0.2500, 0.7797, 0.5417],\n",
      "        [0.9444, 0.4167, 0.8644, 0.9167],\n",
      "        [0.5556, 0.5833, 0.7797, 0.9583],\n",
      "        [0.5833, 0.4583, 0.7627, 0.7083],\n",
      "        [0.4722, 0.4167, 0.6441, 0.7083],\n",
      "        [0.7222, 0.4583, 0.7458, 0.8333],\n",
      "        [0.6667, 0.4583, 0.7797, 0.9583],\n",
      "        [0.7222, 0.4583, 0.6949, 0.9167],\n",
      "        [0.4167, 0.2917, 0.6949, 0.7500],\n",
      "        [0.6944, 0.5000, 0.8305, 0.9167],\n",
      "        [0.6667, 0.5417, 0.7966, 1.0000],\n",
      "        [0.6667, 0.4167, 0.7119, 0.9167],\n",
      "        [0.5556, 0.2083, 0.6780, 0.7500],\n",
      "        [0.6111, 0.4167, 0.7119, 0.7917],\n",
      "        [0.5278, 0.5833, 0.7458, 0.9167],\n",
      "        [0.4444, 0.4167, 0.6949, 0.7083]])\n",
      "Epoch: 1/500, Loss: 1.095, Tr_acc: 33.33, Val_acc: 31.82, best_e: -1\n",
      "Epoch: 2/500, Loss: 0.988, Tr_acc: 33.33, Val_acc: 68.18, best_e: -1\n",
      "Epoch: 3/500, Loss: 0.820, Tr_acc: 66.67, Val_acc: 68.18, best_e: -1\n",
      "Epoch: 4/500, Loss: 0.690, Tr_acc: 66.67, Val_acc: 81.82, best_e: -1\n",
      "Epoch: 5/500, Loss: 0.532, Tr_acc: 79.05, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 6/500, Loss: 0.437, Tr_acc: 93.33, Val_acc: 95.45, best_e: -1\n",
      "Epoch: 7/500, Loss: 0.349, Tr_acc: 95.24, Val_acc: 95.45, best_e: -1\n",
      "Epoch: 8/500, Loss: 0.304, Tr_acc: 90.48, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 9/500, Loss: 0.260, Tr_acc: 98.10, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 10/500, Loss: 0.222, Tr_acc: 99.05, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 11/500, Loss: 0.199, Tr_acc: 95.24, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 12/500, Loss: 0.170, Tr_acc: 99.05, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 13/500, Loss: 0.153, Tr_acc: 99.05, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 14/500, Loss: 0.137, Tr_acc: 98.10, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 15/500, Loss: 0.121, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 16/500, Loss: 0.113, Tr_acc: 99.05, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 17/500, Loss: 0.101, Tr_acc: 99.05, Val_acc: 86.36, best_e: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_490547/1542033949.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
      "/tmp/ipykernel_490547/1542033949.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_data = TensorDataset(torch.tensor(x_val), torch.tensor(y_val))\n",
      "/tmp/ipykernel_490547/1542033949.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data = TensorDataset(torch.tensor(x_test), torch.tensor(y_test))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/500, Loss: 0.095, Tr_acc: 98.10, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 19/500, Loss: 0.087, Tr_acc: 99.05, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 20/500, Loss: 0.081, Tr_acc: 99.05, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 21/500, Loss: 0.077, Tr_acc: 98.10, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 22/500, Loss: 0.072, Tr_acc: 99.05, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 23/500, Loss: 0.069, Tr_acc: 99.05, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 24/500, Loss: 0.065, Tr_acc: 99.05, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 25/500, Loss: 0.064, Tr_acc: 98.10, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 26/500, Loss: 0.061, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 27/500, Loss: 0.060, Tr_acc: 99.05, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 28/500, Loss: 0.057, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 29/500, Loss: 0.056, Tr_acc: 99.05, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 30/500, Loss: 0.054, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 31/500, Loss: 0.053, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 32/500, Loss: 0.053, Tr_acc: 99.05, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 33/500, Loss: 0.051, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 34/500, Loss: 0.050, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 35/500, Loss: 0.050, Tr_acc: 99.05, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 36/500, Loss: 0.049, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 37/500, Loss: 0.048, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 38/500, Loss: 0.047, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 39/500, Loss: 0.046, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 40/500, Loss: 0.046, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 41/500, Loss: 0.045, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 42/500, Loss: 0.044, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 43/500, Loss: 0.044, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 44/500, Loss: 0.044, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 45/500, Loss: 0.043, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 46/500, Loss: 0.043, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 47/500, Loss: 0.042, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 48/500, Loss: 0.041, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 49/500, Loss: 0.041, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 50/500, Loss: 0.041, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 51/500, Loss: 0.040, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 52/500, Loss: 0.040, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 53/500, Loss: 0.040, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 54/500, Loss: 0.039, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 55/500, Loss: 0.039, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 56/500, Loss: 0.039, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 57/500, Loss: 0.038, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 58/500, Loss: 0.038, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 59/500, Loss: 0.038, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 60/500, Loss: 0.037, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 61/500, Loss: 0.037, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 62/500, Loss: 0.037, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 63/500, Loss: 0.036, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 64/500, Loss: 0.036, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 65/500, Loss: 0.036, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 66/500, Loss: 0.036, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 67/500, Loss: 0.036, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 68/500, Loss: 0.035, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 69/500, Loss: 0.035, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 70/500, Loss: 0.035, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 71/500, Loss: 0.035, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 72/500, Loss: 0.037, Tr_acc: 99.05, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 73/500, Loss: 0.038, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 74/500, Loss: 0.039, Tr_acc: 99.05, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 75/500, Loss: 0.036, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 76/500, Loss: 0.034, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 77/500, Loss: 0.034, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 78/500, Loss: 0.035, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 79/500, Loss: 0.036, Tr_acc: 99.05, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 80/500, Loss: 0.034, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 81/500, Loss: 0.033, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 82/500, Loss: 0.034, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 83/500, Loss: 0.034, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 84/500, Loss: 0.033, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 85/500, Loss: 0.032, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 86/500, Loss: 0.032, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 87/500, Loss: 0.033, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 88/500, Loss: 0.032, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 89/500, Loss: 0.031, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 90/500, Loss: 0.031, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 91/500, Loss: 0.032, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 92/500, Loss: 0.031, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 93/500, Loss: 0.031, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 94/500, Loss: 0.030, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 95/500, Loss: 0.031, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 96/500, Loss: 0.031, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 97/500, Loss: 0.031, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 98/500, Loss: 0.031, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 99/500, Loss: 0.030, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 100/500, Loss: 0.030, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 101/500, Loss: 0.029, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 102/500, Loss: 0.029, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 103/500, Loss: 0.029, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 104/500, Loss: 0.029, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 105/500, Loss: 0.029, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 106/500, Loss: 0.029, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 107/500, Loss: 0.029, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 108/500, Loss: 0.029, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 109/500, Loss: 0.028, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 110/500, Loss: 0.028, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 111/500, Loss: 0.028, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 112/500, Loss: 0.028, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 113/500, Loss: 0.028, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 114/500, Loss: 0.028, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 115/500, Loss: 0.028, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 116/500, Loss: 0.028, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 117/500, Loss: 0.028, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 118/500, Loss: 0.028, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 119/500, Loss: 0.029, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 120/500, Loss: 0.031, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 121/500, Loss: 0.036, Tr_acc: 99.05, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 122/500, Loss: 0.040, Tr_acc: 98.10, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 123/500, Loss: 0.046, Tr_acc: 99.05, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 124/500, Loss: 0.032, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 125/500, Loss: 0.028, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 126/500, Loss: 0.037, Tr_acc: 99.05, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 127/500, Loss: 0.030, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 128/500, Loss: 0.027, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 129/500, Loss: 0.031, Tr_acc: 99.05, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 130/500, Loss: 0.030, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 131/500, Loss: 0.026, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 132/500, Loss: 0.028, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 133/500, Loss: 0.029, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 134/500, Loss: 0.026, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 135/500, Loss: 0.027, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 136/500, Loss: 0.028, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 137/500, Loss: 0.026, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 138/500, Loss: 0.026, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 139/500, Loss: 0.027, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 140/500, Loss: 0.026, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 141/500, Loss: 0.026, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 142/500, Loss: 0.026, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 143/500, Loss: 0.026, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 144/500, Loss: 0.025, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 145/500, Loss: 0.026, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 146/500, Loss: 0.025, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 147/500, Loss: 0.025, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 148/500, Loss: 0.025, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 149/500, Loss: 0.025, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 150/500, Loss: 0.025, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 151/500, Loss: 0.025, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 152/500, Loss: 0.025, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 153/500, Loss: 0.024, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 154/500, Loss: 0.025, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 155/500, Loss: 0.025, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 156/500, Loss: 0.024, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 157/500, Loss: 0.024, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 158/500, Loss: 0.024, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 159/500, Loss: 0.024, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 160/500, Loss: 0.024, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 161/500, Loss: 0.024, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 162/500, Loss: 0.024, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 163/500, Loss: 0.024, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 164/500, Loss: 0.024, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 165/500, Loss: 0.024, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 166/500, Loss: 0.023, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 167/500, Loss: 0.023, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 168/500, Loss: 0.023, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 169/500, Loss: 0.023, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 170/500, Loss: 0.023, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 171/500, Loss: 0.023, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 172/500, Loss: 0.023, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 173/500, Loss: 0.023, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 174/500, Loss: 0.023, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 175/500, Loss: 0.023, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 176/500, Loss: 0.023, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 177/500, Loss: 0.023, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 178/500, Loss: 0.023, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 179/500, Loss: 0.023, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 180/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 181/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 182/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 183/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 184/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 185/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 186/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 187/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 188/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 189/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 190/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 191/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 192/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 193/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 194/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 195/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 196/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 197/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 198/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 199/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 200/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 201/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 202/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 203/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 204/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 205/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 206/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 207/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 208/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 209/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 210/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 211/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 212/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 213/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 214/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 215/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 216/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 217/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 218/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 219/500, Loss: 0.020, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 220/500, Loss: 0.020, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 221/500, Loss: 0.020, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 222/500, Loss: 0.020, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 223/500, Loss: 0.020, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 224/500, Loss: 0.020, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 225/500, Loss: 0.020, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 226/500, Loss: 0.020, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 227/500, Loss: 0.020, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 228/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 229/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 230/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 231/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 232/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 233/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 234/500, Loss: 0.024, Tr_acc: 100.00, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 235/500, Loss: 0.025, Tr_acc: 100.00, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 236/500, Loss: 0.029, Tr_acc: 99.05, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 237/500, Loss: 0.035, Tr_acc: 98.10, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 238/500, Loss: 0.070, Tr_acc: 96.19, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 239/500, Loss: 0.132, Tr_acc: 95.24, Val_acc: 95.45, best_e: -1\n",
      "Epoch: 240/500, Loss: 0.178, Tr_acc: 96.19, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 241/500, Loss: 0.070, Tr_acc: 96.19, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 242/500, Loss: 0.241, Tr_acc: 94.29, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 243/500, Loss: 0.026, Tr_acc: 99.05, Val_acc: 95.45, best_e: -1\n",
      "Epoch: 244/500, Loss: 0.175, Tr_acc: 96.19, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 245/500, Loss: 0.043, Tr_acc: 98.10, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 246/500, Loss: 0.122, Tr_acc: 96.19, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 247/500, Loss: 0.044, Tr_acc: 99.05, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 248/500, Loss: 0.060, Tr_acc: 98.10, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 249/500, Loss: 0.070, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Pruned 1/4 features\n",
      "Pruned 1/4 features\n",
      "Pruned 2/4 features\n",
      "Pruned features\n",
      "Epoch: 250/500, Loss: 0.028, Tr_acc: 99.05, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 251/500, Loss: 0.090, Tr_acc: 97.14, Val_acc: 81.82, best_e: 251\n",
      "Epoch: 252/500, Loss: 0.664, Tr_acc: 86.67, Val_acc: 90.91, best_e: 252\n",
      "Epoch: 253/500, Loss: 0.092, Tr_acc: 98.10, Val_acc: 95.45, best_e: 253\n",
      "Epoch: 254/500, Loss: 0.129, Tr_acc: 97.14, Val_acc: 95.45, best_e: 254\n",
      "Epoch: 255/500, Loss: 0.320, Tr_acc: 93.33, Val_acc: 86.36, best_e: 254\n",
      "Epoch: 256/500, Loss: 0.331, Tr_acc: 92.38, Val_acc: 95.45, best_e: 256\n",
      "Epoch: 257/500, Loss: 0.203, Tr_acc: 96.19, Val_acc: 95.45, best_e: 257\n",
      "Epoch: 258/500, Loss: 0.108, Tr_acc: 97.14, Val_acc: 86.36, best_e: 257\n",
      "Epoch: 259/500, Loss: 0.074, Tr_acc: 97.14, Val_acc: 90.91, best_e: 257\n",
      "Epoch: 260/500, Loss: 0.078, Tr_acc: 98.10, Val_acc: 90.91, best_e: 257\n",
      "Epoch: 261/500, Loss: 0.127, Tr_acc: 96.19, Val_acc: 90.91, best_e: 257\n",
      "Epoch: 262/500, Loss: 0.156, Tr_acc: 95.24, Val_acc: 90.91, best_e: 257\n",
      "Epoch: 263/500, Loss: 0.127, Tr_acc: 96.19, Val_acc: 90.91, best_e: 257\n",
      "Epoch: 264/500, Loss: 0.085, Tr_acc: 99.05, Val_acc: 86.36, best_e: 257\n",
      "Epoch: 265/500, Loss: 0.068, Tr_acc: 98.10, Val_acc: 86.36, best_e: 257\n",
      "Epoch: 266/500, Loss: 0.068, Tr_acc: 97.14, Val_acc: 90.91, best_e: 257\n",
      "Epoch: 267/500, Loss: 0.076, Tr_acc: 97.14, Val_acc: 95.45, best_e: 267\n",
      "Epoch: 268/500, Loss: 0.086, Tr_acc: 97.14, Val_acc: 95.45, best_e: 268\n",
      "Epoch: 269/500, Loss: 0.093, Tr_acc: 98.10, Val_acc: 95.45, best_e: 269\n",
      "Epoch: 270/500, Loss: 0.090, Tr_acc: 97.14, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 271/500, Loss: 0.078, Tr_acc: 97.14, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 272/500, Loss: 0.064, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 273/500, Loss: 0.056, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 274/500, Loss: 0.055, Tr_acc: 98.10, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 275/500, Loss: 0.062, Tr_acc: 98.10, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 276/500, Loss: 0.070, Tr_acc: 98.10, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 277/500, Loss: 0.071, Tr_acc: 98.10, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 278/500, Loss: 0.065, Tr_acc: 98.10, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 279/500, Loss: 0.057, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 280/500, Loss: 0.053, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 281/500, Loss: 0.054, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 282/500, Loss: 0.057, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 283/500, Loss: 0.060, Tr_acc: 97.14, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 284/500, Loss: 0.060, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 285/500, Loss: 0.058, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 286/500, Loss: 0.055, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 287/500, Loss: 0.052, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 288/500, Loss: 0.051, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 289/500, Loss: 0.052, Tr_acc: 98.10, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 290/500, Loss: 0.053, Tr_acc: 98.10, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 291/500, Loss: 0.054, Tr_acc: 98.10, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 292/500, Loss: 0.053, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 293/500, Loss: 0.050, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 294/500, Loss: 0.049, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 295/500, Loss: 0.049, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 296/500, Loss: 0.049, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 297/500, Loss: 0.050, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 298/500, Loss: 0.050, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 299/500, Loss: 0.048, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 300/500, Loss: 0.047, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 301/500, Loss: 0.046, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 302/500, Loss: 0.046, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 303/500, Loss: 0.047, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 304/500, Loss: 0.047, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 305/500, Loss: 0.046, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 306/500, Loss: 0.045, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 307/500, Loss: 0.044, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 308/500, Loss: 0.044, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 309/500, Loss: 0.044, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 310/500, Loss: 0.044, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 311/500, Loss: 0.043, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 312/500, Loss: 0.042, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 313/500, Loss: 0.042, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 314/500, Loss: 0.041, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 315/500, Loss: 0.041, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 316/500, Loss: 0.040, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 317/500, Loss: 0.040, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 318/500, Loss: 0.039, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 319/500, Loss: 0.039, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 320/500, Loss: 0.038, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 321/500, Loss: 0.038, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 322/500, Loss: 0.037, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 323/500, Loss: 0.036, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 324/500, Loss: 0.036, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 325/500, Loss: 0.035, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 326/500, Loss: 0.035, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 327/500, Loss: 0.034, Tr_acc: 98.10, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 328/500, Loss: 0.034, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 329/500, Loss: 0.033, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 330/500, Loss: 0.033, Tr_acc: 98.10, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 331/500, Loss: 0.032, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 332/500, Loss: 0.032, Tr_acc: 99.05, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 333/500, Loss: 0.031, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 334/500, Loss: 0.031, Tr_acc: 98.10, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 335/500, Loss: 0.030, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 336/500, Loss: 0.030, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 337/500, Loss: 0.029, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 338/500, Loss: 0.029, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 339/500, Loss: 0.029, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 340/500, Loss: 0.029, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 341/500, Loss: 0.028, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 342/500, Loss: 0.028, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 343/500, Loss: 0.028, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 344/500, Loss: 0.028, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 345/500, Loss: 0.027, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 346/500, Loss: 0.027, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 347/500, Loss: 0.027, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 348/500, Loss: 0.027, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 349/500, Loss: 0.027, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 350/500, Loss: 0.027, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 351/500, Loss: 0.027, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 352/500, Loss: 0.026, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 353/500, Loss: 0.026, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 354/500, Loss: 0.026, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 355/500, Loss: 0.026, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 356/500, Loss: 0.026, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 357/500, Loss: 0.026, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 358/500, Loss: 0.025, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 359/500, Loss: 0.025, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 360/500, Loss: 0.025, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 361/500, Loss: 0.025, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 362/500, Loss: 0.024, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 363/500, Loss: 0.024, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 364/500, Loss: 0.024, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 365/500, Loss: 0.024, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 366/500, Loss: 0.023, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 367/500, Loss: 0.023, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 368/500, Loss: 0.023, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 369/500, Loss: 0.023, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 370/500, Loss: 0.022, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 371/500, Loss: 0.022, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 372/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 373/500, Loss: 0.022, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 374/500, Loss: 0.022, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 375/500, Loss: 0.022, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 376/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 377/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 378/500, Loss: 0.021, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 379/500, Loss: 0.021, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 380/500, Loss: 0.020, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 381/500, Loss: 0.020, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 382/500, Loss: 0.020, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 383/500, Loss: 0.020, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 384/500, Loss: 0.020, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 385/500, Loss: 0.019, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 386/500, Loss: 0.019, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 387/500, Loss: 0.019, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 388/500, Loss: 0.019, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 389/500, Loss: 0.019, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 390/500, Loss: 0.019, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 391/500, Loss: 0.019, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 392/500, Loss: 0.018, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 393/500, Loss: 0.018, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 394/500, Loss: 0.018, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 395/500, Loss: 0.018, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 396/500, Loss: 0.018, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 397/500, Loss: 0.018, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 398/500, Loss: 0.017, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 399/500, Loss: 0.017, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 400/500, Loss: 0.017, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 401/500, Loss: 0.017, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 402/500, Loss: 0.017, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 403/500, Loss: 0.017, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 404/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 405/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 406/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 407/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 408/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 409/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 410/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 411/500, Loss: 0.016, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 412/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 413/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 414/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 415/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 416/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 417/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 418/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 419/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 420/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 421/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 422/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 423/500, Loss: 0.014, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 424/500, Loss: 0.015, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 425/500, Loss: 0.015, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 426/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 427/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 428/500, Loss: 0.014, Tr_acc: 99.05, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 429/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 430/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 431/500, Loss: 0.014, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 432/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 433/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 434/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 435/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 436/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 437/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 438/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 439/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 440/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 441/500, Loss: 0.013, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 442/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 443/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 444/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 445/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 446/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 447/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 448/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 449/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 450/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 451/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 452/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 453/500, Loss: 0.012, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 454/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 455/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 456/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 457/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 458/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 459/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 460/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 461/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 462/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 463/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 464/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 465/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 466/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 467/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 468/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 469/500, Loss: 0.011, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 470/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 471/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 472/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 473/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 474/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 475/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 476/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 477/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 478/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 479/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 480/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 481/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 482/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 483/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 484/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 485/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 486/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 487/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 488/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 269\n",
      "Epoch: 489/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 95.45, best_e: 489\n",
      "Epoch: 490/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 489\n",
      "Epoch: 491/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 489\n",
      "Epoch: 492/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 489\n",
      "Epoch: 493/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 90.91, best_e: 489\n",
      "Epoch: 494/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 95.45, best_e: 494\n",
      "Epoch: 495/500, Loss: 0.010, Tr_acc: 100.00, Val_acc: 90.91, best_e: 494\n",
      "Epoch: 496/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 90.91, best_e: 494\n",
      "Epoch: 497/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 90.91, best_e: 494\n",
      "Epoch: 498/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 90.91, best_e: 494\n",
      "Epoch: 499/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 90.91, best_e: 494\n",
      "Epoch: 500/500, Loss: 0.009, Tr_acc: 100.00, Val_acc: 90.91, best_e: 494\n",
      "Test accuracy: 91.53439153439153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdum/PycharmProjects/URENA_MELENDEZ_PL/logic_explained_networks/lens/models/base.py:348: UserWarning: Loaded model does not have time or explanations. They need to be recalculated but time will only consider rule extraction time.\n",
      "  warnings.warn(\"Loaded model does not have time or explanations. \"\n"
     ]
    }
   ],
   "source": [
    "# Normalize X to be between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_normalized, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"X_tensor shape:\", X_tensor.shape)  # (150, 4)\n",
    "print(\"y_tensor shape:\", y_tensor.shape)  # (150, 3)\n",
    "\n",
    "# Verify values between 0 and 1\n",
    "print(\"X_tensor:\", X_tensor)\n",
    "\n",
    "data = StructuredDataset(X_tensor, y_tensor, dataset_name=\"iris\",\n",
    "                         feature_names=iris.feature_names, class_names=iris.target_names)\n",
    "\n",
    "explanations  = train_len(data=data, target_class=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Explanation</th>\n",
       "      <th>Metric Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>~petal length (cm) &amp; ~petal width (cm)</td>\n",
       "      <td>95.053763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>petal length (cm) &amp; ~sepal width (cm)</td>\n",
       "      <td>55.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>petal length (cm) &amp; petal width (cm)</td>\n",
       "      <td>82.307692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Explanation  Metric Result\n",
       "0  ~petal length (cm) & ~petal width (cm)      95.053763\n",
       "1   petal length (cm) & ~sepal width (cm)      55.769231\n",
       "2    petal length (cm) & petal width (cm)      82.307692"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando los resultados, vemos que el modelo realmente realizÃ³ un buen trabajo y su precisiÃ³n es interesante. Sin embargo, no es de mucha utilidad la explicaciÃ³n '~petal length (cm) & ~petal width (cm) <-> f0' porque, este nÃºmero realmente representa una propiedad continua y serÃ­a mÃ¡s beneficioso para un ser humano leer, petal width between x and y (cm) por poner un ejemplo. Asi que, vamos a discretizar los valores de longitud y anchura de cada atributo separandolo en categorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_490547/1542033949.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
      "/tmp/ipykernel_490547/1542033949.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_data = TensorDataset(torch.tensor(x_val), torch.tensor(y_val))\n",
      "/tmp/ipykernel_490547/1542033949.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data = TensorDataset(torch.tensor(x_test), torch.tensor(y_test))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500, Loss: 1.098, Tr_acc: 35.24, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 2/500, Loss: 0.711, Tr_acc: 83.81, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 3/500, Loss: 0.352, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 4/500, Loss: 0.212, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 5/500, Loss: 0.182, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 6/500, Loss: 0.178, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 7/500, Loss: 0.171, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 8/500, Loss: 0.158, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 9/500, Loss: 0.141, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 10/500, Loss: 0.132, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 11/500, Loss: 0.129, Tr_acc: 95.24, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 12/500, Loss: 0.123, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 13/500, Loss: 0.115, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 14/500, Loss: 0.110, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 15/500, Loss: 0.109, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 16/500, Loss: 0.110, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 17/500, Loss: 0.110, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 18/500, Loss: 0.109, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 19/500, Loss: 0.110, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 20/500, Loss: 0.112, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 21/500, Loss: 0.114, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 22/500, Loss: 0.114, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 23/500, Loss: 0.111, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 24/500, Loss: 0.111, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 25/500, Loss: 0.112, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 26/500, Loss: 0.109, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 27/500, Loss: 0.110, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 28/500, Loss: 0.112, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 29/500, Loss: 0.111, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 30/500, Loss: 0.108, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 31/500, Loss: 0.111, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 32/500, Loss: 0.110, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 33/500, Loss: 0.109, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 34/500, Loss: 0.110, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 35/500, Loss: 0.107, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 36/500, Loss: 0.108, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 37/500, Loss: 0.107, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 38/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 39/500, Loss: 0.106, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 40/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 41/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 42/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 43/500, Loss: 0.104, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 44/500, Loss: 0.104, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 45/500, Loss: 0.103, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 46/500, Loss: 0.103, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 47/500, Loss: 0.103, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 48/500, Loss: 0.102, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 49/500, Loss: 0.102, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 50/500, Loss: 0.101, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 51/500, Loss: 0.101, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 52/500, Loss: 0.101, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 53/500, Loss: 0.100, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 54/500, Loss: 0.100, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 55/500, Loss: 0.100, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 56/500, Loss: 0.100, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 57/500, Loss: 0.099, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 58/500, Loss: 0.098, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 59/500, Loss: 0.098, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 60/500, Loss: 0.098, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 61/500, Loss: 0.097, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 62/500, Loss: 0.097, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 63/500, Loss: 0.097, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 64/500, Loss: 0.096, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 65/500, Loss: 0.096, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 66/500, Loss: 0.095, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 67/500, Loss: 0.095, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 68/500, Loss: 0.095, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 69/500, Loss: 0.095, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 70/500, Loss: 0.094, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 71/500, Loss: 0.094, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 72/500, Loss: 0.094, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 73/500, Loss: 0.093, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 74/500, Loss: 0.093, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 75/500, Loss: 0.093, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 76/500, Loss: 0.092, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 77/500, Loss: 0.092, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 78/500, Loss: 0.092, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 79/500, Loss: 0.092, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 80/500, Loss: 0.091, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 81/500, Loss: 0.091, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 82/500, Loss: 0.091, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 83/500, Loss: 0.091, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 84/500, Loss: 0.091, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 85/500, Loss: 0.091, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 86/500, Loss: 0.093, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 87/500, Loss: 0.094, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 88/500, Loss: 0.090, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 89/500, Loss: 0.090, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 90/500, Loss: 0.093, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 91/500, Loss: 0.091, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 92/500, Loss: 0.089, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 93/500, Loss: 0.090, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 94/500, Loss: 0.090, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 95/500, Loss: 0.088, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 96/500, Loss: 0.089, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 97/500, Loss: 0.089, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 98/500, Loss: 0.088, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 99/500, Loss: 0.088, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 100/500, Loss: 0.088, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 101/500, Loss: 0.088, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 102/500, Loss: 0.087, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 103/500, Loss: 0.088, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 104/500, Loss: 0.087, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 105/500, Loss: 0.087, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 106/500, Loss: 0.087, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 107/500, Loss: 0.087, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 108/500, Loss: 0.087, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 109/500, Loss: 0.086, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 110/500, Loss: 0.086, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 111/500, Loss: 0.086, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 112/500, Loss: 0.086, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 113/500, Loss: 0.086, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 114/500, Loss: 0.085, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 115/500, Loss: 0.086, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 116/500, Loss: 0.085, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 117/500, Loss: 0.085, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 118/500, Loss: 0.085, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 119/500, Loss: 0.085, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 120/500, Loss: 0.085, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 121/500, Loss: 0.085, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 122/500, Loss: 0.084, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 123/500, Loss: 0.084, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 124/500, Loss: 0.084, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 125/500, Loss: 0.084, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 126/500, Loss: 0.084, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 127/500, Loss: 0.084, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 128/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 129/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 130/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 131/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 132/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 133/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 134/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 135/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 136/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 137/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 138/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 139/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 140/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 141/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 142/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 143/500, Loss: 0.095, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 144/500, Loss: 0.139, Tr_acc: 95.24, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 145/500, Loss: 0.089, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 146/500, Loss: 0.153, Tr_acc: 90.48, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 147/500, Loss: 0.115, Tr_acc: 97.14, Val_acc: 90.91, best_e: -1\n",
      "Epoch: 148/500, Loss: 0.168, Tr_acc: 95.24, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 149/500, Loss: 0.123, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 150/500, Loss: 0.100, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 151/500, Loss: 0.158, Tr_acc: 90.48, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 152/500, Loss: 0.088, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 153/500, Loss: 0.135, Tr_acc: 95.24, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 154/500, Loss: 0.114, Tr_acc: 95.24, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 155/500, Loss: 0.091, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 156/500, Loss: 0.127, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 157/500, Loss: 0.096, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 158/500, Loss: 0.101, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 159/500, Loss: 0.109, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 160/500, Loss: 0.094, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 161/500, Loss: 0.091, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 162/500, Loss: 0.101, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 163/500, Loss: 0.089, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 164/500, Loss: 0.094, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 165/500, Loss: 0.097, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 166/500, Loss: 0.090, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 167/500, Loss: 0.091, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 168/500, Loss: 0.095, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 169/500, Loss: 0.090, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 170/500, Loss: 0.091, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 171/500, Loss: 0.092, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 172/500, Loss: 0.090, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 173/500, Loss: 0.089, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 174/500, Loss: 0.090, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 175/500, Loss: 0.089, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 176/500, Loss: 0.089, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 177/500, Loss: 0.089, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 178/500, Loss: 0.088, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 179/500, Loss: 0.089, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 180/500, Loss: 0.088, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 181/500, Loss: 0.088, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 182/500, Loss: 0.088, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 183/500, Loss: 0.087, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 184/500, Loss: 0.087, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 185/500, Loss: 0.087, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 186/500, Loss: 0.087, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 187/500, Loss: 0.087, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 188/500, Loss: 0.086, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 189/500, Loss: 0.087, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 190/500, Loss: 0.086, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 191/500, Loss: 0.086, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 192/500, Loss: 0.086, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 193/500, Loss: 0.086, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 194/500, Loss: 0.086, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 195/500, Loss: 0.085, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 196/500, Loss: 0.085, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 197/500, Loss: 0.085, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 198/500, Loss: 0.085, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 199/500, Loss: 0.085, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 200/500, Loss: 0.085, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 201/500, Loss: 0.085, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 202/500, Loss: 0.084, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 203/500, Loss: 0.084, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 204/500, Loss: 0.084, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 205/500, Loss: 0.084, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 206/500, Loss: 0.084, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 207/500, Loss: 0.084, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 208/500, Loss: 0.084, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 209/500, Loss: 0.084, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 210/500, Loss: 0.084, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 211/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 212/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 213/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 214/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 215/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 216/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 217/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 218/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 219/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 220/500, Loss: 0.083, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 221/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 222/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 223/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 224/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 225/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 226/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 227/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 228/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 229/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 230/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 231/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 232/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 233/500, Loss: 0.082, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 234/500, Loss: 0.081, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 235/500, Loss: 0.081, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 236/500, Loss: 0.081, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 237/500, Loss: 0.081, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 238/500, Loss: 0.081, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 239/500, Loss: 0.081, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 240/500, Loss: 0.081, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 241/500, Loss: 0.081, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 242/500, Loss: 0.081, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 243/500, Loss: 0.081, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 244/500, Loss: 0.081, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 245/500, Loss: 0.081, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 246/500, Loss: 0.081, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 247/500, Loss: 0.081, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 248/500, Loss: 0.081, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 249/500, Loss: 0.080, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Pruned 11/13 features\n",
      "Pruned 11/13 features\n",
      "Pruned 9/13 features\n",
      "Pruned features\n",
      "Epoch: 250/500, Loss: 0.080, Tr_acc: 97.14, Val_acc: 86.36, best_e: -1\n",
      "Epoch: 251/500, Loss: 0.190, Tr_acc: 97.14, Val_acc: 86.36, best_e: 251\n",
      "Epoch: 252/500, Loss: 0.137, Tr_acc: 97.14, Val_acc: 86.36, best_e: 252\n",
      "Epoch: 253/500, Loss: 0.108, Tr_acc: 97.14, Val_acc: 86.36, best_e: 253\n",
      "Epoch: 254/500, Loss: 0.142, Tr_acc: 97.14, Val_acc: 86.36, best_e: 254\n",
      "Epoch: 255/500, Loss: 0.123, Tr_acc: 97.14, Val_acc: 86.36, best_e: 255\n",
      "Epoch: 256/500, Loss: 0.112, Tr_acc: 97.14, Val_acc: 86.36, best_e: 256\n",
      "Epoch: 257/500, Loss: 0.108, Tr_acc: 97.14, Val_acc: 86.36, best_e: 257\n",
      "Epoch: 258/500, Loss: 0.108, Tr_acc: 97.14, Val_acc: 86.36, best_e: 258\n",
      "Epoch: 259/500, Loss: 0.111, Tr_acc: 97.14, Val_acc: 86.36, best_e: 259\n",
      "Epoch: 260/500, Loss: 0.112, Tr_acc: 97.14, Val_acc: 86.36, best_e: 260\n",
      "Epoch: 261/500, Loss: 0.111, Tr_acc: 97.14, Val_acc: 86.36, best_e: 261\n",
      "Epoch: 262/500, Loss: 0.109, Tr_acc: 97.14, Val_acc: 86.36, best_e: 262\n",
      "Epoch: 263/500, Loss: 0.107, Tr_acc: 97.14, Val_acc: 86.36, best_e: 263\n",
      "Epoch: 264/500, Loss: 0.106, Tr_acc: 97.14, Val_acc: 86.36, best_e: 264\n",
      "Epoch: 265/500, Loss: 0.106, Tr_acc: 97.14, Val_acc: 86.36, best_e: 265\n",
      "Epoch: 266/500, Loss: 0.106, Tr_acc: 97.14, Val_acc: 86.36, best_e: 266\n",
      "Epoch: 267/500, Loss: 0.107, Tr_acc: 97.14, Val_acc: 86.36, best_e: 267\n",
      "Epoch: 268/500, Loss: 0.107, Tr_acc: 97.14, Val_acc: 86.36, best_e: 268\n",
      "Epoch: 269/500, Loss: 0.107, Tr_acc: 97.14, Val_acc: 86.36, best_e: 269\n",
      "Epoch: 270/500, Loss: 0.107, Tr_acc: 97.14, Val_acc: 86.36, best_e: 270\n",
      "Epoch: 271/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 271\n",
      "Epoch: 272/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 272\n",
      "Epoch: 273/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 273\n",
      "Epoch: 274/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 274\n",
      "Epoch: 275/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 275\n",
      "Epoch: 276/500, Loss: 0.106, Tr_acc: 97.14, Val_acc: 86.36, best_e: 276\n",
      "Epoch: 277/500, Loss: 0.106, Tr_acc: 97.14, Val_acc: 86.36, best_e: 277\n",
      "Epoch: 278/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 278\n",
      "Epoch: 279/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 279\n",
      "Epoch: 280/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 280\n",
      "Epoch: 281/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 281\n",
      "Epoch: 282/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 282\n",
      "Epoch: 283/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 283\n",
      "Epoch: 284/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 284\n",
      "Epoch: 285/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 285\n",
      "Epoch: 286/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 286\n",
      "Epoch: 287/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 287\n",
      "Epoch: 288/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 288\n",
      "Epoch: 289/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 289\n",
      "Epoch: 290/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 290\n",
      "Epoch: 291/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 291\n",
      "Epoch: 292/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 292\n",
      "Epoch: 293/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 293\n",
      "Epoch: 294/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 294\n",
      "Epoch: 295/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 295\n",
      "Epoch: 296/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 296\n",
      "Epoch: 297/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 297\n",
      "Epoch: 298/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 298\n",
      "Epoch: 299/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 299\n",
      "Epoch: 300/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 300\n",
      "Epoch: 301/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 301\n",
      "Epoch: 302/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 302\n",
      "Epoch: 303/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 303\n",
      "Epoch: 304/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 304\n",
      "Epoch: 305/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 305\n",
      "Epoch: 306/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 306\n",
      "Epoch: 307/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 307\n",
      "Epoch: 308/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 308\n",
      "Epoch: 309/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 309\n",
      "Epoch: 310/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 310\n",
      "Epoch: 311/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 311\n",
      "Epoch: 312/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 312\n",
      "Epoch: 313/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 313\n",
      "Epoch: 314/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 314\n",
      "Epoch: 315/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 315\n",
      "Epoch: 316/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 316\n",
      "Epoch: 317/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 317\n",
      "Epoch: 318/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 318\n",
      "Epoch: 319/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 319\n",
      "Epoch: 320/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 320\n",
      "Epoch: 321/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 321\n",
      "Epoch: 322/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 322\n",
      "Epoch: 323/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 323\n",
      "Epoch: 324/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 324\n",
      "Epoch: 325/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 325\n",
      "Epoch: 326/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 326\n",
      "Epoch: 327/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 327\n",
      "Epoch: 328/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 328\n",
      "Epoch: 329/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 329\n",
      "Epoch: 330/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 330\n",
      "Epoch: 331/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 331\n",
      "Epoch: 332/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 332\n",
      "Epoch: 333/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 333\n",
      "Epoch: 334/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 334\n",
      "Epoch: 335/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 335\n",
      "Epoch: 336/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 336\n",
      "Epoch: 337/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 337\n",
      "Epoch: 338/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 338\n",
      "Epoch: 339/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 339\n",
      "Epoch: 340/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 340\n",
      "Epoch: 341/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 341\n",
      "Epoch: 342/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 342\n",
      "Epoch: 343/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 343\n",
      "Epoch: 344/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 344\n",
      "Epoch: 345/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 345\n",
      "Epoch: 346/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 346\n",
      "Epoch: 347/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 347\n",
      "Epoch: 348/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 348\n",
      "Epoch: 349/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 349\n",
      "Epoch: 350/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 350\n",
      "Epoch: 351/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 351\n",
      "Epoch: 352/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 352\n",
      "Epoch: 353/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 353\n",
      "Epoch: 354/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 354\n",
      "Epoch: 355/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 355\n",
      "Epoch: 356/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 356\n",
      "Epoch: 357/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 357\n",
      "Epoch: 358/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 358\n",
      "Epoch: 359/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 359\n",
      "Epoch: 360/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 360\n",
      "Epoch: 361/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 361\n",
      "Epoch: 362/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 362\n",
      "Epoch: 363/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 363\n",
      "Epoch: 364/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 364\n",
      "Epoch: 365/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 365\n",
      "Epoch: 366/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 366\n",
      "Epoch: 367/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 367\n",
      "Epoch: 368/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 368\n",
      "Epoch: 369/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 369\n",
      "Epoch: 370/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 370\n",
      "Epoch: 371/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 371\n",
      "Epoch: 372/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 372\n",
      "Epoch: 373/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 373\n",
      "Epoch: 374/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 374\n",
      "Epoch: 375/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 375\n",
      "Epoch: 376/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 376\n",
      "Epoch: 377/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 377\n",
      "Epoch: 378/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 378\n",
      "Epoch: 379/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 379\n",
      "Epoch: 380/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 380\n",
      "Epoch: 381/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 381\n",
      "Epoch: 382/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 382\n",
      "Epoch: 383/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 383\n",
      "Epoch: 384/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 384\n",
      "Epoch: 385/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 385\n",
      "Epoch: 386/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 386\n",
      "Epoch: 387/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 387\n",
      "Epoch: 388/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 388\n",
      "Epoch: 389/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 389\n",
      "Epoch: 390/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 390\n",
      "Epoch: 391/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 391\n",
      "Epoch: 392/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 392\n",
      "Epoch: 393/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 393\n",
      "Epoch: 394/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 394\n",
      "Epoch: 395/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 395\n",
      "Epoch: 396/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 396\n",
      "Epoch: 397/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 397\n",
      "Epoch: 398/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 398\n",
      "Epoch: 399/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 399\n",
      "Epoch: 400/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 400\n",
      "Epoch: 401/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 401\n",
      "Epoch: 402/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 402\n",
      "Epoch: 403/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 403\n",
      "Epoch: 404/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 404\n",
      "Epoch: 405/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 405\n",
      "Epoch: 406/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 406\n",
      "Epoch: 407/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 407\n",
      "Epoch: 408/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 408\n",
      "Epoch: 409/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 409\n",
      "Epoch: 410/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 410\n",
      "Epoch: 411/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 411\n",
      "Epoch: 412/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 412\n",
      "Epoch: 413/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 413\n",
      "Epoch: 414/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 414\n",
      "Epoch: 415/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 415\n",
      "Epoch: 416/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 416\n",
      "Epoch: 417/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 417\n",
      "Epoch: 418/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 418\n",
      "Epoch: 419/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 419\n",
      "Epoch: 420/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 420\n",
      "Epoch: 421/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 421\n",
      "Epoch: 422/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 422\n",
      "Epoch: 423/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 423\n",
      "Epoch: 424/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 424\n",
      "Epoch: 425/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 425\n",
      "Epoch: 426/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 426\n",
      "Epoch: 427/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 427\n",
      "Epoch: 428/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 428\n",
      "Epoch: 429/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 429\n",
      "Epoch: 430/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 430\n",
      "Epoch: 431/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 431\n",
      "Epoch: 432/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 432\n",
      "Epoch: 433/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 433\n",
      "Epoch: 434/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 434\n",
      "Epoch: 435/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 435\n",
      "Epoch: 436/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 436\n",
      "Epoch: 437/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 437\n",
      "Epoch: 438/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 438\n",
      "Epoch: 439/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 439\n",
      "Epoch: 440/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 440\n",
      "Epoch: 441/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 441\n",
      "Epoch: 442/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 442\n",
      "Epoch: 443/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 443\n",
      "Epoch: 444/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 444\n",
      "Epoch: 445/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 445\n",
      "Epoch: 446/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 446\n",
      "Epoch: 447/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 447\n",
      "Epoch: 448/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 448\n",
      "Epoch: 449/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 449\n",
      "Epoch: 450/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 450\n",
      "Epoch: 451/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 451\n",
      "Epoch: 452/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 452\n",
      "Epoch: 453/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 453\n",
      "Epoch: 454/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 454\n",
      "Epoch: 455/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 455\n",
      "Epoch: 456/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 456\n",
      "Epoch: 457/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 457\n",
      "Epoch: 458/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 458\n",
      "Epoch: 459/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 459\n",
      "Epoch: 460/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 460\n",
      "Epoch: 461/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 461\n",
      "Epoch: 462/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 462\n",
      "Epoch: 463/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 463\n",
      "Epoch: 464/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 464\n",
      "Epoch: 465/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 465\n",
      "Epoch: 466/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 466\n",
      "Epoch: 467/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 467\n",
      "Epoch: 468/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 468\n",
      "Epoch: 469/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 469\n",
      "Epoch: 470/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 470\n",
      "Epoch: 471/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 471\n",
      "Epoch: 472/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 472\n",
      "Epoch: 473/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 473\n",
      "Epoch: 474/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 474\n",
      "Epoch: 475/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 475\n",
      "Epoch: 476/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 476\n",
      "Epoch: 477/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 477\n",
      "Epoch: 478/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 478\n",
      "Epoch: 479/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 479\n",
      "Epoch: 480/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 480\n",
      "Epoch: 481/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 481\n",
      "Epoch: 482/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 482\n",
      "Epoch: 483/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 483\n",
      "Epoch: 484/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 484\n",
      "Epoch: 485/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 485\n",
      "Epoch: 486/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 486\n",
      "Epoch: 487/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 487\n",
      "Epoch: 488/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 488\n",
      "Epoch: 489/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 489\n",
      "Epoch: 490/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 490\n",
      "Epoch: 491/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 491\n",
      "Epoch: 492/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 492\n",
      "Epoch: 493/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 493\n",
      "Epoch: 494/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 494\n",
      "Epoch: 495/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 495\n",
      "Epoch: 496/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 496\n",
      "Epoch: 497/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 497\n",
      "Epoch: 498/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 498\n",
      "Epoch: 499/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 499\n",
      "Epoch: 500/500, Loss: 0.105, Tr_acc: 97.14, Val_acc: 86.36, best_e: 500\n",
      "Test accuracy: 87.45098039215686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdum/PycharmProjects/URENA_MELENDEZ_PL/logic_explained_networks/lens/models/base.py:348: UserWarning: Loaded model does not have time or explanations. They need to be recalculated but time will only consider rule extraction time.\n",
      "  warnings.warn(\"Loaded model does not have time or explanations. \"\n"
     ]
    }
   ],
   "source": [
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# lets define custom ranges for each feature\n",
    "sepal_length_bins = [0, 5, 7, 10]\n",
    "sepal_width_bins = [0, 2.5, 3.5, 5]\n",
    "petal_length_bins = [0, 2, 4, 6]\n",
    "petal_width_bins = [0, 0.5, 1, 1.5, 2]\n",
    "\n",
    "\n",
    "# Function to encode data based on ranges\n",
    "def encode_ranges(data, bins, labels):\n",
    "    encoded_data = pd.cut(data, bins=bins, labels=labels)\n",
    "    return pd.get_dummies(encoded_data, prefix='', prefix_sep='')\n",
    "\n",
    "\n",
    "# Encode the data based on these ranges\n",
    "sepal_length_encoded = encode_ranges(iris_df['sepal length (cm)'], sepal_length_bins,\n",
    "                                     ['sepal L 0-5', 'sepal L 5-7', 'sepal L 7-10'])\n",
    "sepal_width_encoded = encode_ranges(iris_df['sepal width (cm)'], sepal_width_bins,\n",
    "                                    ['sepal W 0-2.5', 'sepal W 2.5-3.5', 'sepal W 3.5-5'])\n",
    "petal_length_encoded = encode_ranges(iris_df['petal length (cm)'], petal_length_bins,\n",
    "                                     ['petal L 0-2', 'petal L 2-4', 'petal L 4-6'])\n",
    "petal_width_encoded = encode_ranges(iris_df['petal width (cm)'], petal_width_bins,\n",
    "                                    ['petal W 0-0.5', 'petal W 0.5-1', 'petal W 1-1.5', 'petal W 1.5-2'])\n",
    "\n",
    "# Concatenate the encoded columns\n",
    "encoded_df = pd.concat([sepal_length_encoded, sepal_width_encoded, petal_length_encoded, petal_width_encoded], axis=1)\n",
    "# shuffled_df = encoded_df.sample(frac=1, axis=1, random_state=42)\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(encoded_df.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "data = StructuredDataset(X_tensor, y_tensor, dataset_name=\"iris\",\n",
    "                         feature_names=encoded_df.columns, class_names=iris.target_names)\n",
    "\n",
    "explanations = train_len(data=data, target_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Explanation</th>\n",
       "      <th>Metric Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>petal L 0-2 &amp; petal W 0-0.5</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(petal W 0.5-1 &amp; ~petal W 1-1.5) | (petal W 1-...</td>\n",
       "      <td>85.161290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(sepal L 7-10 &amp; ~petal L 4-6 &amp; ~petal W 0.5-1 ...</td>\n",
       "      <td>86.004057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Explanation  Metric Result\n",
       "0                        petal L 0-2 & petal W 0-0.5     100.000000\n",
       "1  (petal W 0.5-1 & ~petal W 1-1.5) | (petal W 1-...      85.161290\n",
       "2  (sepal L 7-10 & ~petal L 4-6 & ~petal W 0.5-1 ...      86.004057"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations.to_csv('results_iris_discrete.csv')\n",
    "explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos observar una explicaciÃ³n mucho mÃ¡s rica y con mejor desempeÃ±o que la anterior. El conocimiento del problema es escencial cuando se trabaja con LENs porque su explicabilidad serÃ¡ tan buena como la explicabilidad de los datos de entrada en forma binaria. Este implica, que cuando los datos contiene rangos y no son booleanos, el preprocesado que se aplique afectarÃ¡ considerablemente los resultados porque al booleanizar es como separar en solo dos categorias mayor a un umbral y menor que un umbral. Entonces, el normalizado que se aplique si tiene alguna escala; logaritmica, cuadratica, etc. O si se decide booleanizar el valor separando en rangos, la selecciÃ³n de dichos rangos tambien juega un factor importante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos un ejemplo real, del [dataset][cdc_dataset] de diabetes del CDC.\n",
    "\n",
    "[cdc_dataset]: https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Al parecer ucimlrepo tiene problemas con este dataset y aveces no quiere descargarse.\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import load_diabetes\n",
    "import ssl\n",
    "\n",
    "# Ignore ssl certificate verification\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# fetch dataset\n",
    "cdc_diabetes_health_indicators = fetch_ucirepo(id=891)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data (as pandas dataframes)\n",
    "X = cdc_diabetes_health_indicators.data.features\n",
    "y = cdc_diabetes_health_indicators.data.targets\n",
    "\n",
    "# drop most likely features to be not relevant\n",
    "X_filtered = X.iloc[:, :-2]\n",
    "X_filtered = X_filtered.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , ..., 1.        , 0.        ,\n",
       "        0.6666667 ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.50000006],\n",
       "       [1.        , 1.        , 1.        , ..., 1.        , 0.        ,\n",
       "        0.6666667 ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "        0.08333334],\n",
       "       [1.        , 0.        , 1.        , ..., 0.        , 1.        ,\n",
       "        0.50000006],\n",
       "       [1.        , 1.        , 1.        , ..., 0.        , 0.        ,\n",
       "        0.6666667 ]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_standardized = scaler.fit_transform(X_filtered)\n",
    "X_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([35346]), 253680)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y.values), len(y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui inmediatamente se hace evidente la necesidad del conocimiento previo del problema, para reflejar esto, vamos a entrenar sin pre-procesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_490547/1542033949.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
      "/tmp/ipykernel_490547/1542033949.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_data = TensorDataset(torch.tensor(x_val), torch.tensor(y_val))\n",
      "/tmp/ipykernel_490547/1542033949.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data = TensorDataset(torch.tensor(x_test), torch.tensor(y_test))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Loss: 0.746, Tr_acc: 26.26, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 2/100, Loss: 0.618, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 3/100, Loss: 0.467, Tr_acc: 86.07, Val_acc: 86.08, best_e: -1\n",
      "Epoch: 4/100, Loss: 0.496, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 5/100, Loss: 0.403, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 6/100, Loss: 0.413, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 7/100, Loss: 0.422, Tr_acc: 86.07, Val_acc: 86.09, best_e: -1\n",
      "Epoch: 8/100, Loss: 0.395, Tr_acc: 86.07, Val_acc: 85.95, best_e: -1\n",
      "Epoch: 9/100, Loss: 0.377, Tr_acc: 86.00, Val_acc: 85.24, best_e: -1\n",
      "Epoch: 10/100, Loss: 0.386, Tr_acc: 85.28, Val_acc: 84.72, best_e: -1\n",
      "Epoch: 11/100, Loss: 0.394, Tr_acc: 84.76, Val_acc: 85.18, best_e: -1\n",
      "Epoch: 12/100, Loss: 0.383, Tr_acc: 85.23, Val_acc: 85.85, best_e: -1\n",
      "Epoch: 13/100, Loss: 0.373, Tr_acc: 85.80, Val_acc: 86.09, best_e: -1\n",
      "Epoch: 14/100, Loss: 0.374, Tr_acc: 86.09, Val_acc: 86.15, best_e: -1\n",
      "Epoch: 15/100, Loss: 0.379, Tr_acc: 86.13, Val_acc: 86.15, best_e: -1\n",
      "Epoch: 16/100, Loss: 0.380, Tr_acc: 86.15, Val_acc: 86.14, best_e: -1\n",
      "Epoch: 17/100, Loss: 0.376, Tr_acc: 86.14, Val_acc: 86.21, best_e: -1\n",
      "Epoch: 18/100, Loss: 0.370, Tr_acc: 86.17, Val_acc: 86.20, best_e: -1\n",
      "Epoch: 19/100, Loss: 0.368, Tr_acc: 86.17, Val_acc: 86.17, best_e: -1\n",
      "Epoch: 20/100, Loss: 0.370, Tr_acc: 86.12, Val_acc: 86.20, best_e: -1\n",
      "Epoch: 21/100, Loss: 0.370, Tr_acc: 86.20, Val_acc: 86.23, best_e: -1\n",
      "Epoch: 22/100, Loss: 0.370, Tr_acc: 86.19, Val_acc: 86.19, best_e: -1\n",
      "Epoch: 23/100, Loss: 0.368, Tr_acc: 86.16, Val_acc: 86.09, best_e: -1\n",
      "Epoch: 24/100, Loss: 0.366, Tr_acc: 86.09, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 25/100, Loss: 0.365, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 26/100, Loss: 0.365, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 27/100, Loss: 0.365, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 28/100, Loss: 0.365, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 29/100, Loss: 0.364, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 30/100, Loss: 0.363, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 31/100, Loss: 0.362, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 32/100, Loss: 0.362, Tr_acc: 86.07, Val_acc: 86.06, best_e: -1\n",
      "Epoch: 33/100, Loss: 0.362, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 34/100, Loss: 0.362, Tr_acc: 86.08, Val_acc: 86.06, best_e: -1\n",
      "Epoch: 35/100, Loss: 0.361, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 36/100, Loss: 0.360, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 37/100, Loss: 0.359, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 38/100, Loss: 0.359, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 39/100, Loss: 0.358, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 40/100, Loss: 0.358, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 41/100, Loss: 0.357, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 42/100, Loss: 0.356, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 43/100, Loss: 0.356, Tr_acc: 86.07, Val_acc: 86.06, best_e: -1\n",
      "Epoch: 44/100, Loss: 0.355, Tr_acc: 86.08, Val_acc: 86.09, best_e: -1\n",
      "Epoch: 45/100, Loss: 0.354, Tr_acc: 86.11, Val_acc: 86.08, best_e: -1\n",
      "Epoch: 46/100, Loss: 0.354, Tr_acc: 86.10, Val_acc: 86.20, best_e: -1\n",
      "Epoch: 47/100, Loss: 0.353, Tr_acc: 86.21, Val_acc: 86.22, best_e: -1\n",
      "Epoch: 48/100, Loss: 0.352, Tr_acc: 86.23, Val_acc: 86.23, best_e: -1\n",
      "Epoch: 49/100, Loss: 0.351, Tr_acc: 86.23, Val_acc: 86.28, best_e: -1\n",
      "Pruned 4/19 features\n",
      "Pruned 4/19 features\n",
      "Pruned features\n",
      "Epoch: 50/100, Loss: 0.351, Tr_acc: 86.25, Val_acc: 86.14, best_e: -1\n",
      "Epoch: 51/100, Loss: 0.331, Tr_acc: 86.14, Val_acc: 56.72, best_e: 51\n",
      "Epoch: 52/100, Loss: 0.723, Tr_acc: 56.97, Val_acc: 86.12, best_e: 52\n",
      "Epoch: 53/100, Loss: 0.340, Tr_acc: 86.06, Val_acc: 86.07, best_e: 52\n",
      "Epoch: 54/100, Loss: 0.447, Tr_acc: 86.07, Val_acc: 86.07, best_e: 52\n",
      "Epoch: 55/100, Loss: 0.449, Tr_acc: 86.07, Val_acc: 86.07, best_e: 52\n",
      "Epoch: 56/100, Loss: 0.393, Tr_acc: 86.07, Val_acc: 86.07, best_e: 52\n",
      "Epoch: 57/100, Loss: 0.359, Tr_acc: 86.07, Val_acc: 86.07, best_e: 52\n",
      "Epoch: 58/100, Loss: 0.353, Tr_acc: 86.07, Val_acc: 86.07, best_e: 52\n",
      "Epoch: 59/100, Loss: 0.360, Tr_acc: 86.07, Val_acc: 86.07, best_e: 52\n",
      "Epoch: 60/100, Loss: 0.366, Tr_acc: 86.07, Val_acc: 86.07, best_e: 52\n",
      "Epoch: 61/100, Loss: 0.364, Tr_acc: 86.07, Val_acc: 86.07, best_e: 52\n",
      "Epoch: 62/100, Loss: 0.358, Tr_acc: 86.07, Val_acc: 86.07, best_e: 52\n",
      "Epoch: 63/100, Loss: 0.353, Tr_acc: 86.07, Val_acc: 86.07, best_e: 52\n",
      "Epoch: 64/100, Loss: 0.347, Tr_acc: 86.07, Val_acc: 86.07, best_e: 52\n",
      "Epoch: 65/100, Loss: 0.340, Tr_acc: 86.07, Val_acc: 86.12, best_e: 65\n",
      "Epoch: 66/100, Loss: 0.336, Tr_acc: 86.14, Val_acc: 86.05, best_e: 65\n",
      "Epoch: 67/100, Loss: 0.337, Tr_acc: 86.09, Val_acc: 85.83, best_e: 65\n",
      "Epoch: 68/100, Loss: 0.342, Tr_acc: 85.83, Val_acc: 85.84, best_e: 65\n",
      "Epoch: 69/100, Loss: 0.344, Tr_acc: 85.86, Val_acc: 86.07, best_e: 65\n",
      "Epoch: 70/100, Loss: 0.343, Tr_acc: 86.04, Val_acc: 86.18, best_e: 70\n",
      "Epoch: 71/100, Loss: 0.340, Tr_acc: 86.19, Val_acc: 86.18, best_e: 71\n",
      "Epoch: 72/100, Loss: 0.336, Tr_acc: 86.18, Val_acc: 86.17, best_e: 71\n",
      "Epoch: 73/100, Loss: 0.334, Tr_acc: 86.21, Val_acc: 86.19, best_e: 73\n",
      "Epoch: 74/100, Loss: 0.333, Tr_acc: 86.21, Val_acc: 86.19, best_e: 74\n",
      "Epoch: 75/100, Loss: 0.334, Tr_acc: 86.22, Val_acc: 86.18, best_e: 74\n",
      "Epoch: 76/100, Loss: 0.335, Tr_acc: 86.22, Val_acc: 86.21, best_e: 76\n",
      "Epoch: 77/100, Loss: 0.336, Tr_acc: 86.23, Val_acc: 86.20, best_e: 76\n",
      "Epoch: 78/100, Loss: 0.336, Tr_acc: 86.21, Val_acc: 86.18, best_e: 76\n",
      "Epoch: 79/100, Loss: 0.335, Tr_acc: 86.18, Val_acc: 86.16, best_e: 76\n",
      "Epoch: 80/100, Loss: 0.334, Tr_acc: 86.21, Val_acc: 86.21, best_e: 80\n",
      "Epoch: 81/100, Loss: 0.333, Tr_acc: 86.24, Val_acc: 86.26, best_e: 81\n",
      "Epoch: 82/100, Loss: 0.332, Tr_acc: 86.25, Val_acc: 86.28, best_e: 82\n",
      "Epoch: 83/100, Loss: 0.331, Tr_acc: 86.28, Val_acc: 86.23, best_e: 82\n",
      "Epoch: 84/100, Loss: 0.331, Tr_acc: 86.29, Val_acc: 86.28, best_e: 82\n",
      "Epoch: 85/100, Loss: 0.331, Tr_acc: 86.27, Val_acc: 86.29, best_e: 85\n",
      "Epoch: 86/100, Loss: 0.331, Tr_acc: 86.27, Val_acc: 86.32, best_e: 86\n",
      "Epoch: 87/100, Loss: 0.331, Tr_acc: 86.29, Val_acc: 86.31, best_e: 86\n",
      "Epoch: 88/100, Loss: 0.330, Tr_acc: 86.32, Val_acc: 86.37, best_e: 88\n",
      "Epoch: 89/100, Loss: 0.329, Tr_acc: 86.32, Val_acc: 86.38, best_e: 89\n",
      "Epoch: 90/100, Loss: 0.329, Tr_acc: 86.34, Val_acc: 86.35, best_e: 89\n",
      "Epoch: 91/100, Loss: 0.328, Tr_acc: 86.35, Val_acc: 86.33, best_e: 89\n",
      "Epoch: 92/100, Loss: 0.328, Tr_acc: 86.36, Val_acc: 86.31, best_e: 89\n",
      "Epoch: 93/100, Loss: 0.327, Tr_acc: 86.36, Val_acc: 86.32, best_e: 89\n",
      "Epoch: 94/100, Loss: 0.327, Tr_acc: 86.36, Val_acc: 86.32, best_e: 89\n",
      "Epoch: 95/100, Loss: 0.327, Tr_acc: 86.35, Val_acc: 86.33, best_e: 89\n",
      "Epoch: 96/100, Loss: 0.326, Tr_acc: 86.35, Val_acc: 86.31, best_e: 89\n",
      "Epoch: 97/100, Loss: 0.326, Tr_acc: 86.32, Val_acc: 86.30, best_e: 89\n",
      "Epoch: 98/100, Loss: 0.325, Tr_acc: 86.33, Val_acc: 86.29, best_e: 89\n",
      "Epoch: 99/100, Loss: 0.325, Tr_acc: 86.32, Val_acc: 86.31, best_e: 89\n",
      "Epoch: 100/100, Loss: 0.325, Tr_acc: 86.37, Val_acc: 86.32, best_e: 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdum/PycharmProjects/URENA_MELENDEZ_PL/logic_explained_networks/lens/models/base.py:348: UserWarning: Loaded model does not have time or explanations. They need to be recalculated but time will only consider rule extraction time.\n",
      "  warnings.warn(\"Loaded model does not have time or explanations. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 53.39330816902639\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Explanation</th>\n",
       "      <th>Metric Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CholCheck &amp; PhysActivity &amp; Fruits &amp; Veggies &amp; ...</td>\n",
       "      <td>20.893652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Explanation  Metric Result\n",
       "0  CholCheck & PhysActivity & Fruits & Veggies & ...      20.893652"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# esto se toma su tiempo ejecutando\n",
    "X_tensor = torch.tensor(X_standardized, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "data = StructuredDataset(X_tensor, y_tensor, dataset_name=\"diabetes\",\n",
    "                         feature_names=X_filtered.columns, class_names=['not diabetes', 'diabetes'])\n",
    "\n",
    "\n",
    "\n",
    "train_len(data=data, print_conf_matrix=1, target_class=1, epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered.loc[:, 'BMI < 18'] = X_filtered['BMI'] < 18\n",
    "X_filtered.loc[:, '18 <= BMI < 25'] = (X_filtered['BMI'] >= 18) & (X_filtered['BMI'] < 25)\n",
    "X_filtered.loc[:, '25 <= BMI < 30'] = (X_filtered['BMI'] >= 25) & (X_filtered['BMI'] < 30)\n",
    "X_filtered.loc[:, 'BMI >= 30'] = X_filtered['BMI'] >= 30\n",
    "\n",
    "# Create boolean columns based on intervals for 'PhysHlth'\n",
    "X_filtered.loc[:, 'PhysHlth < 10'] = X_filtered['PhysHlth'] < 10\n",
    "X_filtered.loc[:, '10 <= PhysHlth <= 20'] = (X_filtered['PhysHlth'] >= 10) & (X_filtered['PhysHlth'] <= 20)\n",
    "X_filtered.loc[:, 'PhysHlth > 20'] = X_filtered['PhysHlth'] > 20\n",
    "\n",
    "# Create boolean columns based on intervals for 'MentHlth'\n",
    "X_filtered.loc[:, 'MentHlth < 10'] = X_filtered['MentHlth'] < 10\n",
    "X_filtered.loc[:, '10 <= MentHlth <= 20'] = (X_filtered['MentHlth'] >= 10) & (X_filtered['MentHlth'] <= 20)\n",
    "X_filtered.loc[:, 'MentHlth > 20'] = X_filtered['MentHlth'] > 20\n",
    "\n",
    "X_filtered = X_filtered.astype('float32')\n",
    "\n",
    "# Min-Max normalization for 'Age' and 'GenHlth'\n",
    "X_filtered.loc[:, 'Age'] = (X_filtered['Age'] - X_filtered['Age'].min()) / (X_filtered['Age'].max() - X_filtered['Age'].min())\n",
    "X_filtered.loc[:, 'GenHlth'] = (X_filtered['GenHlth'] - X_filtered['GenHlth'].min()) / (X_filtered['GenHlth'].max() - X_filtered['GenHlth'].min())\n",
    "\n",
    "\n",
    "X_filtered = X_filtered.drop(columns=['BMI', 'MentHlth', 'PhysHlth'])\n",
    "\n",
    "X_tensor = torch.tensor(X_filtered.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "data = StructuredDataset(X_tensor, y_tensor, dataset_name=\"diabetes\",\n",
    "                         feature_names=X_filtered.columns, class_names=['not diabetes', 'diabetes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_490547/1542033949.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
      "/tmp/ipykernel_490547/1542033949.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_data = TensorDataset(torch.tensor(x_val), torch.tensor(y_val))\n",
      "/tmp/ipykernel_490547/1542033949.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data = TensorDataset(torch.tensor(x_test), torch.tensor(y_test))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Loss: 0.612, Tr_acc: 85.60, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 2/100, Loss: 0.740, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 3/100, Loss: 0.475, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 4/100, Loss: 0.475, Tr_acc: 86.06, Val_acc: 86.08, best_e: -1\n",
      "Epoch: 5/100, Loss: 0.419, Tr_acc: 86.08, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 6/100, Loss: 0.391, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 7/100, Loss: 0.404, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 8/100, Loss: 0.402, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 9/100, Loss: 0.385, Tr_acc: 86.07, Val_acc: 86.10, best_e: -1\n",
      "Epoch: 10/100, Loss: 0.374, Tr_acc: 86.10, Val_acc: 86.09, best_e: -1\n",
      "Epoch: 11/100, Loss: 0.375, Tr_acc: 85.98, Val_acc: 86.22, best_e: -1\n",
      "Epoch: 12/100, Loss: 0.378, Tr_acc: 86.15, Val_acc: 86.11, best_e: -1\n",
      "Epoch: 13/100, Loss: 0.378, Tr_acc: 86.11, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 14/100, Loss: 0.374, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 15/100, Loss: 0.369, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 16/100, Loss: 0.367, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 17/100, Loss: 0.368, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 18/100, Loss: 0.369, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 19/100, Loss: 0.370, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 20/100, Loss: 0.368, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 21/100, Loss: 0.366, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 22/100, Loss: 0.365, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 23/100, Loss: 0.364, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 24/100, Loss: 0.364, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 25/100, Loss: 0.363, Tr_acc: 86.07, Val_acc: 86.07, best_e: -1\n",
      "Epoch: 26/100, Loss: 0.363, Tr_acc: 86.07, Val_acc: 86.08, best_e: -1\n",
      "Epoch: 27/100, Loss: 0.361, Tr_acc: 86.11, Val_acc: 86.14, best_e: -1\n",
      "Epoch: 28/100, Loss: 0.360, Tr_acc: 86.14, Val_acc: 86.17, best_e: -1\n",
      "Epoch: 29/100, Loss: 0.359, Tr_acc: 86.17, Val_acc: 86.12, best_e: -1\n",
      "Epoch: 30/100, Loss: 0.359, Tr_acc: 86.16, Val_acc: 86.22, best_e: -1\n",
      "Epoch: 31/100, Loss: 0.357, Tr_acc: 86.21, Val_acc: 86.23, best_e: -1\n",
      "Epoch: 32/100, Loss: 0.356, Tr_acc: 86.20, Val_acc: 86.13, best_e: -1\n",
      "Epoch: 33/100, Loss: 0.355, Tr_acc: 86.18, Val_acc: 86.11, best_e: -1\n",
      "Epoch: 34/100, Loss: 0.354, Tr_acc: 86.15, Val_acc: 86.32, best_e: -1\n",
      "Epoch: 35/100, Loss: 0.354, Tr_acc: 86.27, Val_acc: 86.13, best_e: -1\n",
      "Epoch: 36/100, Loss: 0.352, Tr_acc: 86.16, Val_acc: 86.17, best_e: -1\n",
      "Epoch: 37/100, Loss: 0.351, Tr_acc: 86.22, Val_acc: 86.37, best_e: -1\n",
      "Epoch: 38/100, Loss: 0.351, Tr_acc: 86.30, Val_acc: 86.25, best_e: -1\n",
      "Epoch: 39/100, Loss: 0.349, Tr_acc: 86.30, Val_acc: 86.37, best_e: -1\n",
      "Epoch: 40/100, Loss: 0.348, Tr_acc: 86.32, Val_acc: 86.34, best_e: -1\n",
      "Epoch: 41/100, Loss: 0.347, Tr_acc: 86.34, Val_acc: 86.30, best_e: -1\n",
      "Epoch: 42/100, Loss: 0.347, Tr_acc: 86.35, Val_acc: 86.20, best_e: -1\n",
      "Epoch: 43/100, Loss: 0.349, Tr_acc: 86.12, Val_acc: 86.24, best_e: -1\n",
      "Epoch: 44/100, Loss: 0.347, Tr_acc: 86.29, Val_acc: 86.32, best_e: -1\n",
      "Epoch: 45/100, Loss: 0.345, Tr_acc: 86.35, Val_acc: 86.18, best_e: -1\n",
      "Epoch: 46/100, Loss: 0.347, Tr_acc: 86.18, Val_acc: 86.39, best_e: -1\n",
      "Epoch: 47/100, Loss: 0.343, Tr_acc: 86.40, Val_acc: 86.30, best_e: -1\n",
      "Epoch: 48/100, Loss: 0.344, Tr_acc: 86.35, Val_acc: 86.41, best_e: -1\n",
      "Epoch: 49/100, Loss: 0.342, Tr_acc: 86.38, Val_acc: 86.38, best_e: -1\n",
      "Pruned 10/19 features\n",
      "Pruned 8/19 features\n",
      "Pruned features\n",
      "Epoch: 50/100, Loss: 0.343, Tr_acc: 86.32, Val_acc: 86.37, best_e: -1\n",
      "Epoch: 51/100, Loss: 0.327, Tr_acc: 86.30, Val_acc: 85.28, best_e: 51\n",
      "Epoch: 52/100, Loss: 0.418, Tr_acc: 85.28, Val_acc: 86.07, best_e: 52\n",
      "Epoch: 53/100, Loss: 0.358, Tr_acc: 86.07, Val_acc: 86.07, best_e: 53\n",
      "Epoch: 54/100, Loss: 0.384, Tr_acc: 86.07, Val_acc: 86.07, best_e: 54\n",
      "Epoch: 55/100, Loss: 0.360, Tr_acc: 86.07, Val_acc: 86.07, best_e: 55\n",
      "Epoch: 56/100, Loss: 0.338, Tr_acc: 86.07, Val_acc: 86.07, best_e: 56\n",
      "Epoch: 57/100, Loss: 0.338, Tr_acc: 86.07, Val_acc: 86.07, best_e: 57\n",
      "Epoch: 58/100, Loss: 0.344, Tr_acc: 86.07, Val_acc: 86.13, best_e: 58\n",
      "Epoch: 59/100, Loss: 0.345, Tr_acc: 86.15, Val_acc: 86.11, best_e: 58\n",
      "Epoch: 60/100, Loss: 0.341, Tr_acc: 86.13, Val_acc: 86.10, best_e: 58\n",
      "Epoch: 61/100, Loss: 0.336, Tr_acc: 86.13, Val_acc: 86.10, best_e: 58\n",
      "Epoch: 62/100, Loss: 0.341, Tr_acc: 86.14, Val_acc: 86.15, best_e: 62\n",
      "Epoch: 63/100, Loss: 0.339, Tr_acc: 86.14, Val_acc: 86.16, best_e: 63\n",
      "Epoch: 64/100, Loss: 0.335, Tr_acc: 86.15, Val_acc: 86.12, best_e: 63\n",
      "Epoch: 65/100, Loss: 0.333, Tr_acc: 86.12, Val_acc: 86.14, best_e: 63\n",
      "Epoch: 66/100, Loss: 0.333, Tr_acc: 86.15, Val_acc: 86.16, best_e: 66\n",
      "Epoch: 67/100, Loss: 0.333, Tr_acc: 86.16, Val_acc: 86.12, best_e: 66\n",
      "Epoch: 68/100, Loss: 0.332, Tr_acc: 86.16, Val_acc: 86.13, best_e: 66\n",
      "Epoch: 69/100, Loss: 0.331, Tr_acc: 86.14, Val_acc: 86.13, best_e: 66\n",
      "Epoch: 70/100, Loss: 0.331, Tr_acc: 86.14, Val_acc: 86.12, best_e: 66\n",
      "Epoch: 71/100, Loss: 0.330, Tr_acc: 86.15, Val_acc: 86.15, best_e: 66\n",
      "Epoch: 72/100, Loss: 0.330, Tr_acc: 86.17, Val_acc: 86.19, best_e: 72\n",
      "Epoch: 73/100, Loss: 0.329, Tr_acc: 86.15, Val_acc: 86.11, best_e: 72\n",
      "Epoch: 74/100, Loss: 0.328, Tr_acc: 86.09, Val_acc: 86.18, best_e: 72\n",
      "Epoch: 75/100, Loss: 0.328, Tr_acc: 86.11, Val_acc: 86.13, best_e: 72\n",
      "Epoch: 76/100, Loss: 0.327, Tr_acc: 86.16, Val_acc: 86.12, best_e: 72\n",
      "Epoch: 77/100, Loss: 0.327, Tr_acc: 86.12, Val_acc: 86.15, best_e: 72\n",
      "Epoch: 78/100, Loss: 0.326, Tr_acc: 86.18, Val_acc: 86.18, best_e: 72\n",
      "Epoch: 79/100, Loss: 0.326, Tr_acc: 86.20, Val_acc: 86.13, best_e: 72\n",
      "Epoch: 80/100, Loss: 0.325, Tr_acc: 86.14, Val_acc: 86.31, best_e: 80\n",
      "Epoch: 81/100, Loss: 0.325, Tr_acc: 86.25, Val_acc: 86.29, best_e: 80\n",
      "Epoch: 82/100, Loss: 0.324, Tr_acc: 86.25, Val_acc: 86.28, best_e: 80\n",
      "Epoch: 83/100, Loss: 0.324, Tr_acc: 86.29, Val_acc: 86.32, best_e: 83\n",
      "Epoch: 84/100, Loss: 0.323, Tr_acc: 86.33, Val_acc: 86.34, best_e: 84\n",
      "Epoch: 85/100, Loss: 0.323, Tr_acc: 86.29, Val_acc: 86.36, best_e: 85\n",
      "Epoch: 86/100, Loss: 0.322, Tr_acc: 86.35, Val_acc: 86.37, best_e: 86\n",
      "Epoch: 87/100, Loss: 0.322, Tr_acc: 86.42, Val_acc: 86.37, best_e: 87\n",
      "Epoch: 88/100, Loss: 0.321, Tr_acc: 86.34, Val_acc: 86.31, best_e: 87\n",
      "Epoch: 89/100, Loss: 0.321, Tr_acc: 86.38, Val_acc: 86.15, best_e: 87\n",
      "Epoch: 90/100, Loss: 0.322, Tr_acc: 86.23, Val_acc: 86.30, best_e: 87\n",
      "Epoch: 91/100, Loss: 0.324, Tr_acc: 86.37, Val_acc: 86.11, best_e: 87\n",
      "Epoch: 92/100, Loss: 0.321, Tr_acc: 86.26, Val_acc: 86.28, best_e: 87\n",
      "Epoch: 93/100, Loss: 0.321, Tr_acc: 86.32, Val_acc: 86.39, best_e: 93\n",
      "Epoch: 94/100, Loss: 0.321, Tr_acc: 86.42, Val_acc: 86.37, best_e: 93\n",
      "Epoch: 95/100, Loss: 0.321, Tr_acc: 86.44, Val_acc: 86.22, best_e: 93\n",
      "Epoch: 96/100, Loss: 0.320, Tr_acc: 86.28, Val_acc: 86.16, best_e: 93\n",
      "Epoch: 97/100, Loss: 0.320, Tr_acc: 86.23, Val_acc: 86.36, best_e: 93\n",
      "Epoch: 98/100, Loss: 0.320, Tr_acc: 86.39, Val_acc: 86.38, best_e: 93\n",
      "Epoch: 99/100, Loss: 0.320, Tr_acc: 86.43, Val_acc: 86.34, best_e: 93\n",
      "Epoch: 100/100, Loss: 0.319, Tr_acc: 86.38, Val_acc: 86.30, best_e: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdum/PycharmProjects/URENA_MELENDEZ_PL/logic_explained_networks/lens/models/base.py:348: UserWarning: Loaded model does not have time or explanations. They need to be recalculated but time will only consider rule extraction time.\n",
      "  warnings.warn(\"Loaded model does not have time or explanations. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 52.93913611704786\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgwAAAGwCAYAAADFZj2cAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABK+klEQVR4nO3de1wUVf8H8M8C7gLKLqICoogoXhNRsYjKC08EGl1MeyovSYaahaVYSpYX1BIffdQ0L1Sm2O/Rx0slFfqoSCkaaIHiLaVQFAwWLygrKLfd+f1BTG2oyzILCPN5v17zijlz5swZX8R+93vOmVEIgiCAiIiI6B6sGroDREREdP9jwEBEREQmMWAgIiIikxgwEBERkUkMGIiIiMgkBgxERERkEgMGIiIiMsmmoTsghcFgQG5uLhwcHKBQKBq6O0REZCZBEHDz5k24ubnByqruvsOWlJSgrKxMcjtKpRK2trYW6FHj06gDhtzcXLi7uzd0N4iISKKcnBy0b9++TtouKSmBp0cLaC/rJbfl6uqKrKwsWQYNjTpgcHBwAABcPNoR6hYcXaGmqXfcuIbuAlGdMZSU4Pc5H4p/z+tCWVkZtJf1uJjWEWqH2n9W6G4a4OF7AWVlZQwYGpuqYQh1CytJvwRE9zMrO/n9YSL5qY9h5RYOCrRwqP11DJD30HejDhiIiIhqSi8YoJfw9iS9YLBcZxohBgxERCQLBggwoPYRg5RzmwLm8YmIiMgkZhiIiEgWDDBAyqCCtLMbPwYMREQkC3pBgF6o/bCClHObAg5JEBERkUnMMBARkSxw0qM0DBiIiEgWDBCgZ8BQaxySICIiIpOYYSAiIlngkIQ0DBiIiEgWuEpCGg5JEBERkUnMMBARkSwY/tiknC9nDBiIiEgW9BJXSUg5tylgwEBERLKgFyDxbZWW60tjxDkMREREZBIzDEREJAucwyANAwYiIpIFAxTQQyHpfDnjkAQRERGZxAwDERHJgkGo3KScL2fMMBARkSzo/xiSkLKZIzo6Gg8++CAcHBzg7OyMYcOGISMjw6hOSUkJwsPD0apVK7Ro0QIjRoxAfn6+UZ3s7GyEhITA3t4ezs7OmD59OioqKozq7N+/H/369YNKpYKXlxdiY2Or9Wf16tXo2LEjbG1t4efnh59++sms+2HAQEREVAcOHDiA8PBwHD58GAkJCSgvL0dQUBCKi4vFOhEREfjuu++wfft2HDhwALm5uRg+fLh4XK/XIyQkBGVlZUhOTsbGjRsRGxuLOXPmiHWysrIQEhKCgIAApKenY+rUqRg/fjz27Nkj1tm6dSumTZuGuXPn4ujRo/Dx8UFwcDAuX75c4/tRCELjfTi2TqeDRqPB9V87Qe3A2Ieapk5fvtbQXSCqM4bbJciZMRuFhYVQq9V1co2qz4rk023RQsJnRdFNAx55IK/Wfb1y5QqcnZ1x4MABDBw4EIWFhWjTpg02b96M559/HgBw9uxZ9OjRAykpKXj44Yfxv//9D0899RRyc3Ph4uICAIiJiUFkZCSuXLkCpVKJyMhI7Ny5E6dOnRKv9dJLL+HGjRvYvXs3AMDPzw8PPvggVq1aBQAwGAxwd3fHm2++iXfffbdG/eenLBERyYJBUEjegMoA5K9baWlpja5fWFgIAHBycgIApKWloby8HIGBgWKd7t27o0OHDkhJSQEApKSkwNvbWwwWACA4OBg6nQ6nT58W6/y1jao6VW2UlZUhLS3NqI6VlRUCAwPFOjXBgIGIiMgM7u7u0Gg04hYdHW3yHIPBgKlTp+LRRx9Fr169AABarRZKpRKOjo5GdV1cXKDVasU6fw0Wqo5XHbtXHZ1Oh9u3b+Pq1avQ6/V3rFPVRk1wlQQREclCbSYu/v18AMjJyTEaklCpVCbPDQ8Px6lTp3Do0KFaX7+hMWAgIiJZ0MMKegmJdf0f/1Wr1WbNYZg8eTLi4+ORlJSE9u3bi+Wurq4oKyvDjRs3jLIM+fn5cHV1Fev8fTVD1SqKv9b5+8qK/Px8qNVq2NnZwdraGtbW1nesU9VGTXBIgoiIZEGQOH9BEMzLTgiCgMmTJ2PHjh34/vvv4enpaXTc19cXzZo1Q2JioliWkZGB7Oxs+Pv7AwD8/f1x8uRJo9UMCQkJUKvV6Nmzp1jnr21U1alqQ6lUwtfX16iOwWBAYmKiWKcmmGEgIiKqA+Hh4di8eTO++eYbODg4iPMFNBoN7OzsoNFoEBYWhmnTpsHJyQlqtRpvvvkm/P398fDDDwMAgoKC0LNnT7z88stYvHgxtFotZs2ahfDwcHEoZNKkSVi1ahVmzJiBV199Fd9//z22bduGnTt3in2ZNm0aQkND0b9/fzz00EP46KOPUFxcjHHjxtX4fhgwEBGRLFhqDkNNrV27FgAwePBgo/INGzbglVdeAQAsX74cVlZWGDFiBEpLSxEcHIw1a9aIda2trREfH4/XX38d/v7+aN68OUJDQzF//nyxjqenJ3bu3ImIiAisWLEC7du3x7p16xAcHCzWefHFF3HlyhXMmTMHWq0Wffr0we7du6tNhLwXPoeB6D7H5zBQU1afz2H43wlPNJfwWVF804ChvbPqtK/3M37KEhERkUkckiAiIlkwQAGDhO/JBjTahLxFMGAgIiJZqO85DE0NhySIiIjIJGYYiIhIFvSCFfSChAc3Nd41AhbBgIGIiGShcg5D7YcVpJzbFHBIgoiIiExihoGIiGTBIPFdElwlQUREJAOcwyANAwYiIpIFA6z4HAYJOIeBiIiITGKGgYiIZEEvKKA38xXVfz9fzhgwEBGRLOglTnrUc0iCiIiI6N6YYSAiIlkwCFYwSFglYeAqCSIioqaPQxLScEiCiIiITGKGgYiIZMEAaSsdDJbrSqPEgIGIiGRB+oOb5J2Ul/fdExERUY0ww0BERLIg/V0S8v6OzYCBiIhkwQAFDJAyh4FPeiQiImrymGGQRt53T0RERDXCDAMREcmC9Ac3yfs7NgMGIiKSBYOggEHKcxhk/rZKeYdLREREVCPMMBARkSwYJA5JyP3BTQwYiIhIFqS/rVLeAYO8756IiIhqhBkGIiKSBT0U0Et4+JKUc5sCBgxERCQLHJKQRt53T0RERDXCgIGIiGRBjz+HJWq3mScpKQlPP/003NzcoFAoEBcXZ3RcoVDccVuyZIlYp2PHjtWOL1q0yKidEydOYMCAAbC1tYW7uzsWL15crS/bt29H9+7dYWtrC29vb+zatcvMu2HAQEREMlE1JCFlM0dxcTF8fHywevXqOx7Py8sz2tavXw+FQoERI0YY1Zs/f75RvTfffFM8ptPpEBQUBA8PD6SlpWHJkiWIiorCp59+KtZJTk7GyJEjERYWhmPHjmHYsGEYNmwYTp06Zdb9cA4DERHJgqVePqXT6YzKVSoVVCpVtfpDhw7F0KFD79qeq6ur0f4333yDgIAAdOrUyajcwcGhWt0qmzZtQllZGdavXw+lUokHHngA6enpWLZsGSZOnAgAWLFiBYYMGYLp06cDABYsWICEhASsWrUKMTExJu76T8wwEBERmcHd3R0ajUbcoqOjJbeZn5+PnTt3IiwsrNqxRYsWoVWrVujbty+WLFmCiooK8VhKSgoGDhwIpVIplgUHByMjIwPXr18X6wQGBhq1GRwcjJSUFLP6yAwDERHJggAFDBKWRgp/nJuTkwO1Wi2W3ym7YK6NGzfCwcEBw4cPNyp/66230K9fPzg5OSE5ORkzZ85EXl4eli1bBgDQarXw9PQ0OsfFxUU81rJlS2i1WrHsr3W0Wq1ZfWTAQEREsmCpIQm1Wm0UMFjC+vXrMXr0aNja2hqVT5s2Tfy5d+/eUCqVeO211xAdHW2RQMUcHJIgIiJqQAcPHkRGRgbGjx9vsq6fnx8qKipw4cIFAJXzIPLz843qVO1XzXu4W527zYu4GwYMREQkC1Wvt5ay1YXPP/8cvr6+8PHxMVk3PT0dVlZWcHZ2BgD4+/sjKSkJ5eXlYp2EhAR069YNLVu2FOskJiYatZOQkAB/f3+z+skhCSIikgW9xLdVmntuUVERMjMzxf2srCykp6fDyckJHTp0AFC54mL79u1YunRptfNTUlJw5MgRBAQEwMHBASkpKYiIiMCYMWPEYGDUqFGYN28ewsLCEBkZiVOnTmHFihVYvny52M6UKVMwaNAgLF26FCEhIdiyZQtSU1ONll7WBAMGIiKiOpCamoqAgABxv2o+QmhoKGJjYwEAW7ZsgSAIGDlyZLXzVSoVtmzZgqioKJSWlsLT0xMRERFG8xo0Gg327t2L8PBw+Pr6onXr1pgzZ464pBIAHnnkEWzevBmzZs3Ce++9hy5duiAuLg69evUy634UgiAIZp1xH9HpdNBoNLj+ayeoHTi6Qk1Tpy9fa+guENUZw+0S5MyYjcLCQotPJKxS9Vnx1qFnoWrRrNbtlBaVY+Vj39RpX+9nzDAQEZEsGGAFg4QhCSnnNgXyvnsiIiKqEWYYiIhIFvSCAnoJKx2knNsUMGAgIiJZkLo0sq6WVTYWDBiIiEgWhFq8cfLv58uZvO+eiIiIaoQZBiIikgU9FNBLePmUlHObAgYMREQkCwZB2jwEQ6N9apFlcEiCiIiITGKGoQnb8rEzftzliJxMFZS2BvTsfwth7+fC3atUrLPrP63ww46WyDxph1tF1vjqzEm00OiN2hn7UE/kX1Ialb06MxcvvnkZAFBWosDKd93x2wk7ZP9mC79AHaI2ZFXrz/dft8S2Nc7IPa9Cc7Ue/QN0mDA7F2onfbW6RDVlm6lDy8Q82OYUw0ZXjtzxXVDc20k83vx4ATSH8mGbcwvWtypwcUYvlLVvLh63Kq5Aq/9dgv3ZQthcL4W+RTMUe7fEtZD2MNjZ/FGnHK4bz0GVewtWxRXQO/xR56k/6zQ/XgDHQ/lQXroFRYUBZW3tUTC0HW71cKzXfw+6O4PESY9Szm0KGDA0YSdSWuDpV66ia59b0FcAsYva4r2RnfHZgbOwtTcAAEpuW6H/YB36D9ZhfbTbXdsaOz0PQ0dfE/ftWxjEnw0GBZS2BjwbdgWHdjre8fzTPzXHkrc64LWo3/FwkA5X85ph5bvt8dF0d8z5/IJF7pfkyarMgLJ29tA93AZun/9W/XipHiWdHFDUtxVctlQPZG0Ky2BTWIarz3ZAmasdbK6XwnnrBVgXlkEb1rWykkIhBgj6Fs3Q7EoJnLdfgPWtCmhDvQAAdpk3caubBlefcofBzgbqI1fg9umvyJn2AErdm1e7LtU/AxQwSJiHIOXcpuC+CBhWr16NJUuWQKvVwsfHBx9//DEeeuihhu5Wo7dw83mj/bc/ysaL3t747YQdvB8uBgAMn3AFAHA8ucU927JrYYCTc8Udj9naG/DWoksAgF9+boGiQutqdX5Js4eLexmGjb8KAHDtUIaQMdewbY2zeTdF9De3ejriVk/Hux6/+VAbAIDNtdI7Hi9zs0deVWAAoLyNLa491R4uX5wD9AJgrYDB3gaFA1zEOhVOKtwY4IKWiXli2dURHkbtXnvaHc1PXkfzU9cZMFCT0OD5la1bt2LatGmYO3cujh49Ch8fHwQHB+Py5csN3bUmp1hX+UHu4Gj+EMC2Vc54/oFeeOOJrti+pg30d44d7qqn7y1cyW2GnxIdIAjA9Ss2OLjTEQ/+Q2d2X4jqmtVtPQy21oD1nb9RWheWocXxAtz2crh7IwYBVqV66JvfF9/LCH8+6VHKJmcN/pu8bNkyTJgwAePGjQMAxMTEYOfOnVi/fj3efffdBu5d02EwADFz2+GBB4vQsXuJWec+G3YFXt634eBYgV9Sm2NDdFsUXG6G16Jya9zGAw8VI3LVRSyc1BFlpVbQVyjw8BOFmLzwkrm3QlSnrIrK4bTnd+gerZ79co3NRPOT12FVbkBRL0dcHtnpru20/D4PVqV6FPVtVZfdJTNwDoM0DRowlJWVIS0tDTNnzhTLrKysEBgYiJSUlGr1S0tLUVr6Z1pRp+O305pa9V57XDxrh6Vx1cd4TRnx2hXx5049S9CsmYAVke4YNzMPSlXN1hld/FWFtXPaY3SEFr6Db6LgcjOsW+CGlZHumLYsx+w+EdUFq9sVaPdJBspc7XBtaLtqx68M74BrQ9tBebkErb7LQesdF3HlBc9q9RxSr8Jp9+/IndAVeofav06Z6H7SoOHS1atXodfr4eLiYlTu4uICrVZbrX50dDQ0Go24ubu711dXG7VV77XDkQQ1Fn+ZiTZu5ZLb69bvFvQVCuTnKE1X/sPWj13wwIPF+OcbV9CpZwn6D76JyQsvYc+WVriW3+CJLiIoSvRwW5sBg8oaeeO7AtbV/zzq1UqUu9ih2LslLr/oCcdDl2FdWGZUp0XaNTj/Nwt547xwu5umvrpPNWCAQnyfRK02mU96bFT5lZkzZ6KwsFDccnL4zfReBKEyWEjercHi7Zlw7VBm+qQaOH/aDlZWAhxb13wiQ8ltKygUxtkIK+s/9mX+MBRqeFa3K9BuzVkINgrkTuwKoVkN/jQKlb+4ioo/f4FbpF2Fy+Zz0IZ2xq0HWtZVd6mWhD9WSdR2E2QeMDToV7vWrVvD2toa+fn5RuX5+flwdXWtVl+lUkGlUtVX9xq9Ve+1xw87WiJqw3nYtTCg4PIf68Ud9FDZVf6RK7hsg+uXmyE3qzJbkHXWFvbNDWjTrgzqlnr8kmqPs8eaw+eRm7BvYcCZtOaImeuGf4y4bjR58uKvKlSUWeHmdWvcKrbCuVN2AIDOvW4DAB5+QoePprvju41F6D/4JgrymyFmbjt061uMVq5mzqAk+gtFqR7Nrvw5L6fZtVIoLxXDYG+DCicVrIorYHO9FDaFldk15eXKunp1M+jVSljdroDbmrOwKjcg7+WusCrRAyWVv9v6Fs0AKwXsT9+Azc1ylHRoDoPKGkrtLbSOy8btTi1Q0aryb5JD6lW4/Oc8rozwQEnHFrDWVQboQjMr8VkN1LD4tkppGvS3WKlUwtfXF4mJiRg2bBgAwGAwIDExEZMnT27IrjUJ8RtbAwCmj+hiVP728mwEvVgAANj5RWv8Z9mfwdk7z3UxqtNMKeDAN474z1JXlJcp4OpehuETr2D4xCtGbc4e09no4U5vBHUDAOzJTQcABL1YgNtFVvh2Q2t8Nq8dmmv06PPoTYS9nwciKWyzi9H+4zPifpsd2QAA3UOtkT+mM5qfug7XTX8uMW4bmwkAuDakHQqebA/VpVuwu1i5zLjjguNGbWfN7YOKVioIzRRQJ19G6x23oagwoMJRhSKflrge+OezS9TJl6EwCHDefgHO2y+I5VX9IGrsFIIgNGhCeOvWrQgNDcUnn3yChx56CB999BG2bduGs2fPVpvb8Hc6nQ4ajQbXf+0EtUOjGl0hqrFOX77W0F0gqjOG2yXImTEbhYWFUKvVdXKNqs+K5xLGoVnzms+9+rvy4jLseGJDnfb1ftbgebIXX3wRV65cwZw5c6DVatGnTx/s3r3bZLBARERkDg5JSNPgAQMATJ48mUMQRERE97H7ImAgIiKqa3yXhDQMGIiISBY4JCENZwoSERGRScwwEBGRLDDDIA0DBiIikgUGDNJwSIKIiIhMYoaBiIhkgRkGaRgwEBGRLAiQtjRS7u/JY8BARESywAyDNJzDQERERCYxYCAiIlmoyjBI2cyRlJSEp59+Gm5ublAoFIiLizM6/sorr0ChUBhtQ4YMMapTUFCA0aNHQ61Ww9HREWFhYSgqKjKqc+LECQwYMAC2trZwd3fH4sWLq/Vl+/bt6N69O2xtbeHt7Y1du3aZdS8AAwYiIpKJ+g4YiouL4ePjg9WrV9+1zpAhQ5CXlydu//3vf42Ojx49GqdPn0ZCQgLi4+ORlJSEiRMnisd1Oh2CgoLg4eGBtLQ0LFmyBFFRUfj000/FOsnJyRg5ciTCwsJw7NgxDBs2DMOGDcOpU6fMuh/OYSAiIqoDQ4cOxdChQ+9ZR6VSwdXV9Y7Hzpw5g927d+Pnn39G//79AQAff/wxnnzySfz73/+Gm5sbNm3ahLKyMqxfvx5KpRIPPPAA0tPTsWzZMjGwWLFiBYYMGYLp06cDABYsWICEhASsWrUKMTExNb4fZhiIiEgWLJVh0Ol0RltpaWmt+7R//344OzujW7dueP3113Ht2jXxWEpKChwdHcVgAQACAwNhZWWFI0eOiHUGDhwIpVIp1gkODkZGRgauX78u1gkMDDS6bnBwMFJSUszqKwMGIiKSBUFQSN4AwN3dHRqNRtyio6Nr1Z8hQ4bgiy++QGJiIv71r3/hwIEDGDp0KPR6PQBAq9XC2dnZ6BwbGxs4OTlBq9WKdVxcXIzqVO2bqlN1vKY4JEFERGSGnJwcqNVqcV+lUtWqnZdeekn82dvbG71790bnzp2xf/9+PP7445L7aWnMMBARkSwYoJC8AYBarTbaahsw/F2nTp3QunVrZGZmAgBcXV1x+fJlozoVFRUoKCgQ5z24uroiPz/fqE7Vvqk6d5s7cTcMGIiISBbqe5WEuS5duoRr166hbdu2AAB/f3/cuHEDaWlpYp3vv/8eBoMBfn5+Yp2kpCSUl5eLdRISEtCtWze0bNlSrJOYmGh0rYSEBPj7+5vVPwYMREREdaCoqAjp6elIT08HAGRlZSE9PR3Z2dkoKirC9OnTcfjwYVy4cAGJiYl49tln4eXlheDgYABAjx49MGTIEEyYMAE//fQTfvzxR0yePBkvvfQS3NzcAACjRo2CUqlEWFgYTp8+ja1bt2LFihWYNm2a2I8pU6Zg9+7dWLp0Kc6ePYuoqCikpqZi8uTJZt0PAwYiIpIFS016rKnU1FT07dsXffv2BQBMmzYNffv2xZw5c2BtbY0TJ07gmWeeQdeuXREWFgZfX18cPHjQaIhj06ZN6N69Ox5//HE8+eSTeOyxx4yesaDRaLB3715kZWXB19cXb7/9NubMmWP0rIZHHnkEmzdvxqeffgofHx98+eWXiIuLQ69evcy6H4UgCI32fRo6nQ4ajQbXf+0EtQNjH2qaOn35WkN3gajOGG6XIGfGbBQWFhpNJLSkqs+K/l9PhU3z2s83qCguRerwj+q0r/czrpIgIiJZqE2W4O/nyxm/lhMREZFJzDAQEZEsCBJXOsg9w8CAgYiIZEEAIGXWXqOd8GchHJIgIiIik5hhICIiWTBAAQVqP6xgkHBuU8CAgYiIZIGrJKThkAQRERGZxAwDERHJgkFQQCEhS1DX75K43zFgICIiWRAEiaskZL5MgkMSREREZBIzDEREJAuc9CgNAwYiIpIFBgzSMGAgIiJZ4KRHaTiHgYiIiExihoGIiGSBqySkYcBARESyUBkwSJnDYMHONEIckiAiIiKTmGEgIiJZ4CoJaRgwEBGRLAh/bFLOlzMOSRAREZFJzDAQEZEscEhCGgYMREQkDxyTkIQBAxERyYPEDANknmHgHAYiIiIyiRkGIiKSBT7pURoGDEREJAuc9CgNhySIiIjIJGYYiIhIHgSFtImLMs8wMGAgIiJZ4BwGaTgkQURERCYxw0BERPLABzdJwoCBiIhkgaskpKnRkMS3335b442IiIiApKQkPP3003Bzc4NCoUBcXJx4rLy8HJGRkfD29kbz5s3h5uaGsWPHIjc316iNjh07QqFQGG2LFi0yqnPixAkMGDAAtra2cHd3x+LFi6v1Zfv27ejevTtsbW3h7e2NXbt2mX0/NcowDBs2rEaNKRQK6PV6sztBRERUL+pxWKG4uBg+Pj549dVXMXz4cKNjt27dwtGjRzF79mz4+Pjg+vXrmDJlCp555hmkpqYa1Z0/fz4mTJgg7js4OIg/63Q6BAUFITAwEDExMTh58iReffVVODo6YuLEiQCA5ORkjBw5EtHR0XjqqaewefNmDBs2DEePHkWvXr1qfD81ChgMBkONGyQiIrof1feQxNChQzF06NA7HtNoNEhISDAqW7VqFR566CFkZ2ejQ4cOYrmDgwNcXV3v2M6mTZtQVlaG9evXQ6lU4oEHHkB6ejqWLVsmBgwrVqzAkCFDMH36dADAggULkJCQgFWrViEmJqbG9yNplURJSYmU04mIiOqPYIENld/q/7qVlpZapHuFhYVQKBRwdHQ0Kl+0aBFatWqFvn37YsmSJaioqBCPpaSkYODAgVAqlWJZcHAwMjIycP36dbFOYGCgUZvBwcFISUkxq39mBwx6vR4LFixAu3bt0KJFC5w/fx4AMHv2bHz++efmNkdERNSouLu7Q6PRiFt0dLTkNktKShAZGYmRI0dCrVaL5W+99Ra2bNmCH374Aa+99hoWLlyIGTNmiMe1Wi1cXFyM2qra12q196xTdbymzF4l8eGHH2Ljxo1YvHix0ZhKr1698NFHHyEsLMzcJomIiOqB4o9NyvlATk6O0Ye6SqWS1Kvy8nK88MILEAQBa9euNTo2bdo08efevXtDqVTitddeQ3R0tOTrmsvsDMMXX3yBTz/9FKNHj4a1tbVY7uPjg7Nnz1q0c0RERBZjoSEJtVpttEn54K4KFi5evIiEhASjQORO/Pz8UFFRgQsXLgAAXF1dkZ+fb1Snar9q3sPd6txtXsTdmB0w/P777/Dy8qpWbjAYUF5ebm5zREREslQVLPz222/Yt28fWrVqZfKc9PR0WFlZwdnZGQDg7++PpKQko8/fhIQEdOvWDS1bthTrJCYmGrWTkJAAf39/s/pr9pBEz549cfDgQXh4eBiVf/nll+jbt6+5zREREdWPen7SY1FRETIzM8X9rKwspKenw8nJCW3btsXzzz+Po0ePIj4+Hnq9XpxT4OTkBKVSiZSUFBw5cgQBAQFwcHBASkoKIiIiMGbMGDEYGDVqFObNm4ewsDBERkbi1KlTWLFiBZYvXy5ed8qUKRg0aBCWLl2KkJAQbNmyBampqfj000/Nuh+zA4Y5c+YgNDQUv//+OwwGA77++mtkZGTgiy++QHx8vLnNERER1Y96fltlamoqAgICxP2q+QihoaGIiooSH3bYp08fo/N++OEHDB48GCqVClu2bEFUVBRKS0vh6emJiIgIo3kNGo0Ge/fuRXh4OHx9fdG6dWvMmTNHXFIJAI888gg2b96MWbNm4b333kOXLl0QFxdn1jMYAEAhCOa/f+vgwYOYP38+jh8/jqKiIvTr1w9z5sxBUFCQuU1JotPpoNFocP3XTlA78D1a1DR1+vK1hu4CUZ0x3C5BzozZKCwsNDl+X1tVnxXuq+fBys621u0YbpcgJ3xunfb1flard0kMGDCg2gMniIiI7md8vbU0tX75VGpqKs6cOQOgcl6Dr6+vxTpFRERkcXxbpSRmBwyXLl3CyJEj8eOPP4pPo7px4wYeeeQRbNmyBe3bt7d0H4mIiKiBmT3wP378eJSXl+PMmTMoKChAQUEBzpw5A4PBgPHjx9dFH4mIiKSrmvQoZZMxszMMBw4cQHJyMrp16yaWdevWDR9//DEGDBhg0c4RERFZikKo3KScL2dmBwzu7u53fECTXq+Hm5ubRTpFRERkcZzDIInZQxJLlizBm2++afS+7tTUVEyZMgX//ve/Ldo5IiIiuj/UKMPQsmVLKBR/jt0UFxfDz88PNjaVp1dUVMDGxgavvvoqhg0bVicdJSIikqSeH9zU1NQoYPjoo4/quBtERER1jEMSktQoYAgNDa3rfhAREdF9rNYPbgKAkpISlJWVGZXJ8XGZRETUCDDDIInZkx6Li4sxefJkODs7o3nz5mjZsqXRRkREdF8SLLDJmNkBw4wZM/D9999j7dq1UKlUWLduHebNmwc3Nzd88cUXddFHIiIiamBmD0l89913+OKLLzB48GCMGzcOAwYMgJeXFzw8PLBp0yaMHj26LvpJREQkDVdJSGJ2hqGgoACdOnUCUDlfoaCgAADw2GOPISkpybK9IyIispCqJz1K2eTM7IChU6dOyMrKAgB0794d27ZtA1CZeah6GRURERE1LWYHDOPGjcPx48cBAO+++y5Wr14NW1tbREREYPr06RbvIBERkUVw0qMkZs9hiIiIEH8ODAzE2bNnkZaWBi8vL/Tu3duinSMiIqL7g6TnMACAh4cHPDw8LNEXIiKiOqOAxLdVWqwnjVONAoaVK1fWuMG33nqr1p0hIiKi+1ONAobly5fXqDGFQtEgAUPIG2NhY2Nb79clqg9d9hxp6C4Q1ZkKoRw59XUxLquUpEYBQ9WqCCIiokaLj4aWxOxVEkRERCQ/kic9EhERNQrMMEjCgIGIiGRB6tMa+aRHIiIiIhOYYSAiInngkIQktcowHDx4EGPGjIG/vz9+//13AMD//d//4dChQxbtHBERkcXw0dCSmB0wfPXVVwgODoadnR2OHTuG0tJSAEBhYSEWLlxo8Q4SERFRwzM7YPjggw8QExODzz77DM2aNRPLH330URw9etSinSMiIrIUvt5aGrPnMGRkZGDgwIHVyjUaDW7cuGGJPhEREVken/QoidkZBldXV2RmZlYrP3ToEDp16mSRThEREVkc5zBIYnbAMGHCBEyZMgVHjhyBQqFAbm4uNm3ahHfeeQevv/56XfSRiIiIGpjZAcO7776LUaNG4fHHH0dRUREGDhyI8ePH47XXXsObb75ZF30kIiKSrL7nMCQlJeHpp5+Gm5sbFAoF4uLijI4LgoA5c+agbdu2sLOzQ2BgIH777TejOgUFBRg9ejTUajUcHR0RFhaGoqIiozonTpzAgAEDYGtrC3d3dyxevLhaX7Zv347u3bvD1tYW3t7e2LVrl3k3g1oEDAqFAu+//z4KCgpw6tQpHD58GFeuXMGCBQvMvjgREVG9qechieLiYvj4+GD16tV3PL548WKsXLkSMTExOHLkCJo3b47g4GCUlJSIdUaPHo3Tp08jISEB8fHxSEpKwsSJE8XjOp0OQUFB8PDwQFpaGpYsWYKoqCh8+umnYp3k5GSMHDkSYWFhOHbsGIYNG4Zhw4bh1KlTZt2PQhCERjsqo9PpoNFo8EhgFF9vTU2Wck9qQ3eBqM5UCOXYj29QWFgItVpdJ9eo+qzoNGchrGxr/1lhKCnB+fnvIScnx6ivKpUKKpXqnucqFArs2LEDw4YNA1CZXXBzc8Pbb7+Nd955B0Dl4wlcXFwQGxuLl156CWfOnEHPnj3x888/o3///gCA3bt348knn8SlS5fg5uaGtWvX4v3334dWq4VSqQRQORIQFxeHs2fPAgBefPFFFBcXIz4+XuzPww8/jD59+iAmJqbG9292hiEgIAD/+Mc/7roRERHdl6QOR/zx9drd3R0ajUbcoqOjze5KVlYWtFotAgMDxTKNRgM/Pz+kpKQAAFJSUuDo6CgGCwAQGBgIKysrHDlyRKwzcOBAMVgAgODgYGRkZOD69etinb9ep6pO1XVqyuxllX369DHaLy8vR3p6Ok6dOoXQ0FBzmyMiIqofFno09J0yDObSarUAABcXF6NyFxcX8ZhWq4Wzs7PRcRsbGzg5ORnV8fT0rNZG1bGWLVtCq9Xe8zo1ZXbAsHz58juWR0VFVZuIQURE1NSo1eo6Gz65n1nsbZVjxozB+vXrLdUcERGRZd1Hz2FwdXUFAOTn5xuV5+fni8dcXV1x+fJlo+MVFRUoKCgwqnOnNv56jbvVqTpeUxYLGFJSUmArYTIJERFRXbqfHg3t6ekJV1dXJCYmimU6nQ5HjhyBv78/AMDf3x83btxAWlqaWOf777+HwWCAn5+fWCcpKQnl5eVinYSEBHTr1g0tW7YU6/z1OlV1qq5TU2YPSQwfPtxoXxAE5OXlITU1FbNnzza3OSIioiapqKjI6MnIWVlZSE9Ph5OTEzp06ICpU6figw8+QJcuXeDp6YnZs2fDzc1NXEnRo0cPDBkyBBMmTEBMTAzKy8sxefJkvPTSS3BzcwMAjBo1CvPmzUNYWBgiIyNx6tQprFixwmj6wJQpUzBo0CAsXboUISEh2LJlC1JTU42WXtaE2QGDRqMx2reyskK3bt0wf/58BAUFmdscERFRk5SamoqAgABxf9q0aQCA0NBQxMbGYsaMGSguLsbEiRNx48YNPPbYY9i9e7dRtn7Tpk2YPHkyHn/8cVhZWWHEiBFYuXKleFyj0WDv3r0IDw+Hr68vWrdujTlz5hg9q+GRRx7B5s2bMWvWLLz33nvo0qUL4uLi0KtXL7Pux6znMOj1evz444/w9vYWUx0Nic9hIDngcxioKavP5zB0nrkQ1hKGzvUlJTgX/V6d9vV+ZtYcBmtrawQFBfGtlERE1OjcT3MYGiOzJz326tUL58+fr4u+EBER0X3K7IDhgw8+wDvvvIP4+Hjk5eVBp9MZbURERPet+2BJZWNV40mP8+fPx9tvv40nn3wSAPDMM89AoVCIxwVBgEKhgF6vt3wviYiIpLLQkx7lqsYBw7x58zBp0iT88MMPddkfIiIiug/VOGCoWkwxaNCgOusMERFRXZE6cVHukx7Neg7DX4cgiIiIGhUOSUhiVsDQtWtXk0FDQUGBpA4RERHR/cesgGHevHnVnvRIRETUGHBIQhqzAoaXXnqp2ru5iYiIGgUOSUhS4+cwcP4CERGRfJm9SoKIiKhRYoZBkhoHDAaDoS77QUREVKc4h0Eas19vTURE1CgxwyCJ2e+SICIiIvlhhoGIiOSBGQZJGDAQEZEscA6DNBySICIiIpOYYSAiInngkIQkDBiIiEgWOCQhDYckiIiIyCRmGIiISB44JCEJAwYiIpIHBgyScEiCiIiITGKGgYiIZEHxxyblfDljwEBERPLAIQlJGDAQEZEscFmlNJzDQERERCYxw0BERPLAIQlJGDAQEZF8yPxDXwoOSRAREZFJzDAQEZEscNKjNMwwEBGRPAgW2MzQsWNHKBSKalt4eDgAYPDgwdWOTZo0yaiN7OxshISEwN7eHs7Ozpg+fToqKiqM6uzfvx/9+vWDSqWCl5cXYmNjzetoDTHDQEREVAd+/vln6PV6cf/UqVN44okn8M9//lMsmzBhAubPny/u29vbiz/r9XqEhITA1dUVycnJyMvLw9ixY9GsWTMsXLgQAJCVlYWQkBBMmjQJmzZtQmJiIsaPH4+2bdsiODjYovfDgIGIiGShvock2rRpY7S/aNEidO7cGYMGDRLL7O3t4erqesfz9+7di19++QX79u2Di4sL+vTpgwULFiAyMhJRUVFQKpWIiYmBp6cnli5dCgDo0aMHDh06hOXLl1s8YOCQBBERyYOFhiR0Op3RVlpaavLSZWVl+M9//oNXX30VCsWfD5netGkTWrdujV69emHmzJm4deuWeCwlJQXe3t5wcXERy4KDg6HT6XD69GmxTmBgoNG1goODkZKSYs6/TI0ww0BERGQGd3d3o/25c+ciKirqnufExcXhxo0beOWVV8SyUaNGwcPDA25ubjhx4gQiIyORkZGBr7/+GgCg1WqNggUA4r5Wq71nHZ1Oh9u3b8POzq42t3hHDBiIiEgWLDUkkZOTA7VaLZarVCqT537++ecYOnQo3NzcxLKJEyeKP3t7e6Nt27Z4/PHHce7cOXTu3Ln2Ha0jHJIgIiJ5sNCQhFqtNtpMBQwXL17Evn37MH78+HvW8/PzAwBkZmYCAFxdXZGfn29Up2q/at7D3eqo1WqLZhcABgxERCQX9bysssqGDRvg7OyMkJCQe9ZLT08HALRt2xYA4O/vj5MnT+Ly5ctinYSEBKjVavTs2VOsk5iYaNROQkIC/P39a9fZe2DAQEREVEcMBgM2bNiA0NBQ2Nj8OQvg3LlzWLBgAdLS0nDhwgV8++23GDt2LAYOHIjevXsDAIKCgtCzZ0+8/PLLOH78OPbs2YNZs2YhPDxczGpMmjQJ58+fx4wZM3D27FmsWbMG27ZtQ0REhMXvhQEDERHJQtUcBimbufbt24fs7Gy8+uqrRuVKpRL79u1DUFAQunfvjrfffhsjRozAd999J9axtrZGfHw8rK2t4e/vjzFjxmDs2LFGz23w9PTEzp07kZCQAB8fHyxduhTr1q2z+JJKgJMeiYhILhrgbZVBQUEQhOonuru748CBAybP9/DwwK5du+5ZZ/DgwTh27Jj5nTMTMwxERERkEjMMREQkCwpBgOIO3/bNOV/OGDAQEZE8NMCQRFPCIQkiIiIyiRkGIiKShfp++VRTw4CBiIjkgUMSknBIgoiIiExihoGIiGSBQxLSMGAgIiJ54JCEJAwYiIhIFphhkIZzGIiIiMgkZhiIiEgeOCQhCQMGIiKSDbkPK0jBIQkiIiIyiRkGIiKSB0Go3KScL2MMGIiISBa4SkIaDkkQERGRScwwEBGRPHCVhCQMGIiISBYUhspNyvlyxiEJIiIiMokZBpkb+eRxTPxnKr7c+wBW//dhAIBbGx0mvfgTvLvmo5mNHj+fbI+Vm/xxXWdX7fxmNnqsmf0tvDoUYPycYTiX0woA4NMtD/8MPoXunldgb1eO3/PV2Po/b+w77FWv90f01NirCBl7DS7uZQCAixm22LTcBak/qMU6PXyL8UqkFt373YJeD5w/bYf3RnVCWUnldyoHxwq88cHv8HtCB8EAHNrliLWz3VByy7pB7olqiUMSkjBgkLFunlfw9OCzOJftJJbZKsux+J3dOJfjhGmLhwIAXn0uDR9O2YvwD56BICiM2njthZ9w9YY9vDoUGJX38srHuRwn/HdXb1wvtIN/n2y8OyEJRbeVOHy8Q93fHNEfruQ1w/qFbfF7lgoKBfDEPwsQteECwoO64uKvtujhW4wPN53HllXOWDOrHfR6oFPPEgh/ST9HrsqGk0s5Zr7UCTbNBLy9LAdTl1zConCPhrsxMhtXSUjToEMSSUlJePrpp+Hm5gaFQoG4uLiG7I6s2KrK8f7E/fh37GO4eUsplvfqkg/X1kX417qByLrkhKxLTli0bhC6dbyKvj1yjdp4yDsH/R/4HTFbH6rW/qadfbBhhy9OZ7og94oaXyX0ws8n22Gg74W6vjUiI0cSNPj5ezVys1T4/bwKsf9qi5JiK3T3LQYAvBaVi7jPW2PbKhdc/NUWl87ZIuk7R5SXVf55dPcqwYP/uInlb7sj41hznP6pBdbMaodBz96Ak0t5Q94amavqOQxSNhlr0IChuLgYPj4+WL16dUN2Q5amvpyMw8fdcfSXdkblzWwMgACUV/yZai0rt4YgKODdJV8sa6m+jXdeOYSFnw1CSWnNElXN7cqhK1ZZ5gaIasHKSsCgZ69DZW/AmdTm0LQqRw/fW7hxzQbLv/0NW46fxpKvMvHAQ0XiOT36F+PmDWv8dsJeLDt60AGCAeje91ZD3AZRg2jQIYmhQ4di6NChNa5fWlqK0tJScV+n09VFt5q8gIfOoYvHNUya90y1Y7+cb4PbpTaY+M+fse6r/lBAwIR/psLaWkArx6o/jgIiw5Lw7f7u+PVCG7i0umnymoMfPI9unlewbOOjFr4bItM6dr+Nj77LhFJlwO1iK8wP64js32zRvV9lluHlafn4bIEbzp22ReDz17Fo63m89o9uyM1SwalNBW5cM/5TadArcPOGDZycmWFoTDgkIU2jWiURHR0NjUYjbu7u7g3dpUanjVMRJo86jA8/GYzyiurxYuFNO8xb8w/498nGrrUbEb/m/9DCvhS/XmgFg6Fy/sLwwF9gb1uOzfE+Nbpmn+65mBF2EEtjH8OF3JYWvR+imrh0ToU3nuiKt0K6IP6L1nhnRTY6dCmB1R9/AXf9pxX2bnXCuVP2+CSqHS6dUyH4pYJ7N0qNj2CBTcYa1aTHmTNnYtq0aeK+Tqdj0GCmrh5X4aQpwadRcWKZtbWA3l21eO7xXxA04RWknm6PMZEvQN2iBHq9AsW3Vfjqo83Iu+IAAOjbIxc9vS5j72exRm1/Mvcb7DvcGYvWDRLLfLrlYeGUBKz5rx/2Jnepj1skqqai3Aq5FyqHwzJP2qNbn1sYNv4Ktq5yBgBc/NXWqH5OpgrO7SpXVRRcsYFjqwqj41bWAhwcK1BwuVk99J7o/tCoAgaVSgWVimPgUhw944Zxs54zKosMO4jsPA3+u6s3DMKfSSddUeUf0b49cuHocBvJ6ZWrGz7e5I/Pv/YV67V2vIUl7+zB/LUB+OW8s1ju0y0P0VP34tPtDyL+QPe6vC0isygUQDOlgPwcJa7m2aB95xKj4+06lSL1+8pll2dSm8PBUQ8v71vIPFk5j6HPY0VQWAFnj9lXa5vuXxySkKZRBQwk3e0SJS787mRUVlJqA12RrVg+5LFfcTHXEYU3bdHT6zImjzqML/f2Qo7WEQBwuaDF39qs/Jb1+2U1rl5vDqByGGLh1AR8nfAADqR2REt15fyHCr01bnLiI9WjcTPz8PP3DrjyuxJ2LfQIeO4Gej9ShPdHdQKgwJdrnfHyO1qc/8UO50/bIfCfBXDvXIoPJlT+/5CTaYufv3fA1H9fwseR7WHdTED4B5dw4BtHFOQzw9Co8G2VkjBgoGrcXQsx4flUODQvhfZqC2z6zgfb9/Yyq43gRzNhp6rA6KeOY/RTx8Xy9LOuiPhXiKW7THRXjq0rMH1lNpycK3DrpjWyztji/VGdcDSpcohtx7o2aGZrwKR5uXBw1OP8L7aYObIT8i7+Gdj+a3IHhH/4OxZtO/fHg5s0WDOr3d0uSdQkKQSh4UKmoqIiZGZmAgD69u2LZcuWISAgAE5OTujQwfTDfXQ6HTQaDR4JjIKNja3J+kSNkXJPakN3gajOVAjl2I9vUFhYCLVabfqEWqj6rPAfOh82zWr/WVFRXoKU/82p077ezxo0w5CamoqAgABxv2pCY2hoKGJjYxuoV0RE1CTx0dCSNGjAMHjwYDRggoOIiIhqqFE9h4GIiKi2qlZJSNnMERUVBYVCYbR17/7nirGSkhKEh4ejVatWaNGiBUaMGIH8/HyjNrKzsxESEgJ7e3s4Oztj+vTpqKgwXua7f/9+9OvXDyqVCl5eXnWWoWfAQERE8mAQpG9meuCBB5CXlyduhw4dEo9FRETgu+++w/bt23HgwAHk5uZi+PDh4nG9Xo+QkBCUlZUhOTkZGzduRGxsLObMmSPWycrKQkhICAICApCeno6pU6di/Pjx2LNnj7R/qzvgKgkiIpKHBpjDYGNjA1dX12rlhYWF+Pzzz7F582b84x//AABs2LABPXr0wOHDh/Hwww9j7969+OWXX7Bv3z64uLigT58+WLBgASIjIxEVFQWlUomYmBh4enpi6dKlAIAePXrg0KFDWL58OYKDgyXcbHXMMBAREZlBp9MZbX99x9Hf/fbbb3Bzc0OnTp0wevRoZGdnAwDS0tJQXl6OwMBAsW737t3RoUMHpKSkAABSUlLg7e0NFxcXsU5wcDB0Oh1Onz4t1vlrG1V1qtqwJAYMREQkCwpInMPwRzvu7u5G7zWKjo6+4/X8/PwQGxuL3bt3Y+3atcjKysKAAQNw8+ZNaLVaKJVKODo6Gp3j4uICrVYLANBqtUbBQtXxqmP3qqPT6XD79m1p/2B/wyEJIiKSBws96TEnJ8foOQx3e2XBX9/G3Lt3b/j5+cHDwwPbtm2DnZ1d7fvRQJhhICIiMoNarTbaavqOI0dHR3Tt2hWZmZlwdXVFWVkZbty4YVQnPz9fnPPg6upabdVE1b6pOmq12uJBCQMGIiKShfpeVvl3RUVFOHfuHNq2bQtfX180a9YMiYmJ4vGMjAxkZ2fD398fAODv74+TJ0/i8uXLYp2EhASo1Wr07NlTrPPXNqrqVLVhSQwYiIhIHgQLbGZ45513cODAAVy4cAHJycl47rnnYG1tjZEjR0Kj0SAsLAzTpk3DDz/8gLS0NIwbNw7+/v54+OGHAQBBQUHo2bMnXn75ZRw/fhx79uzBrFmzEB4eLmY1Jk2ahPPnz2PGjBk4e/Ys1qxZg23btiEiIkLqv1Y1nMNARERUBy5duoSRI0fi2rVraNOmDR577DEcPnwYbdq0AQAsX74cVlZWGDFiBEpLSxEcHIw1a9aI51tbWyM+Ph6vv/46/P390bx5c4SGhmL+/PliHU9PT+zcuRMRERFYsWIF2rdvj3Xr1ll8SSXQwC+fkoovnyI54MunqCmrz5dPDRg8V9JnRUVFCQ7un8eXTxERETVphj82KefLGOcwEBERkUnMMBARkSwoBAEKCaPwUs5tChgwEBGRPDTAuySaEgYMREQkDxZ60qNccQ4DERERmcQMAxERyYLUpzVKfdJjY8eAgYiI5IFDEpJwSIKIiIhMYoaBiIhkQWGo3KScL2cMGIiISB44JCEJhySIiIjIJGYYiIhIHvjgJkkYMBARkSzw0dDScEiCiIiITGKGgYiI5IGTHiVhwEBERPIgAJCyNFLe8QIDBiIikgfOYZCGcxiIiIjIJGYYiIhIHgRInMNgsZ40SgwYiIhIHjjpURIOSRAREZFJzDAQEZE8GAAoJJ4vYwwYiIhIFrhKQhoOSRAREZFJzDAQEZE8cNKjJAwYiIhIHhgwSMIhCSIiIjKJGQYiIpIHZhgkYcBARETywGWVkjBgICIiWeCySmk4h4GIiIhMYoaBiIjkgXMYJGGGgYiI5MEgSN/MEB0djQcffBAODg5wdnbGsGHDkJGRYVRn8ODBUCgURtukSZOM6mRnZyMkJAT29vZwdnbG9OnTUVFRYVRn//796NevH1QqFby8vBAbG1urf6J7YcBARERUBw4cOIDw8HAcPnwYCQkJKC8vR1BQEIqLi43qTZgwAXl5eeK2ePFi8Zher0dISAjKysqQnJyMjRs3IjY2FnPmzBHrZGVlISQkBAEBAUhPT8fUqVMxfvx47Nmzx6L3wyEJIiKSBwsNSeh0OqNilUoFlUpVrfru3buN9mNjY+Hs7Iy0tDQMHDhQLLe3t4erq+sdL7l371788ssv2LdvH1xcXNCnTx8sWLAAkZGRiIqKglKpRExMDDw9PbF06VIAQI8ePXDo0CEsX74cwcHBtb/fv2GGgYiIZEL4M2iozYbKgMHd3R0ajUbcoqOja3T1wsJCAICTk5NR+aZNm9C6dWv06tULM2fOxK1bt8RjKSkp8Pb2houLi1gWHBwMnU6H06dPi3UCAwON2gwODkZKSorZ/0L3wgwDERGRGXJycqBWq8X9O2UX/s5gMGDq1Kl49NFH0atXL7F81KhR8PDwgJubG06cOIHIyEhkZGTg66+/BgBotVqjYAGAuK/Vau9ZR6fT4fbt27Czs6vdjf4NAwYiIpIHCw1JqNVqo4ChJsLDw3Hq1CkcOnTIqHzixIniz97e3mjbti0ef/xxnDt3Dp07d659X+sAhySIiEge6nmVRJXJkycjPj4eP/zwA9q3b3/Pun5+fgCAzMxMAICrqyvy8/ON6lTtV817uFsdtVptsewCwICBiIioTgiCgMmTJ2PHjh34/vvv4enpafKc9PR0AEDbtm0BAP7+/jh58iQuX74s1klISIBarUbPnj3FOomJiUbtJCQkwN/f30J3UokBAxERyYNgkL6ZITw8HP/5z3+wefNmODg4QKvVQqvV4vbt2wCAc+fOYcGCBUhLS8OFCxfw7bffYuzYsRg4cCB69+4NAAgKCkLPnj3x8ssv4/jx49izZw9mzZqF8PBwce7EpEmTcP78ecyYMQNnz57FmjVrsG3bNkRERFj0n48BAxERyYOUFRK1mP+wdu1aFBYWYvDgwWjbtq24bd26FQCgVCqxb98+BAUFoXv37nj77bcxYsQIfPfdd2Ib1tbWiI+Ph7W1Nfz9/TFmzBiMHTsW8+fPF+t4enpi586dSEhIgI+PD5YuXYp169ZZdEklwEmPREQkF4Y/l0bW/vyaE0wEGO7u7jhw4IDJdjw8PLBr16571hk8eDCOHTtmVv/MxQwDERERmcQMAxERyQNfPiUJAwYiIpIHARIDBov1pFHikAQRERGZxAwDERHJA4ckJGHAQERE8mAwADDvWQrVz5cvDkkQERGRScwwEBGRPHBIQhIGDEREJA8MGCThkAQRERGZxAwDERHJQz0/GrqpYcBARESyIAgGCGa+cfLv58sZAwYiIpIHQZCWJeAcBiIiIqJ7Y4aBiIjkQZA4h0HmGQYGDEREJA8GA6CQMA9B5nMYOCRBREREJjHDQERE8sAhCUkYMBARkSwIBgMECUMScl9WySEJIiIiMokZBiIikgcOSUjCgIGIiOTBIAAKBgy1xSEJIiIiMokZBiIikgdBACDlOQzyzjAwYCAiIlkQDAIECUMSAgMGIiIiGRAMkJZh4LJKIiIiontihoGIiGSBQxLSMGAgIiJ54JCEJI06YKiK9ioqShq4J0R1x0oob+guENWZClT+ftfHt/cKlEt6blNVX+WqUQcMN2/eBAD8tH9RA/eEiIikuHnzJjQaTZ20rVQq4erqikPaXZLbcnV1hVKptECvGh+F0IgHZQwGA3Jzc+Hg4ACFQtHQ3ZEFnU4Hd3d35OTkQK1WN3R3iCyKv9/1TxAE3Lx5E25ubrCyqrt5+CUlJSgrK5PcjlKphK2trQV61Pg06gyDlZUV2rdv39DdkCW1Ws0/qNRk8fe7ftVVZuGvbG1tZftBbylcVklEREQmMWAgIiIikxgwkFlUKhXmzp0LlUrV0F0hsjj+fhPdXaOe9EhERET1gxkGIiIiMokBAxEREZnEgIGIiIhMYsBAREREJjFgoBpbvXo1OnbsCFtbW/j5+eGnn35q6C4RWURSUhKefvppuLm5QaFQIC4urqG7RHTfYcBANbJ161ZMmzYNc+fOxdGjR+Hj44Pg4GBcvny5obtGJFlxcTF8fHywevXqhu4K0X2LyyqpRvz8/PDggw9i1apVACrf4+Hu7o4333wT7777bgP3jshyFAoFduzYgWHDhjV0V4juK8wwkEllZWVIS0tDYGCgWGZlZYXAwECkpKQ0YM+IiKi+MGAgk65evQq9Xg8XFxejchcXF2i12gbqFRER1ScGDERERGQSAwYyqXXr1rC2tkZ+fr5ReX5+PlxdXRuoV0REVJ8YMJBJSqUSvr6+SExMFMsMBgMSExPh7+/fgD0jIqL6YtPQHaDGYdq0aQgNDUX//v3x0EMP4aOPPkJxcTHGjRvX0F0jkqyoqAiZmZniflZWFtLT0+Hk5IQOHTo0YM+I7h9cVkk1tmrVKixZsgRarRZ9+vTBypUr4efn19DdIpJs//79CAgIqFYeGhqK2NjY+u8Q0X2IAQMRERGZxDkMREREZBIDBiIiIjKJAQMRERGZxICBiIiITGLAQERERCYxYCAiIiKTGDAQERGRSQwYiIiIyCQGDEQSvfLKKxg2bJi4P3jwYEydOrXe+7F//34oFArcuHHjrnUUCgXi4uJq3GZUVBT69OkjqV8XLlyAQqFAenq6pHaIqGExYKAm6ZVXXoFCoYBCoYBSqYSXlxfmz5+PioqKOr/2119/jQULFtSobk0+5ImI7gd8+RQ1WUOGDMGGDRtQWlqKXbt2ITw8HM2aNcPMmTOr1S0rK4NSqbTIdZ2cnCzSDhHR/YQZBmqyVCoVXF1d4eHhgddffx2BgYH49ttvAfw5jPDhhx/Czc0N3bp1AwDk5OTghRdegKOjI5ycnPDss8/iwoULYpt6vR7Tpk2Do6MjWrVqhRkzZuDvr2P5+5BEaWkpIiMj4e7uDpVKBS8vL3z++ee4cOGC+MKjli1bQqFQ4JVXXgFQ+frw6OhoeHp6ws7ODj4+Pvjyyy+NrrNr1y507doVdnZ2CAgIMOpnTUVGRqJr166wt7dHp06dMHv2bJSXl1er98knn8Dd3R329vZ44YUXUFhYaHR83bp16NGjB2xtbdG9e3esWbPG7L4Q0f2NAQPJhp2dHcrKysT9xMREZGRkICEhAfHx8SgvL0dwcDAcHBxw8OBB/Pjjj2jRogWGDBkinrd06VLExsZi/fr1OHToEAoKCrBjx457Xnfs2LH473//i5UrV+LMmTP45JNP0KJFC7i7u+Orr74CAGRkZCAvLw8rVqwAAERHR+OLL75ATEwMTp8+jYiICIwZMwYHDhwAUBnYDB8+HE8//TTS09Mxfvx4vPvuu2b/mzg4OCA2Nha//PILVqxYgc8++wzLly83qpOZmYlt27bhu+++w+7du3Hs2DG88cYb4vFNmzZhzpw5+PDDD3HmzBksXLgQs2fPxsaNG83uDxHdxwSiJig0NFR49tlnBUEQBIPBICQkJAgqlUp45513xOMuLi5CaWmpeM7//d//Cd26dRMMBoNYVlpaKtjZ2Ql79uwRBEEQ2rZtKyxevFg8Xl5eLrRv3168liAIwqBBg4QpU6YIgiAIGRkZAgAhISHhjv384YcfBADC9evXxbKSkhLB3t5eSE5ONqobFhYmjBw5UhAEQZg5c6bQs2dPo+ORkZHV2vo7AMKOHTvuenzJkiWCr6+vuD937lzB2tpauHTpklj2v//9T7CyshLy8vIEQRCEzp07C5s3bzZqZ8GCBYK/v78gCIKQlZUlABCOHTt21+sS0f2PcxioyYqPj0eLFi1QXl4Og8GAUaNGISoqSjzu7e1tNG/h+PHjyMzMhIODg1E7JSUlOHfuHAoLC5GXlwc/Pz/xmI2NDfr3719tWKJKeno6rK2tMWjQoBr3OzMzE7du3cITTzxhVF5WVoa+ffsCAM6cOWPUDwDw9/ev8TWqbN26FStXrsS5c+dQVFSEiooKqNVqozodOnRAu3btjK5jMBiQkZEBBwcHnDt3DmFhYZgwYYJYp6KiAhqNxuz+ENH9iwEDNVkBAQFYu3YtlEol3NzcYGNj/OvevHlzo/2ioiL4+vpi06ZN1dpq06ZNrfpgZ2dn9jlFRUUAgJ07dxp9UAOV8zIsJSUlBaNHj8a8efMQHBwMjUaDLVu2YOnSpWb39bPPPqsWwFhbW1usr0TU8BgwUJPVvHlzeHl51bh+v379sHXrVjg7O1f7ll2lbdu2OHLkCAYOHAig8pt0Wloa+vXrd8f63t7eMBgMOHDgAAIDA6sdr8pw6PV6saxnz55QqVTIzs6+a2aiR48e4gTOKocPHzZ9k3+RnJwMDw8PvP/++2LZxYsXq9XLzs5Gbm4u3NzcxOtYWVmhW7ducHFxgZubG86fP4/Ro0ebdX0ialw46ZHoD6NHj0br1q3x7LPP4uDBg8jKysL+/fvx1ltv4dKlSwCAKVOmYNGiRYiLi8PZs2fxxhtv3PMZCh07dkRoaCheffVVxMXFiW1u27YNAODh4QGFQoH4+HhcuXIFRUVFcHBwwDvvvIOIiAhs3LgR586dw9GjR/Hxxx+LEwknTZqE3377DdOnT0dGRgY2b96M2NhYs+63S5cuyM7OxpYtW3Du3DmsXLnyjhM4bW1tERoaiuPHj+PgwYN466238MILL8DV1RUAMG/ePERHR2PlypX49ddfcfLkSWzYsAHLli0zqz9EdH9jwED0B3t7eyQlJaFDhw4YPnw4evTogbCwMJSUlIgZh7fffhsvv/wyQkND4e/vDwcHBzz33HP3bHft2rV4/vnn8cYbb6B79+6YMGECiouLAQDt2rXDvHnz8O6778LFxQWTJ08GACxYsACzZ89GdHQ0evTogSFDhmDnzp3w9PQEUDmv4KuvvkJcXBx8fHwQExODhQsXmnW/zzzzDCIiIjB58mT06dMHycnJmD17drV6Xl5eGD58OJ588kkEBQWhd+/eRssmx48fj3Xr1mHDhg3w9vbGoEGDEBsbK/aViJoGhXC32VpEREREf2CGgYiIiExiwEBEREQmMWAgIiIikxgwEBERkUkMGIiIiMgkBgxERERkEgMGIiIiMokBAxEREZnEgIGIiIhMYsBAREREJjFgICIiIpP+H8JyoKKbFNkqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Explanation</th>\n",
       "      <th>Metric Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CholCheck &amp; AnyHealthcare &amp; ~HighBP &amp; ~HighCho...</td>\n",
       "      <td>40.88986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Explanation  Metric Result\n",
       "0  CholCheck & AnyHealthcare & ~HighBP & ~HighCho...       40.88986"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len(data=data, target_class=1, print_conf_matrix=1, epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El rendimiento de la explicaciÃ³n no  mejorÃ³ significativamente, esto puede ser en gran parte porque el modelo se vÃ© muy influenciado de la clase mayoritaria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boolean_columns(df: pd.DataFrame, n: int):\n",
    "    new_df = df.copy()\n",
    "\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype in [int, float]:\n",
    "            min_val = df[column].min()\n",
    "            max_val = df[column].max()\n",
    "            step = (max_val - min_val) / n\n",
    "\n",
    "            for i in range(n):\n",
    "                lower_bound = np.round(min_val + i * step, 2)\n",
    "                upper_bound = np.round(min_val + (i + 1) * step, 2)\n",
    "                bool_col_name = f\"{lower_bound}<={column}<{upper_bound}\"\n",
    "                new_df[bool_col_name] = (df[column] >= lower_bound) & (df[column] < upper_bound)\n",
    "            new_df.drop(column, axis=1, inplace=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_len(data: StructuredDataset, print_conf_matrix=0, metric: Metric = F1Score(), target_class=0, epoch=500,\n",
    "            l_r=0.3, hidden_neurons=40):\n",
    "    # Dataset splitting into train, validation, and testing with stratification\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data.x, data.y, test_size=0.3, stratify=data.y, random_state=42)\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, stratify=y_test, random_state=42)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    train_data = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
    "    val_data = TensorDataset(torch.tensor(x_val), torch.tensor(y_val))\n",
    "    test_data = TensorDataset(torch.tensor(x_test), torch.tensor(y_test))\n",
    "\n",
    "    # Model instantiation\n",
    "    model_mu = lens.models.XMuNN(n_classes=len(torch.unique(data.y)), n_features=data.x.shape[1],\n",
    "                                    hidden_neurons=[hidden_neurons], loss=torch.nn.CrossEntropyLoss())\n",
    "\n",
    "    model_relu = lens.models.XReluNN(n_classes=len(torch.unique(data.y)), n_features=data.x.shape[1],\n",
    "                                        hidden_neurons=[hidden_neurons], loss=torch.nn.CrossEntropyLoss())\n",
    "\n",
    "    models = [model_mu, model_relu]\n",
    "\n",
    "    # Training\n",
    "    i = 0\n",
    "    for model in models:\n",
    "        model.fit(train_data, val_data, epochs=epoch, l_r=l_r)\n",
    "\n",
    "        # Get accuracy on test samples\n",
    "        test_acc = model_mu.evaluate(test_data, metric=metric)\n",
    "        print(\"Test accuracy:\", test_acc)\n",
    "\n",
    "        concept_names = data.feature_names\n",
    "\n",
    "        # Create a DataFrame to store explanations and their metrics\n",
    "        explanations_dict = {\"Explanation\": [], \"Metric Result\": []}\n",
    "\n",
    "        for target in range(target_class):\n",
    "            formula = model.get_global_explanation(x_train, y_train, target,\n",
    "                                                    top_k_explanations=4, metric=metric, concept_names=concept_names)\n",
    "\n",
    "            # Compute explanation accuracy\n",
    "            exp_accuracy, predictions = lens.logic.test_explanation(formula, target, x_test, y_test, metric=metric,\n",
    "                                                                    concept_names=concept_names)\n",
    "\n",
    "            # Store the explanation and its metric result in the dictionary\n",
    "            explanations_dict[\"Explanation\"].append(formula)\n",
    "            explanations_dict[\"Metric Result\"].append(exp_accuracy)\n",
    "\n",
    "            if print_conf_matrix:\n",
    "                print_confusion_matrix(targets=y_test, predictions_np=predictions)\n",
    "        explanations_df = pd.DataFrame(explanations_dict)\n",
    "        explanations_df.to_csv(f\"\"\"exaplanations_{i}.csv\"\"\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Crop_Recommendation.csv')\n",
    "y = dataset[\"Crop\"].to_numpy()\n",
    "label_encoder = LabelEncoder()\n",
    "dataset['Crop'] = label_encoder.fit_transform(dataset['Crop'])\n",
    "y_encoded = dataset[\"Crop\"].to_numpy()\n",
    "dataset.drop(\"Crop\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = create_boolean_columns(dataset, 3)\n",
    "X_tensor = torch.tensor(new_dataset.to_numpy(), dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_encoded, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StructuredDataset(X_tensor, y_tensor, dataset_name=\"crops\",\n",
    "                            feature_names=new_dataset.columns.to_numpy(), class_names=np.unique(y).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_490547/2046207098.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
      "/tmp/ipykernel_490547/2046207098.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_data = TensorDataset(torch.tensor(x_val), torch.tensor(y_val))\n",
      "/tmp/ipykernel_490547/2046207098.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data = TensorDataset(torch.tensor(x_test), torch.tensor(y_test))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500, Loss: 3.332, Tr_acc: 4.29, Val_acc: 21.21, best_e: -1\n",
      "Epoch: 2/500, Loss: 3.495, Tr_acc: 23.77, Val_acc: 50.91, best_e: -1\n",
      "Epoch: 3/500, Loss: 9.024, Tr_acc: 49.74, Val_acc: 36.67, best_e: -1\n",
      "Epoch: 4/500, Loss: 15.033, Tr_acc: 35.26, Val_acc: 27.88, best_e: -1\n",
      "Epoch: 5/500, Loss: 13.955, Tr_acc: 28.12, Val_acc: 37.58, best_e: -1\n",
      "Epoch: 6/500, Loss: 11.961, Tr_acc: 34.03, Val_acc: 51.21, best_e: -1\n",
      "Epoch: 7/500, Loss: 8.947, Tr_acc: 51.49, Val_acc: 47.88, best_e: -1\n",
      "Epoch: 8/500, Loss: 7.876, Tr_acc: 50.13, Val_acc: 51.21, best_e: -1\n",
      "Epoch: 9/500, Loss: 6.648, Tr_acc: 54.48, Val_acc: 40.61, best_e: -1\n",
      "Epoch: 10/500, Loss: 8.553, Tr_acc: 39.29, Val_acc: 53.94, best_e: -1\n",
      "Epoch: 11/500, Loss: 5.910, Tr_acc: 51.23, Val_acc: 59.09, best_e: -1\n",
      "Epoch: 12/500, Loss: 4.928, Tr_acc: 56.56, Val_acc: 60.00, best_e: -1\n",
      "Epoch: 13/500, Loss: 4.073, Tr_acc: 61.04, Val_acc: 58.18, best_e: -1\n",
      "Epoch: 14/500, Loss: 3.500, Tr_acc: 58.51, Val_acc: 63.94, best_e: -1\n",
      "Epoch: 15/500, Loss: 2.953, Tr_acc: 64.61, Val_acc: 65.15, best_e: -1\n",
      "Epoch: 16/500, Loss: 2.805, Tr_acc: 65.78, Val_acc: 66.06, best_e: -1\n",
      "Epoch: 17/500, Loss: 2.912, Tr_acc: 66.23, Val_acc: 66.36, best_e: -1\n",
      "Epoch: 18/500, Loss: 2.946, Tr_acc: 67.47, Val_acc: 69.09, best_e: -1\n",
      "Epoch: 19/500, Loss: 2.865, Tr_acc: 69.48, Val_acc: 70.30, best_e: -1\n",
      "Epoch: 20/500, Loss: 2.474, Tr_acc: 71.62, Val_acc: 69.39, best_e: -1\n",
      "Epoch: 21/500, Loss: 2.506, Tr_acc: 69.35, Val_acc: 67.58, best_e: -1\n",
      "Epoch: 22/500, Loss: 2.148, Tr_acc: 67.08, Val_acc: 65.45, best_e: -1\n",
      "Epoch: 23/500, Loss: 2.360, Tr_acc: 68.12, Val_acc: 67.27, best_e: -1\n",
      "Epoch: 24/500, Loss: 2.111, Tr_acc: 68.38, Val_acc: 67.58, best_e: -1\n",
      "Epoch: 25/500, Loss: 1.979, Tr_acc: 67.60, Val_acc: 64.85, best_e: -1\n",
      "Epoch: 26/500, Loss: 1.891, Tr_acc: 64.81, Val_acc: 67.58, best_e: -1\n",
      "Epoch: 27/500, Loss: 1.859, Tr_acc: 70.45, Val_acc: 70.91, best_e: -1\n",
      "Epoch: 28/500, Loss: 1.799, Tr_acc: 71.62, Val_acc: 68.79, best_e: -1\n",
      "Epoch: 29/500, Loss: 1.784, Tr_acc: 68.96, Val_acc: 70.61, best_e: -1\n",
      "Epoch: 30/500, Loss: 1.716, Tr_acc: 70.97, Val_acc: 68.48, best_e: -1\n",
      "Epoch: 31/500, Loss: 1.702, Tr_acc: 70.84, Val_acc: 71.82, best_e: -1\n",
      "Epoch: 32/500, Loss: 1.710, Tr_acc: 71.43, Val_acc: 73.64, best_e: -1\n",
      "Epoch: 33/500, Loss: 1.700, Tr_acc: 72.01, Val_acc: 70.91, best_e: -1\n",
      "Epoch: 34/500, Loss: 1.698, Tr_acc: 70.19, Val_acc: 68.48, best_e: -1\n",
      "Epoch: 35/500, Loss: 1.659, Tr_acc: 69.03, Val_acc: 72.42, best_e: -1\n",
      "Epoch: 36/500, Loss: 1.614, Tr_acc: 72.14, Val_acc: 74.55, best_e: -1\n",
      "Epoch: 37/500, Loss: 1.627, Tr_acc: 72.40, Val_acc: 73.94, best_e: -1\n",
      "Epoch: 38/500, Loss: 1.590, Tr_acc: 71.75, Val_acc: 69.70, best_e: -1\n",
      "Epoch: 39/500, Loss: 1.637, Tr_acc: 70.78, Val_acc: 74.24, best_e: -1\n",
      "Epoch: 40/500, Loss: 1.568, Tr_acc: 73.57, Val_acc: 73.03, best_e: -1\n",
      "Epoch: 41/500, Loss: 1.589, Tr_acc: 72.60, Val_acc: 73.03, best_e: -1\n",
      "Epoch: 42/500, Loss: 1.571, Tr_acc: 72.73, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 43/500, Loss: 1.559, Tr_acc: 72.53, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 44/500, Loss: 1.570, Tr_acc: 70.78, Val_acc: 75.15, best_e: -1\n",
      "Epoch: 45/500, Loss: 1.510, Tr_acc: 73.05, Val_acc: 73.33, best_e: -1\n",
      "Epoch: 46/500, Loss: 1.526, Tr_acc: 72.60, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 47/500, Loss: 1.516, Tr_acc: 73.12, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 48/500, Loss: 1.508, Tr_acc: 72.99, Val_acc: 71.21, best_e: -1\n",
      "Epoch: 49/500, Loss: 1.469, Tr_acc: 72.21, Val_acc: 71.82, best_e: -1\n",
      "Epoch: 50/500, Loss: 1.523, Tr_acc: 72.40, Val_acc: 73.33, best_e: -1\n",
      "Epoch: 51/500, Loss: 1.450, Tr_acc: 72.79, Val_acc: 73.03, best_e: -1\n",
      "Epoch: 52/500, Loss: 1.517, Tr_acc: 72.14, Val_acc: 74.24, best_e: -1\n",
      "Epoch: 53/500, Loss: 1.433, Tr_acc: 72.99, Val_acc: 73.64, best_e: -1\n",
      "Epoch: 54/500, Loss: 1.498, Tr_acc: 73.18, Val_acc: 73.33, best_e: -1\n",
      "Epoch: 55/500, Loss: 1.470, Tr_acc: 71.49, Val_acc: 73.03, best_e: -1\n",
      "Epoch: 56/500, Loss: 1.455, Tr_acc: 71.17, Val_acc: 73.94, best_e: -1\n",
      "Epoch: 57/500, Loss: 1.455, Tr_acc: 72.73, Val_acc: 73.94, best_e: -1\n",
      "Epoch: 58/500, Loss: 1.429, Tr_acc: 73.31, Val_acc: 73.64, best_e: -1\n",
      "Epoch: 59/500, Loss: 1.368, Tr_acc: 73.44, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 60/500, Loss: 1.467, Tr_acc: 71.17, Val_acc: 73.64, best_e: -1\n",
      "Epoch: 61/500, Loss: 1.394, Tr_acc: 71.82, Val_acc: 73.64, best_e: -1\n",
      "Epoch: 62/500, Loss: 1.409, Tr_acc: 73.51, Val_acc: 72.42, best_e: -1\n",
      "Epoch: 63/500, Loss: 1.404, Tr_acc: 72.79, Val_acc: 72.42, best_e: -1\n",
      "Epoch: 64/500, Loss: 1.401, Tr_acc: 71.88, Val_acc: 74.85, best_e: -1\n",
      "Epoch: 65/500, Loss: 1.382, Tr_acc: 71.82, Val_acc: 73.33, best_e: -1\n",
      "Epoch: 66/500, Loss: 1.322, Tr_acc: 72.99, Val_acc: 73.33, best_e: -1\n",
      "Epoch: 67/500, Loss: 1.314, Tr_acc: 72.99, Val_acc: 69.09, best_e: -1\n",
      "Epoch: 68/500, Loss: 1.354, Tr_acc: 69.29, Val_acc: 71.82, best_e: -1\n",
      "Epoch: 69/500, Loss: 1.351, Tr_acc: 71.62, Val_acc: 70.91, best_e: -1\n",
      "Epoch: 70/500, Loss: 1.439, Tr_acc: 71.43, Val_acc: 72.42, best_e: -1\n",
      "Epoch: 71/500, Loss: 1.329, Tr_acc: 72.47, Val_acc: 71.21, best_e: -1\n",
      "Epoch: 72/500, Loss: 1.343, Tr_acc: 70.19, Val_acc: 70.00, best_e: -1\n",
      "Epoch: 73/500, Loss: 1.318, Tr_acc: 69.29, Val_acc: 70.30, best_e: -1\n",
      "Epoch: 74/500, Loss: 1.300, Tr_acc: 71.04, Val_acc: 70.91, best_e: -1\n",
      "Epoch: 75/500, Loss: 1.291, Tr_acc: 71.88, Val_acc: 71.52, best_e: -1\n",
      "Epoch: 76/500, Loss: 1.324, Tr_acc: 71.04, Val_acc: 70.91, best_e: -1\n",
      "Epoch: 77/500, Loss: 1.387, Tr_acc: 70.26, Val_acc: 70.61, best_e: -1\n",
      "Epoch: 78/500, Loss: 1.325, Tr_acc: 70.58, Val_acc: 72.42, best_e: -1\n",
      "Epoch: 79/500, Loss: 1.410, Tr_acc: 72.01, Val_acc: 65.15, best_e: -1\n",
      "Epoch: 80/500, Loss: 1.417, Tr_acc: 64.94, Val_acc: 70.30, best_e: -1\n",
      "Epoch: 81/500, Loss: 1.484, Tr_acc: 67.86, Val_acc: 71.52, best_e: -1\n",
      "Epoch: 82/500, Loss: 1.339, Tr_acc: 70.45, Val_acc: 63.94, best_e: -1\n",
      "Epoch: 83/500, Loss: 1.388, Tr_acc: 63.25, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 84/500, Loss: 1.435, Tr_acc: 71.43, Val_acc: 73.33, best_e: -1\n",
      "Epoch: 85/500, Loss: 1.328, Tr_acc: 71.82, Val_acc: 69.70, best_e: -1\n",
      "Epoch: 86/500, Loss: 1.349, Tr_acc: 68.51, Val_acc: 67.27, best_e: -1\n",
      "Epoch: 87/500, Loss: 1.381, Tr_acc: 66.69, Val_acc: 71.82, best_e: -1\n",
      "Epoch: 88/500, Loss: 1.284, Tr_acc: 71.82, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 89/500, Loss: 1.322, Tr_acc: 72.53, Val_acc: 70.30, best_e: -1\n",
      "Epoch: 90/500, Loss: 1.303, Tr_acc: 71.36, Val_acc: 65.45, best_e: -1\n",
      "Epoch: 91/500, Loss: 1.350, Tr_acc: 65.84, Val_acc: 69.70, best_e: -1\n",
      "Epoch: 92/500, Loss: 1.306, Tr_acc: 68.96, Val_acc: 74.24, best_e: -1\n",
      "Epoch: 93/500, Loss: 1.287, Tr_acc: 72.92, Val_acc: 71.21, best_e: -1\n",
      "Epoch: 94/500, Loss: 1.298, Tr_acc: 71.04, Val_acc: 71.21, best_e: -1\n",
      "Epoch: 95/500, Loss: 1.445, Tr_acc: 70.58, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 96/500, Loss: 1.307, Tr_acc: 71.69, Val_acc: 66.97, best_e: -1\n",
      "Epoch: 97/500, Loss: 1.317, Tr_acc: 67.40, Val_acc: 69.70, best_e: -1\n",
      "Epoch: 98/500, Loss: 1.300, Tr_acc: 70.71, Val_acc: 71.82, best_e: -1\n",
      "Epoch: 99/500, Loss: 1.263, Tr_acc: 71.10, Val_acc: 71.52, best_e: -1\n",
      "Epoch: 100/500, Loss: 1.320, Tr_acc: 71.30, Val_acc: 70.00, best_e: -1\n",
      "Epoch: 101/500, Loss: 1.264, Tr_acc: 69.29, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 102/500, Loss: 1.289, Tr_acc: 71.62, Val_acc: 71.52, best_e: -1\n",
      "Epoch: 103/500, Loss: 1.288, Tr_acc: 70.91, Val_acc: 68.79, best_e: -1\n",
      "Epoch: 104/500, Loss: 1.249, Tr_acc: 70.45, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 105/500, Loss: 1.216, Tr_acc: 72.79, Val_acc: 70.61, best_e: -1\n",
      "Epoch: 106/500, Loss: 1.241, Tr_acc: 70.97, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 107/500, Loss: 1.226, Tr_acc: 71.56, Val_acc: 69.09, best_e: -1\n",
      "Epoch: 108/500, Loss: 1.312, Tr_acc: 68.70, Val_acc: 73.03, best_e: -1\n",
      "Epoch: 109/500, Loss: 1.263, Tr_acc: 72.66, Val_acc: 70.00, best_e: -1\n",
      "Epoch: 110/500, Loss: 1.272, Tr_acc: 70.84, Val_acc: 69.09, best_e: -1\n",
      "Epoch: 111/500, Loss: 1.270, Tr_acc: 68.25, Val_acc: 73.94, best_e: -1\n",
      "Epoch: 112/500, Loss: 1.255, Tr_acc: 72.21, Val_acc: 69.09, best_e: -1\n",
      "Epoch: 113/500, Loss: 1.319, Tr_acc: 70.26, Val_acc: 74.24, best_e: -1\n",
      "Epoch: 114/500, Loss: 1.255, Tr_acc: 72.14, Val_acc: 70.91, best_e: -1\n",
      "Epoch: 115/500, Loss: 1.257, Tr_acc: 70.71, Val_acc: 73.64, best_e: -1\n",
      "Epoch: 116/500, Loss: 1.230, Tr_acc: 72.47, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 117/500, Loss: 1.337, Tr_acc: 71.17, Val_acc: 73.33, best_e: -1\n",
      "Epoch: 118/500, Loss: 1.228, Tr_acc: 72.21, Val_acc: 73.94, best_e: -1\n",
      "Epoch: 119/500, Loss: 1.229, Tr_acc: 71.36, Val_acc: 73.64, best_e: -1\n",
      "Epoch: 120/500, Loss: 1.209, Tr_acc: 73.12, Val_acc: 71.52, best_e: -1\n",
      "Epoch: 121/500, Loss: 1.241, Tr_acc: 70.78, Val_acc: 70.00, best_e: -1\n",
      "Epoch: 122/500, Loss: 1.261, Tr_acc: 70.19, Val_acc: 68.79, best_e: -1\n",
      "Epoch: 123/500, Loss: 1.233, Tr_acc: 68.31, Val_acc: 72.42, best_e: -1\n",
      "Epoch: 124/500, Loss: 1.273, Tr_acc: 71.56, Val_acc: 70.30, best_e: -1\n",
      "Epoch: 125/500, Loss: 1.276, Tr_acc: 69.16, Val_acc: 70.61, best_e: -1\n",
      "Epoch: 126/500, Loss: 1.372, Tr_acc: 69.81, Val_acc: 70.30, best_e: -1\n",
      "Epoch: 127/500, Loss: 1.237, Tr_acc: 70.97, Val_acc: 70.91, best_e: -1\n",
      "Epoch: 128/500, Loss: 1.251, Tr_acc: 71.23, Val_acc: 67.27, best_e: -1\n",
      "Epoch: 129/500, Loss: 1.240, Tr_acc: 68.70, Val_acc: 70.61, best_e: -1\n",
      "Epoch: 130/500, Loss: 1.226, Tr_acc: 71.43, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 131/500, Loss: 1.254, Tr_acc: 72.66, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 132/500, Loss: 1.268, Tr_acc: 71.49, Val_acc: 72.42, best_e: -1\n",
      "Epoch: 133/500, Loss: 1.244, Tr_acc: 69.94, Val_acc: 71.52, best_e: -1\n",
      "Epoch: 134/500, Loss: 1.213, Tr_acc: 70.13, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 135/500, Loss: 1.238, Tr_acc: 71.49, Val_acc: 73.33, best_e: -1\n",
      "Epoch: 136/500, Loss: 1.318, Tr_acc: 73.38, Val_acc: 70.30, best_e: -1\n",
      "Epoch: 137/500, Loss: 1.300, Tr_acc: 69.42, Val_acc: 71.52, best_e: -1\n",
      "Epoch: 138/500, Loss: 1.274, Tr_acc: 72.21, Val_acc: 71.52, best_e: -1\n",
      "Epoch: 139/500, Loss: 1.208, Tr_acc: 71.95, Val_acc: 74.24, best_e: -1\n",
      "Epoch: 140/500, Loss: 1.264, Tr_acc: 72.92, Val_acc: 71.21, best_e: -1\n",
      "Epoch: 141/500, Loss: 1.304, Tr_acc: 70.39, Val_acc: 70.91, best_e: -1\n",
      "Epoch: 142/500, Loss: 1.246, Tr_acc: 69.48, Val_acc: 71.21, best_e: -1\n",
      "Epoch: 143/500, Loss: 1.303, Tr_acc: 69.81, Val_acc: 73.03, best_e: -1\n",
      "Epoch: 144/500, Loss: 1.264, Tr_acc: 71.10, Val_acc: 69.09, best_e: -1\n",
      "Epoch: 145/500, Loss: 1.344, Tr_acc: 70.13, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 146/500, Loss: 1.387, Tr_acc: 72.92, Val_acc: 70.91, best_e: -1\n",
      "Epoch: 147/500, Loss: 1.536, Tr_acc: 70.26, Val_acc: 66.06, best_e: -1\n",
      "Epoch: 148/500, Loss: 1.384, Tr_acc: 65.84, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 149/500, Loss: 1.310, Tr_acc: 72.53, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 150/500, Loss: 1.292, Tr_acc: 73.25, Val_acc: 69.09, best_e: -1\n",
      "Epoch: 151/500, Loss: 1.439, Tr_acc: 68.90, Val_acc: 69.70, best_e: -1\n",
      "Epoch: 152/500, Loss: 1.510, Tr_acc: 67.40, Val_acc: 70.91, best_e: -1\n",
      "Epoch: 153/500, Loss: 1.433, Tr_acc: 69.81, Val_acc: 71.82, best_e: -1\n",
      "Epoch: 154/500, Loss: 1.505, Tr_acc: 70.58, Val_acc: 71.52, best_e: -1\n",
      "Epoch: 155/500, Loss: 1.388, Tr_acc: 71.75, Val_acc: 72.42, best_e: -1\n",
      "Epoch: 156/500, Loss: 1.283, Tr_acc: 71.49, Val_acc: 63.03, best_e: -1\n",
      "Epoch: 157/500, Loss: 1.678, Tr_acc: 63.96, Val_acc: 70.61, best_e: -1\n",
      "Epoch: 158/500, Loss: 1.450, Tr_acc: 70.78, Val_acc: 71.52, best_e: -1\n",
      "Epoch: 159/500, Loss: 1.522, Tr_acc: 70.91, Val_acc: 66.06, best_e: -1\n",
      "Epoch: 160/500, Loss: 1.448, Tr_acc: 68.05, Val_acc: 66.06, best_e: -1\n",
      "Epoch: 161/500, Loss: 1.389, Tr_acc: 67.47, Val_acc: 70.30, best_e: -1\n",
      "Epoch: 162/500, Loss: 1.511, Tr_acc: 69.09, Val_acc: 69.39, best_e: -1\n",
      "Epoch: 163/500, Loss: 1.412, Tr_acc: 69.81, Val_acc: 71.21, best_e: -1\n",
      "Epoch: 164/500, Loss: 1.310, Tr_acc: 69.35, Val_acc: 69.09, best_e: -1\n",
      "Epoch: 165/500, Loss: 1.382, Tr_acc: 70.19, Val_acc: 73.33, best_e: -1\n",
      "Epoch: 166/500, Loss: 1.277, Tr_acc: 72.40, Val_acc: 65.15, best_e: -1\n",
      "Epoch: 167/500, Loss: 1.443, Tr_acc: 65.84, Val_acc: 66.67, best_e: -1\n",
      "Epoch: 168/500, Loss: 1.700, Tr_acc: 69.29, Val_acc: 68.79, best_e: -1\n",
      "Epoch: 169/500, Loss: 1.425, Tr_acc: 68.96, Val_acc: 73.33, best_e: -1\n",
      "Epoch: 170/500, Loss: 1.373, Tr_acc: 72.73, Val_acc: 71.52, best_e: -1\n",
      "Epoch: 171/500, Loss: 1.407, Tr_acc: 70.45, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 172/500, Loss: 1.399, Tr_acc: 70.97, Val_acc: 70.61, best_e: -1\n",
      "Epoch: 173/500, Loss: 1.307, Tr_acc: 71.10, Val_acc: 66.06, best_e: -1\n",
      "Epoch: 174/500, Loss: 1.626, Tr_acc: 66.10, Val_acc: 69.39, best_e: -1\n",
      "Epoch: 175/500, Loss: 1.432, Tr_acc: 69.42, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 176/500, Loss: 1.298, Tr_acc: 73.25, Val_acc: 71.21, best_e: -1\n",
      "Epoch: 177/500, Loss: 1.345, Tr_acc: 71.04, Val_acc: 73.33, best_e: -1\n",
      "Epoch: 178/500, Loss: 1.338, Tr_acc: 72.53, Val_acc: 68.48, best_e: -1\n",
      "Epoch: 179/500, Loss: 1.337, Tr_acc: 69.74, Val_acc: 69.70, best_e: -1\n",
      "Epoch: 180/500, Loss: 1.383, Tr_acc: 68.77, Val_acc: 71.21, best_e: -1\n",
      "Epoch: 181/500, Loss: 1.331, Tr_acc: 70.65, Val_acc: 71.21, best_e: -1\n",
      "Epoch: 182/500, Loss: 1.277, Tr_acc: 71.69, Val_acc: 71.21, best_e: -1\n",
      "Epoch: 183/500, Loss: 1.276, Tr_acc: 71.82, Val_acc: 70.61, best_e: -1\n",
      "Epoch: 184/500, Loss: 1.294, Tr_acc: 70.97, Val_acc: 73.33, best_e: -1\n",
      "Epoch: 185/500, Loss: 1.264, Tr_acc: 72.99, Val_acc: 73.64, best_e: -1\n",
      "Epoch: 186/500, Loss: 1.287, Tr_acc: 72.73, Val_acc: 73.33, best_e: -1\n",
      "Epoch: 187/500, Loss: 1.266, Tr_acc: 72.99, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 188/500, Loss: 1.369, Tr_acc: 71.17, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 189/500, Loss: 1.383, Tr_acc: 71.49, Val_acc: 71.21, best_e: -1\n",
      "Epoch: 190/500, Loss: 1.270, Tr_acc: 70.13, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 191/500, Loss: 1.379, Tr_acc: 70.52, Val_acc: 70.00, best_e: -1\n",
      "Epoch: 192/500, Loss: 1.283, Tr_acc: 68.51, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 193/500, Loss: 1.254, Tr_acc: 71.62, Val_acc: 74.24, best_e: -1\n",
      "Epoch: 194/500, Loss: 1.283, Tr_acc: 72.60, Val_acc: 74.55, best_e: -1\n",
      "Epoch: 195/500, Loss: 1.262, Tr_acc: 72.92, Val_acc: 74.55, best_e: -1\n",
      "Epoch: 196/500, Loss: 1.266, Tr_acc: 72.99, Val_acc: 73.33, best_e: -1\n",
      "Epoch: 197/500, Loss: 1.241, Tr_acc: 72.92, Val_acc: 73.03, best_e: -1\n",
      "Epoch: 198/500, Loss: 1.237, Tr_acc: 72.79, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 199/500, Loss: 1.216, Tr_acc: 72.66, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 200/500, Loss: 1.227, Tr_acc: 73.12, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 201/500, Loss: 1.236, Tr_acc: 73.05, Val_acc: 74.24, best_e: -1\n",
      "Epoch: 202/500, Loss: 1.255, Tr_acc: 73.05, Val_acc: 73.03, best_e: -1\n",
      "Epoch: 203/500, Loss: 1.225, Tr_acc: 71.82, Val_acc: 73.03, best_e: -1\n",
      "Epoch: 204/500, Loss: 1.394, Tr_acc: 71.95, Val_acc: 70.30, best_e: -1\n",
      "Epoch: 205/500, Loss: 1.235, Tr_acc: 70.52, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 206/500, Loss: 1.233, Tr_acc: 71.75, Val_acc: 71.82, best_e: -1\n",
      "Epoch: 207/500, Loss: 1.235, Tr_acc: 71.30, Val_acc: 73.64, best_e: -1\n",
      "Epoch: 208/500, Loss: 1.194, Tr_acc: 72.27, Val_acc: 73.64, best_e: -1\n",
      "Epoch: 209/500, Loss: 1.201, Tr_acc: 73.70, Val_acc: 71.82, best_e: -1\n",
      "Epoch: 210/500, Loss: 1.247, Tr_acc: 72.34, Val_acc: 70.91, best_e: -1\n",
      "Epoch: 211/500, Loss: 1.192, Tr_acc: 71.69, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 212/500, Loss: 1.204, Tr_acc: 71.95, Val_acc: 73.03, best_e: -1\n",
      "Epoch: 213/500, Loss: 1.180, Tr_acc: 72.86, Val_acc: 71.82, best_e: -1\n",
      "Epoch: 214/500, Loss: 1.203, Tr_acc: 71.23, Val_acc: 70.91, best_e: -1\n",
      "Epoch: 215/500, Loss: 1.269, Tr_acc: 71.88, Val_acc: 70.91, best_e: -1\n",
      "Epoch: 216/500, Loss: 1.216, Tr_acc: 71.49, Val_acc: 73.94, best_e: -1\n",
      "Epoch: 217/500, Loss: 1.220, Tr_acc: 73.05, Val_acc: 71.82, best_e: -1\n",
      "Epoch: 218/500, Loss: 1.200, Tr_acc: 70.65, Val_acc: 73.33, best_e: -1\n",
      "Epoch: 219/500, Loss: 1.165, Tr_acc: 72.40, Val_acc: 71.52, best_e: -1\n",
      "Epoch: 220/500, Loss: 1.166, Tr_acc: 72.53, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 221/500, Loss: 1.167, Tr_acc: 72.92, Val_acc: 73.94, best_e: -1\n",
      "Epoch: 222/500, Loss: 1.166, Tr_acc: 73.18, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 223/500, Loss: 1.167, Tr_acc: 71.82, Val_acc: 70.61, best_e: -1\n",
      "Epoch: 224/500, Loss: 1.162, Tr_acc: 69.61, Val_acc: 72.42, best_e: -1\n",
      "Epoch: 225/500, Loss: 1.155, Tr_acc: 72.01, Val_acc: 73.94, best_e: -1\n",
      "Epoch: 226/500, Loss: 1.142, Tr_acc: 73.18, Val_acc: 70.61, best_e: -1\n",
      "Epoch: 227/500, Loss: 1.164, Tr_acc: 71.36, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 228/500, Loss: 1.232, Tr_acc: 72.27, Val_acc: 72.73, best_e: -1\n",
      "Epoch: 229/500, Loss: 1.177, Tr_acc: 71.43, Val_acc: 70.30, best_e: -1\n",
      "Epoch: 230/500, Loss: 1.251, Tr_acc: 70.32, Val_acc: 67.58, best_e: -1\n",
      "Epoch: 231/500, Loss: 1.320, Tr_acc: 66.82, Val_acc: 71.82, best_e: -1\n",
      "Epoch: 232/500, Loss: 1.324, Tr_acc: 71.10, Val_acc: 68.18, best_e: -1\n",
      "Epoch: 233/500, Loss: 1.555, Tr_acc: 68.25, Val_acc: 62.12, best_e: -1\n",
      "Epoch: 234/500, Loss: 1.488, Tr_acc: 65.39, Val_acc: 64.85, best_e: -1\n",
      "Epoch: 235/500, Loss: 1.662, Tr_acc: 65.32, Val_acc: 72.12, best_e: -1\n",
      "Epoch: 236/500, Loss: 1.291, Tr_acc: 70.78, Val_acc: 70.00, best_e: -1\n",
      "Epoch: 237/500, Loss: 1.275, Tr_acc: 70.13, Val_acc: 68.79, best_e: -1\n",
      "Epoch: 238/500, Loss: 1.345, Tr_acc: 69.87, Val_acc: 69.09, best_e: -1\n",
      "Epoch: 239/500, Loss: 1.347, Tr_acc: 69.42, Val_acc: 71.21, best_e: -1\n",
      "Epoch: 240/500, Loss: 1.353, Tr_acc: 72.27, Val_acc: 67.88, best_e: -1\n",
      "Epoch: 241/500, Loss: 1.413, Tr_acc: 68.18, Val_acc: 68.79, best_e: -1\n",
      "Epoch: 242/500, Loss: 1.365, Tr_acc: 71.04, Val_acc: 73.03, best_e: -1\n",
      "Epoch: 243/500, Loss: 1.368, Tr_acc: 72.66, Val_acc: 57.88, best_e: -1\n",
      "Epoch: 244/500, Loss: 1.664, Tr_acc: 56.69, Val_acc: 69.39, best_e: -1\n",
      "Epoch: 245/500, Loss: 1.385, Tr_acc: 69.22, Val_acc: 66.67, best_e: -1\n",
      "Epoch: 246/500, Loss: 1.496, Tr_acc: 67.99, Val_acc: 66.97, best_e: -1\n",
      "Epoch: 247/500, Loss: 1.492, Tr_acc: 68.12, Val_acc: 63.33, best_e: -1\n",
      "Epoch: 248/500, Loss: 1.739, Tr_acc: 65.97, Val_acc: 68.79, best_e: -1\n",
      "Epoch: 249/500, Loss: 1.407, Tr_acc: 70.52, Val_acc: 67.27, best_e: -1\n",
      "Pruned 13/21 features\n",
      "Pruned 10/21 features\n",
      "Pruned 8/21 features\n",
      "Pruned 12/21 features\n",
      "Pruned 9/21 features\n",
      "Pruned 19/21 features\n",
      "Pruned 12/21 features\n",
      "Pruned 13/21 features\n",
      "Pruned 10/21 features\n",
      "Pruned 5/21 features\n",
      "Pruned 9/21 features\n",
      "Pruned 8/21 features\n",
      "Pruned 11/21 features\n",
      "Pruned 7/21 features\n",
      "Pruned 13/21 features\n",
      "Pruned 13/21 features\n",
      "Pruned 11/21 features\n",
      "Pruned 8/21 features\n",
      "Pruned 9/21 features\n",
      "Pruned 12/21 features\n",
      "Pruned 10/21 features\n",
      "Pruned 11/21 features\n",
      "Pruned features\n",
      "Epoch: 250/500, Loss: 1.721, Tr_acc: 67.60, Val_acc: 70.30, best_e: -1\n",
      "Epoch: 251/500, Loss: 1.373, Tr_acc: 60.52, Val_acc: 39.09, best_e: 251\n",
      "Epoch: 252/500, Loss: 11.222, Tr_acc: 40.52, Val_acc: 23.03, best_e: 251\n",
      "Epoch: 253/500, Loss: 13.107, Tr_acc: 23.70, Val_acc: 33.03, best_e: 251\n",
      "Epoch: 254/500, Loss: 7.092, Tr_acc: 33.44, Val_acc: 37.58, best_e: 251\n",
      "Epoch: 255/500, Loss: 9.908, Tr_acc: 37.08, Val_acc: 40.61, best_e: 255\n",
      "Epoch: 256/500, Loss: 6.899, Tr_acc: 41.04, Val_acc: 36.67, best_e: 255\n",
      "Epoch: 257/500, Loss: 8.777, Tr_acc: 36.75, Val_acc: 43.64, best_e: 257\n",
      "Epoch: 258/500, Loss: 6.602, Tr_acc: 41.95, Val_acc: 44.24, best_e: 258\n",
      "Epoch: 259/500, Loss: 5.976, Tr_acc: 47.14, Val_acc: 49.70, best_e: 259\n",
      "Epoch: 260/500, Loss: 5.668, Tr_acc: 50.00, Val_acc: 52.12, best_e: 260\n",
      "Epoch: 261/500, Loss: 4.404, Tr_acc: 52.08, Val_acc: 54.85, best_e: 261\n",
      "Epoch: 262/500, Loss: 3.884, Tr_acc: 54.16, Val_acc: 60.30, best_e: 262\n",
      "Epoch: 263/500, Loss: 2.456, Tr_acc: 60.13, Val_acc: 59.70, best_e: 262\n",
      "Epoch: 264/500, Loss: 2.247, Tr_acc: 59.68, Val_acc: 53.94, best_e: 262\n",
      "Epoch: 265/500, Loss: 2.611, Tr_acc: 57.73, Val_acc: 60.30, best_e: 265\n",
      "Epoch: 266/500, Loss: 2.047, Tr_acc: 63.25, Val_acc: 64.55, best_e: 266\n",
      "Epoch: 267/500, Loss: 1.667, Tr_acc: 67.27, Val_acc: 68.48, best_e: 267\n",
      "Epoch: 268/500, Loss: 1.401, Tr_acc: 68.96, Val_acc: 63.64, best_e: 267\n",
      "Epoch: 269/500, Loss: 1.681, Tr_acc: 64.16, Val_acc: 60.91, best_e: 267\n",
      "Epoch: 270/500, Loss: 1.654, Tr_acc: 60.52, Val_acc: 66.06, best_e: 267\n",
      "Epoch: 271/500, Loss: 1.252, Tr_acc: 65.32, Val_acc: 68.48, best_e: 271\n",
      "Epoch: 272/500, Loss: 0.998, Tr_acc: 69.22, Val_acc: 65.15, best_e: 271\n",
      "Epoch: 273/500, Loss: 1.067, Tr_acc: 67.34, Val_acc: 65.15, best_e: 271\n",
      "Epoch: 274/500, Loss: 0.993, Tr_acc: 67.40, Val_acc: 66.67, best_e: 271\n",
      "Epoch: 275/500, Loss: 0.967, Tr_acc: 67.21, Val_acc: 67.27, best_e: 271\n",
      "Epoch: 276/500, Loss: 0.959, Tr_acc: 67.47, Val_acc: 67.88, best_e: 271\n",
      "Epoch: 277/500, Loss: 0.827, Tr_acc: 65.78, Val_acc: 71.82, best_e: 277\n",
      "Epoch: 278/500, Loss: 0.760, Tr_acc: 70.84, Val_acc: 70.91, best_e: 277\n",
      "Epoch: 279/500, Loss: 0.798, Tr_acc: 69.94, Val_acc: 66.36, best_e: 277\n",
      "Epoch: 280/500, Loss: 0.790, Tr_acc: 68.31, Val_acc: 66.36, best_e: 277\n",
      "Epoch: 281/500, Loss: 0.699, Tr_acc: 69.35, Val_acc: 68.48, best_e: 277\n",
      "Epoch: 282/500, Loss: 0.740, Tr_acc: 70.13, Val_acc: 66.67, best_e: 277\n",
      "Epoch: 283/500, Loss: 0.732, Tr_acc: 68.64, Val_acc: 65.45, best_e: 277\n",
      "Epoch: 284/500, Loss: 0.701, Tr_acc: 67.99, Val_acc: 67.27, best_e: 277\n",
      "Epoch: 285/500, Loss: 0.652, Tr_acc: 67.27, Val_acc: 68.18, best_e: 277\n",
      "Epoch: 286/500, Loss: 0.648, Tr_acc: 69.29, Val_acc: 67.88, best_e: 277\n",
      "Epoch: 287/500, Loss: 0.676, Tr_acc: 69.87, Val_acc: 67.88, best_e: 277\n",
      "Epoch: 288/500, Loss: 0.614, Tr_acc: 69.61, Val_acc: 71.82, best_e: 288\n",
      "Epoch: 289/500, Loss: 0.616, Tr_acc: 72.27, Val_acc: 70.30, best_e: 288\n",
      "Epoch: 290/500, Loss: 0.655, Tr_acc: 70.91, Val_acc: 71.52, best_e: 288\n",
      "Epoch: 291/500, Loss: 0.624, Tr_acc: 72.08, Val_acc: 72.73, best_e: 291\n",
      "Epoch: 292/500, Loss: 0.602, Tr_acc: 72.47, Val_acc: 72.42, best_e: 291\n",
      "Epoch: 293/500, Loss: 0.628, Tr_acc: 72.01, Val_acc: 72.42, best_e: 291\n",
      "Epoch: 294/500, Loss: 0.607, Tr_acc: 71.69, Val_acc: 72.12, best_e: 291\n",
      "Epoch: 295/500, Loss: 0.583, Tr_acc: 71.04, Val_acc: 72.73, best_e: 295\n",
      "Epoch: 296/500, Loss: 0.594, Tr_acc: 72.40, Val_acc: 71.21, best_e: 295\n",
      "Epoch: 297/500, Loss: 0.630, Tr_acc: 69.61, Val_acc: 74.55, best_e: 297\n",
      "Epoch: 298/500, Loss: 0.578, Tr_acc: 72.73, Val_acc: 72.42, best_e: 297\n",
      "Epoch: 299/500, Loss: 0.592, Tr_acc: 72.21, Val_acc: 73.64, best_e: 297\n",
      "Epoch: 300/500, Loss: 0.595, Tr_acc: 71.82, Val_acc: 73.03, best_e: 297\n",
      "Epoch: 301/500, Loss: 0.583, Tr_acc: 72.01, Val_acc: 73.33, best_e: 297\n",
      "Epoch: 302/500, Loss: 0.582, Tr_acc: 71.43, Val_acc: 72.73, best_e: 297\n",
      "Epoch: 303/500, Loss: 0.581, Tr_acc: 72.01, Val_acc: 73.94, best_e: 297\n",
      "Epoch: 304/500, Loss: 0.567, Tr_acc: 72.73, Val_acc: 72.73, best_e: 297\n",
      "Epoch: 305/500, Loss: 0.569, Tr_acc: 72.60, Val_acc: 73.33, best_e: 297\n",
      "Epoch: 306/500, Loss: 0.576, Tr_acc: 72.73, Val_acc: 73.33, best_e: 297\n",
      "Epoch: 307/500, Loss: 0.569, Tr_acc: 72.86, Val_acc: 72.73, best_e: 297\n",
      "Epoch: 308/500, Loss: 0.568, Tr_acc: 72.92, Val_acc: 73.33, best_e: 297\n",
      "Epoch: 309/500, Loss: 0.569, Tr_acc: 72.99, Val_acc: 73.03, best_e: 297\n",
      "Epoch: 310/500, Loss: 0.562, Tr_acc: 73.05, Val_acc: 73.33, best_e: 297\n",
      "Epoch: 311/500, Loss: 0.562, Tr_acc: 72.92, Val_acc: 72.12, best_e: 297\n",
      "Epoch: 312/500, Loss: 0.566, Tr_acc: 72.66, Val_acc: 72.73, best_e: 297\n",
      "Epoch: 313/500, Loss: 0.560, Tr_acc: 72.79, Val_acc: 73.33, best_e: 297\n",
      "Epoch: 314/500, Loss: 0.559, Tr_acc: 73.12, Val_acc: 73.03, best_e: 297\n",
      "Epoch: 315/500, Loss: 0.560, Tr_acc: 73.12, Val_acc: 72.73, best_e: 297\n",
      "Epoch: 316/500, Loss: 0.557, Tr_acc: 73.12, Val_acc: 73.03, best_e: 297\n",
      "Epoch: 317/500, Loss: 0.557, Tr_acc: 72.27, Val_acc: 73.33, best_e: 297\n",
      "Epoch: 318/500, Loss: 0.558, Tr_acc: 72.27, Val_acc: 73.94, best_e: 297\n",
      "Epoch: 319/500, Loss: 0.555, Tr_acc: 72.60, Val_acc: 74.24, best_e: 297\n",
      "Epoch: 320/500, Loss: 0.555, Tr_acc: 72.92, Val_acc: 73.03, best_e: 297\n",
      "Epoch: 321/500, Loss: 0.556, Tr_acc: 73.12, Val_acc: 73.33, best_e: 297\n",
      "Epoch: 322/500, Loss: 0.555, Tr_acc: 73.05, Val_acc: 72.42, best_e: 297\n",
      "Epoch: 323/500, Loss: 0.555, Tr_acc: 72.73, Val_acc: 72.42, best_e: 297\n",
      "Epoch: 324/500, Loss: 0.556, Tr_acc: 72.79, Val_acc: 73.33, best_e: 297\n",
      "Epoch: 325/500, Loss: 0.554, Tr_acc: 73.12, Val_acc: 73.33, best_e: 297\n",
      "Epoch: 326/500, Loss: 0.553, Tr_acc: 73.05, Val_acc: 73.94, best_e: 297\n",
      "Epoch: 327/500, Loss: 0.553, Tr_acc: 73.31, Val_acc: 73.94, best_e: 297\n",
      "Epoch: 328/500, Loss: 0.552, Tr_acc: 73.31, Val_acc: 73.94, best_e: 297\n",
      "Epoch: 329/500, Loss: 0.552, Tr_acc: 72.53, Val_acc: 73.33, best_e: 297\n",
      "Epoch: 330/500, Loss: 0.553, Tr_acc: 72.34, Val_acc: 73.64, best_e: 297\n",
      "Epoch: 331/500, Loss: 0.551, Tr_acc: 72.66, Val_acc: 73.64, best_e: 297\n",
      "Epoch: 332/500, Loss: 0.551, Tr_acc: 72.73, Val_acc: 73.94, best_e: 297\n",
      "Epoch: 333/500, Loss: 0.551, Tr_acc: 73.31, Val_acc: 73.94, best_e: 297\n",
      "Epoch: 334/500, Loss: 0.551, Tr_acc: 73.31, Val_acc: 73.94, best_e: 297\n",
      "Epoch: 335/500, Loss: 0.551, Tr_acc: 73.25, Val_acc: 73.33, best_e: 297\n",
      "Epoch: 336/500, Loss: 0.550, Tr_acc: 72.92, Val_acc: 73.03, best_e: 297\n",
      "Epoch: 337/500, Loss: 0.550, Tr_acc: 73.05, Val_acc: 73.03, best_e: 297\n",
      "Epoch: 338/500, Loss: 0.550, Tr_acc: 73.05, Val_acc: 73.33, best_e: 297\n",
      "Epoch: 339/500, Loss: 0.550, Tr_acc: 73.05, Val_acc: 73.33, best_e: 297\n",
      "Epoch: 340/500, Loss: 0.550, Tr_acc: 73.12, Val_acc: 73.94, best_e: 297\n",
      "Epoch: 341/500, Loss: 0.550, Tr_acc: 72.66, Val_acc: 74.55, best_e: 341\n",
      "Epoch: 342/500, Loss: 0.549, Tr_acc: 72.86, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 343/500, Loss: 0.550, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 344/500, Loss: 0.549, Tr_acc: 73.31, Val_acc: 73.33, best_e: 341\n",
      "Epoch: 345/500, Loss: 0.550, Tr_acc: 73.05, Val_acc: 73.03, best_e: 341\n",
      "Epoch: 346/500, Loss: 0.550, Tr_acc: 73.12, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 347/500, Loss: 0.550, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 348/500, Loss: 0.550, Tr_acc: 73.31, Val_acc: 73.33, best_e: 341\n",
      "Epoch: 349/500, Loss: 0.549, Tr_acc: 73.05, Val_acc: 73.33, best_e: 341\n",
      "Epoch: 350/500, Loss: 0.549, Tr_acc: 73.05, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 351/500, Loss: 0.549, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 352/500, Loss: 0.549, Tr_acc: 73.31, Val_acc: 73.33, best_e: 341\n",
      "Epoch: 353/500, Loss: 0.549, Tr_acc: 73.05, Val_acc: 73.33, best_e: 341\n",
      "Epoch: 354/500, Loss: 0.548, Tr_acc: 73.05, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 355/500, Loss: 0.548, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 356/500, Loss: 0.548, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 357/500, Loss: 0.548, Tr_acc: 73.25, Val_acc: 73.33, best_e: 341\n",
      "Epoch: 358/500, Loss: 0.548, Tr_acc: 73.05, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 359/500, Loss: 0.548, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 360/500, Loss: 0.548, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 361/500, Loss: 0.548, Tr_acc: 73.31, Val_acc: 73.33, best_e: 341\n",
      "Epoch: 362/500, Loss: 0.549, Tr_acc: 73.05, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 363/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 364/500, Loss: 0.548, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 365/500, Loss: 0.548, Tr_acc: 73.31, Val_acc: 73.33, best_e: 341\n",
      "Epoch: 366/500, Loss: 0.548, Tr_acc: 73.05, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 367/500, Loss: 0.548, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 368/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 369/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 73.33, best_e: 341\n",
      "Epoch: 370/500, Loss: 0.547, Tr_acc: 73.05, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 371/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 372/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 373/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 374/500, Loss: 0.546, Tr_acc: 73.25, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 375/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 376/500, Loss: 0.546, Tr_acc: 73.31, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 377/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 73.33, best_e: 341\n",
      "Epoch: 378/500, Loss: 0.547, Tr_acc: 73.05, Val_acc: 74.24, best_e: 341\n",
      "Epoch: 379/500, Loss: 0.550, Tr_acc: 72.73, Val_acc: 73.94, best_e: 341\n",
      "Epoch: 380/500, Loss: 0.556, Tr_acc: 73.31, Val_acc: 74.24, best_e: 341\n",
      "Epoch: 381/500, Loss: 0.551, Tr_acc: 72.66, Val_acc: 74.24, best_e: 341\n",
      "Epoch: 382/500, Loss: 0.548, Tr_acc: 72.66, Val_acc: 74.55, best_e: 382\n",
      "Epoch: 383/500, Loss: 0.550, Tr_acc: 72.92, Val_acc: 73.33, best_e: 382\n",
      "Epoch: 384/500, Loss: 0.560, Tr_acc: 73.05, Val_acc: 74.55, best_e: 384\n",
      "Epoch: 385/500, Loss: 0.547, Tr_acc: 72.92, Val_acc: 74.55, best_e: 385\n",
      "Epoch: 386/500, Loss: 0.550, Tr_acc: 72.86, Val_acc: 74.55, best_e: 386\n",
      "Epoch: 387/500, Loss: 0.551, Tr_acc: 72.86, Val_acc: 74.55, best_e: 387\n",
      "Epoch: 388/500, Loss: 0.549, Tr_acc: 72.92, Val_acc: 74.55, best_e: 388\n",
      "Epoch: 389/500, Loss: 0.547, Tr_acc: 72.92, Val_acc: 73.33, best_e: 388\n",
      "Epoch: 390/500, Loss: 0.547, Tr_acc: 73.05, Val_acc: 73.94, best_e: 388\n",
      "Epoch: 391/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 73.94, best_e: 388\n",
      "Epoch: 392/500, Loss: 0.548, Tr_acc: 73.31, Val_acc: 73.94, best_e: 388\n",
      "Epoch: 393/500, Loss: 0.548, Tr_acc: 73.25, Val_acc: 73.94, best_e: 388\n",
      "Epoch: 394/500, Loss: 0.548, Tr_acc: 73.18, Val_acc: 73.94, best_e: 388\n",
      "Epoch: 395/500, Loss: 0.547, Tr_acc: 73.25, Val_acc: 73.94, best_e: 388\n",
      "Epoch: 396/500, Loss: 0.547, Tr_acc: 73.25, Val_acc: 73.33, best_e: 388\n",
      "Epoch: 397/500, Loss: 0.547, Tr_acc: 73.12, Val_acc: 73.33, best_e: 388\n",
      "Epoch: 398/500, Loss: 0.547, Tr_acc: 73.12, Val_acc: 74.55, best_e: 398\n",
      "Epoch: 399/500, Loss: 0.547, Tr_acc: 72.92, Val_acc: 74.55, best_e: 399\n",
      "Epoch: 400/500, Loss: 0.548, Tr_acc: 72.92, Val_acc: 74.55, best_e: 400\n",
      "Epoch: 401/500, Loss: 0.549, Tr_acc: 72.92, Val_acc: 74.55, best_e: 401\n",
      "Epoch: 402/500, Loss: 0.550, Tr_acc: 72.92, Val_acc: 74.55, best_e: 402\n",
      "Epoch: 403/500, Loss: 0.548, Tr_acc: 72.92, Val_acc: 73.33, best_e: 402\n",
      "Epoch: 404/500, Loss: 0.548, Tr_acc: 73.12, Val_acc: 73.33, best_e: 402\n",
      "Epoch: 405/500, Loss: 0.548, Tr_acc: 73.12, Val_acc: 73.94, best_e: 402\n",
      "Epoch: 406/500, Loss: 0.547, Tr_acc: 73.25, Val_acc: 73.94, best_e: 402\n",
      "Epoch: 407/500, Loss: 0.547, Tr_acc: 73.25, Val_acc: 73.94, best_e: 402\n",
      "Epoch: 408/500, Loss: 0.547, Tr_acc: 73.25, Val_acc: 73.94, best_e: 402\n",
      "Epoch: 409/500, Loss: 0.547, Tr_acc: 73.25, Val_acc: 73.94, best_e: 402\n",
      "Epoch: 410/500, Loss: 0.547, Tr_acc: 73.25, Val_acc: 73.94, best_e: 402\n",
      "Epoch: 411/500, Loss: 0.547, Tr_acc: 73.25, Val_acc: 73.94, best_e: 402\n",
      "Epoch: 412/500, Loss: 0.547, Tr_acc: 73.25, Val_acc: 73.94, best_e: 402\n",
      "Epoch: 413/500, Loss: 0.546, Tr_acc: 73.31, Val_acc: 73.94, best_e: 402\n",
      "Epoch: 414/500, Loss: 0.546, Tr_acc: 73.31, Val_acc: 73.64, best_e: 402\n",
      "Epoch: 415/500, Loss: 0.546, Tr_acc: 73.31, Val_acc: 73.64, best_e: 402\n",
      "Epoch: 416/500, Loss: 0.546, Tr_acc: 73.31, Val_acc: 73.64, best_e: 402\n",
      "Epoch: 417/500, Loss: 0.546, Tr_acc: 73.31, Val_acc: 73.64, best_e: 402\n",
      "Epoch: 418/500, Loss: 0.546, Tr_acc: 73.31, Val_acc: 73.64, best_e: 402\n",
      "Epoch: 419/500, Loss: 0.546, Tr_acc: 73.25, Val_acc: 74.24, best_e: 402\n",
      "Epoch: 420/500, Loss: 0.546, Tr_acc: 72.86, Val_acc: 74.55, best_e: 420\n",
      "Epoch: 421/500, Loss: 0.546, Tr_acc: 72.92, Val_acc: 74.55, best_e: 421\n",
      "Epoch: 422/500, Loss: 0.547, Tr_acc: 72.92, Val_acc: 74.55, best_e: 422\n",
      "Epoch: 423/500, Loss: 0.547, Tr_acc: 72.92, Val_acc: 73.94, best_e: 422\n",
      "Epoch: 424/500, Loss: 0.546, Tr_acc: 73.31, Val_acc: 73.94, best_e: 422\n",
      "Epoch: 425/500, Loss: 0.545, Tr_acc: 73.31, Val_acc: 73.94, best_e: 422\n",
      "Epoch: 426/500, Loss: 0.546, Tr_acc: 73.25, Val_acc: 73.94, best_e: 422\n",
      "Epoch: 427/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 73.64, best_e: 422\n",
      "Epoch: 428/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 73.94, best_e: 422\n",
      "Epoch: 429/500, Loss: 0.548, Tr_acc: 73.31, Val_acc: 73.94, best_e: 422\n",
      "Epoch: 430/500, Loss: 0.548, Tr_acc: 73.31, Val_acc: 73.03, best_e: 422\n",
      "Epoch: 431/500, Loss: 0.546, Tr_acc: 73.12, Val_acc: 73.94, best_e: 422\n",
      "Epoch: 432/500, Loss: 0.548, Tr_acc: 73.31, Val_acc: 74.55, best_e: 432\n",
      "Epoch: 433/500, Loss: 0.549, Tr_acc: 72.66, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 434/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 435/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 436/500, Loss: 0.548, Tr_acc: 72.53, Val_acc: 73.03, best_e: 432\n",
      "Epoch: 437/500, Loss: 0.547, Tr_acc: 73.12, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 438/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 73.03, best_e: 432\n",
      "Epoch: 439/500, Loss: 0.547, Tr_acc: 73.05, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 440/500, Loss: 0.546, Tr_acc: 73.31, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 441/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 73.33, best_e: 432\n",
      "Epoch: 442/500, Loss: 0.546, Tr_acc: 73.05, Val_acc: 73.03, best_e: 432\n",
      "Epoch: 443/500, Loss: 0.546, Tr_acc: 73.12, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 444/500, Loss: 0.546, Tr_acc: 73.31, Val_acc: 73.33, best_e: 432\n",
      "Epoch: 445/500, Loss: 0.546, Tr_acc: 73.05, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 446/500, Loss: 0.546, Tr_acc: 73.31, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 447/500, Loss: 0.546, Tr_acc: 73.31, Val_acc: 73.33, best_e: 432\n",
      "Epoch: 448/500, Loss: 0.545, Tr_acc: 73.05, Val_acc: 73.03, best_e: 432\n",
      "Epoch: 449/500, Loss: 0.546, Tr_acc: 73.12, Val_acc: 73.33, best_e: 432\n",
      "Epoch: 450/500, Loss: 0.545, Tr_acc: 73.12, Val_acc: 73.33, best_e: 432\n",
      "Epoch: 451/500, Loss: 0.545, Tr_acc: 73.05, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 452/500, Loss: 0.545, Tr_acc: 73.31, Val_acc: 73.33, best_e: 432\n",
      "Epoch: 453/500, Loss: 0.545, Tr_acc: 73.05, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 454/500, Loss: 0.545, Tr_acc: 73.31, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 455/500, Loss: 0.545, Tr_acc: 73.31, Val_acc: 73.33, best_e: 432\n",
      "Epoch: 456/500, Loss: 0.544, Tr_acc: 73.05, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 457/500, Loss: 0.544, Tr_acc: 73.31, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 458/500, Loss: 0.545, Tr_acc: 73.31, Val_acc: 73.33, best_e: 432\n",
      "Epoch: 459/500, Loss: 0.544, Tr_acc: 73.05, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 460/500, Loss: 0.544, Tr_acc: 73.31, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 461/500, Loss: 0.544, Tr_acc: 73.31, Val_acc: 73.33, best_e: 432\n",
      "Epoch: 462/500, Loss: 0.544, Tr_acc: 73.05, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 463/500, Loss: 0.544, Tr_acc: 73.31, Val_acc: 73.33, best_e: 432\n",
      "Epoch: 464/500, Loss: 0.545, Tr_acc: 73.12, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 465/500, Loss: 0.547, Tr_acc: 73.31, Val_acc: 74.24, best_e: 432\n",
      "Epoch: 466/500, Loss: 0.549, Tr_acc: 72.73, Val_acc: 73.33, best_e: 432\n",
      "Epoch: 467/500, Loss: 0.555, Tr_acc: 73.05, Val_acc: 74.24, best_e: 432\n",
      "Epoch: 468/500, Loss: 0.545, Tr_acc: 72.73, Val_acc: 74.24, best_e: 432\n",
      "Epoch: 469/500, Loss: 0.547, Tr_acc: 72.66, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 470/500, Loss: 0.551, Tr_acc: 73.31, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 471/500, Loss: 0.570, Tr_acc: 73.25, Val_acc: 73.94, best_e: 432\n",
      "Epoch: 472/500, Loss: 0.548, Tr_acc: 72.73, Val_acc: 74.55, best_e: 472\n",
      "Epoch: 473/500, Loss: 0.549, Tr_acc: 72.92, Val_acc: 74.85, best_e: 473\n",
      "Epoch: 474/500, Loss: 0.549, Tr_acc: 72.34, Val_acc: 74.55, best_e: 473\n",
      "Epoch: 475/500, Loss: 0.548, Tr_acc: 72.92, Val_acc: 74.55, best_e: 473\n",
      "Epoch: 476/500, Loss: 0.545, Tr_acc: 72.92, Val_acc: 73.94, best_e: 473\n",
      "Epoch: 477/500, Loss: 0.548, Tr_acc: 73.31, Val_acc: 73.33, best_e: 473\n",
      "Epoch: 478/500, Loss: 0.555, Tr_acc: 73.05, Val_acc: 73.94, best_e: 473\n",
      "Epoch: 479/500, Loss: 0.548, Tr_acc: 73.31, Val_acc: 68.18, best_e: 473\n",
      "Epoch: 480/500, Loss: 0.573, Tr_acc: 69.42, Val_acc: 74.24, best_e: 473\n",
      "Epoch: 481/500, Loss: 0.584, Tr_acc: 72.66, Val_acc: 73.33, best_e: 473\n",
      "Epoch: 482/500, Loss: 0.583, Tr_acc: 73.18, Val_acc: 73.94, best_e: 473\n",
      "Epoch: 483/500, Loss: 0.552, Tr_acc: 73.31, Val_acc: 67.58, best_e: 473\n",
      "Epoch: 484/500, Loss: 0.597, Tr_acc: 67.40, Val_acc: 70.91, best_e: 473\n",
      "Epoch: 485/500, Loss: 0.580, Tr_acc: 71.23, Val_acc: 72.12, best_e: 473\n",
      "Epoch: 486/500, Loss: 0.620, Tr_acc: 72.40, Val_acc: 73.64, best_e: 473\n",
      "Epoch: 487/500, Loss: 0.590, Tr_acc: 72.60, Val_acc: 68.79, best_e: 473\n",
      "Epoch: 488/500, Loss: 0.619, Tr_acc: 68.44, Val_acc: 73.33, best_e: 473\n",
      "Epoch: 489/500, Loss: 0.598, Tr_acc: 71.04, Val_acc: 73.33, best_e: 473\n",
      "Epoch: 490/500, Loss: 0.596, Tr_acc: 72.73, Val_acc: 72.73, best_e: 473\n",
      "Epoch: 491/500, Loss: 0.573, Tr_acc: 71.10, Val_acc: 70.91, best_e: 473\n",
      "Epoch: 492/500, Loss: 0.589, Tr_acc: 71.30, Val_acc: 70.30, best_e: 473\n",
      "Epoch: 493/500, Loss: 0.577, Tr_acc: 70.65, Val_acc: 72.73, best_e: 473\n",
      "Epoch: 494/500, Loss: 0.559, Tr_acc: 72.47, Val_acc: 72.42, best_e: 473\n",
      "Epoch: 495/500, Loss: 0.574, Tr_acc: 72.86, Val_acc: 73.03, best_e: 473\n",
      "Epoch: 496/500, Loss: 0.561, Tr_acc: 73.05, Val_acc: 73.94, best_e: 473\n",
      "Epoch: 497/500, Loss: 0.558, Tr_acc: 73.25, Val_acc: 70.30, best_e: 473\n",
      "Epoch: 498/500, Loss: 0.563, Tr_acc: 70.58, Val_acc: 71.82, best_e: 473\n",
      "Epoch: 499/500, Loss: 0.558, Tr_acc: 72.08, Val_acc: 73.33, best_e: 473\n",
      "Epoch: 500/500, Loss: 0.553, Tr_acc: 73.12, Val_acc: 73.64, best_e: 473\n",
      "Test accuracy: 68.23262704308432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdum/PycharmProjects/URENA_MELENDEZ_PL/logic_explained_networks/lens/models/base.py:348: UserWarning: Loaded model does not have time or explanations. They need to be recalculated but time will only consider rule extraction time.\n",
      "  warnings.warn(\"Loaded model does not have time or explanations. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500, Loss: 3.121, Tr_acc: 4.55, Val_acc: 13.33, best_e: 1\n",
      "Epoch: 2/500, Loss: 3.990, Tr_acc: 14.55, Val_acc: 12.12, best_e: 1\n",
      "Epoch: 3/500, Loss: 4.555, Tr_acc: 12.53, Val_acc: 30.61, best_e: 3\n",
      "Epoch: 4/500, Loss: 2.768, Tr_acc: 32.53, Val_acc: 39.39, best_e: 4\n",
      "Epoch: 5/500, Loss: 2.216, Tr_acc: 39.16, Val_acc: 36.97, best_e: 4\n",
      "Epoch: 6/500, Loss: 2.088, Tr_acc: 38.77, Val_acc: 45.76, best_e: 6\n",
      "Epoch: 7/500, Loss: 1.808, Tr_acc: 46.82, Val_acc: 52.73, best_e: 7\n",
      "Epoch: 8/500, Loss: 1.720, Tr_acc: 53.51, Val_acc: 51.21, best_e: 7\n",
      "Epoch: 9/500, Loss: 1.659, Tr_acc: 51.75, Val_acc: 52.42, best_e: 7\n",
      "Epoch: 10/500, Loss: 1.581, Tr_acc: 55.39, Val_acc: 56.06, best_e: 10\n",
      "Epoch: 11/500, Loss: 1.446, Tr_acc: 58.44, Val_acc: 59.70, best_e: 11\n",
      "Epoch: 12/500, Loss: 1.366, Tr_acc: 61.30, Val_acc: 61.21, best_e: 12\n",
      "Epoch: 13/500, Loss: 1.280, Tr_acc: 62.34, Val_acc: 61.52, best_e: 13\n",
      "Epoch: 14/500, Loss: 1.182, Tr_acc: 62.92, Val_acc: 66.06, best_e: 14\n",
      "Epoch: 15/500, Loss: 1.099, Tr_acc: 67.21, Val_acc: 66.67, best_e: 15\n",
      "Epoch: 16/500, Loss: 1.062, Tr_acc: 66.69, Val_acc: 68.18, best_e: 16\n",
      "Epoch: 17/500, Loss: 1.030, Tr_acc: 68.05, Val_acc: 67.88, best_e: 16\n",
      "Epoch: 18/500, Loss: 0.978, Tr_acc: 66.88, Val_acc: 68.48, best_e: 18\n",
      "Epoch: 19/500, Loss: 0.928, Tr_acc: 68.12, Val_acc: 69.70, best_e: 19\n",
      "Epoch: 20/500, Loss: 0.901, Tr_acc: 67.40, Val_acc: 70.91, best_e: 20\n",
      "Epoch: 21/500, Loss: 0.871, Tr_acc: 68.31, Val_acc: 68.48, best_e: 20\n",
      "Epoch: 22/500, Loss: 0.867, Tr_acc: 66.75, Val_acc: 67.88, best_e: 20\n",
      "Epoch: 23/500, Loss: 0.873, Tr_acc: 66.49, Val_acc: 69.09, best_e: 20\n",
      "Epoch: 24/500, Loss: 0.854, Tr_acc: 67.60, Val_acc: 68.18, best_e: 20\n",
      "Epoch: 25/500, Loss: 0.848, Tr_acc: 67.34, Val_acc: 72.42, best_e: 25\n",
      "Epoch: 26/500, Loss: 0.829, Tr_acc: 70.45, Val_acc: 72.42, best_e: 26\n",
      "Epoch: 27/500, Loss: 0.817, Tr_acc: 70.71, Val_acc: 71.21, best_e: 26\n",
      "Epoch: 28/500, Loss: 0.808, Tr_acc: 70.06, Val_acc: 73.03, best_e: 28\n",
      "Epoch: 29/500, Loss: 0.801, Tr_acc: 70.97, Val_acc: 73.03, best_e: 29\n",
      "Epoch: 30/500, Loss: 0.799, Tr_acc: 72.27, Val_acc: 73.03, best_e: 30\n",
      "Epoch: 31/500, Loss: 0.789, Tr_acc: 72.01, Val_acc: 73.64, best_e: 31\n",
      "Epoch: 32/500, Loss: 0.781, Tr_acc: 70.91, Val_acc: 73.33, best_e: 31\n",
      "Epoch: 33/500, Loss: 0.778, Tr_acc: 71.49, Val_acc: 73.94, best_e: 33\n",
      "Epoch: 34/500, Loss: 0.769, Tr_acc: 71.56, Val_acc: 73.03, best_e: 33\n",
      "Epoch: 35/500, Loss: 0.766, Tr_acc: 71.75, Val_acc: 74.85, best_e: 35\n",
      "Epoch: 36/500, Loss: 0.763, Tr_acc: 72.73, Val_acc: 71.52, best_e: 35\n",
      "Epoch: 37/500, Loss: 0.761, Tr_acc: 71.49, Val_acc: 72.12, best_e: 35\n",
      "Epoch: 38/500, Loss: 0.753, Tr_acc: 71.88, Val_acc: 72.73, best_e: 35\n",
      "Epoch: 39/500, Loss: 0.749, Tr_acc: 72.08, Val_acc: 73.33, best_e: 35\n",
      "Epoch: 40/500, Loss: 0.744, Tr_acc: 72.53, Val_acc: 73.94, best_e: 35\n",
      "Epoch: 41/500, Loss: 0.740, Tr_acc: 72.66, Val_acc: 73.64, best_e: 35\n",
      "Epoch: 42/500, Loss: 0.738, Tr_acc: 72.21, Val_acc: 74.85, best_e: 42\n",
      "Epoch: 43/500, Loss: 0.749, Tr_acc: 72.60, Val_acc: 72.73, best_e: 42\n",
      "Epoch: 44/500, Loss: 0.756, Tr_acc: 71.56, Val_acc: 73.33, best_e: 42\n",
      "Epoch: 45/500, Loss: 0.767, Tr_acc: 72.08, Val_acc: 72.73, best_e: 42\n",
      "Epoch: 46/500, Loss: 0.728, Tr_acc: 72.27, Val_acc: 72.12, best_e: 42\n",
      "Epoch: 47/500, Loss: 0.754, Tr_acc: 71.17, Val_acc: 71.82, best_e: 42\n",
      "Epoch: 48/500, Loss: 0.764, Tr_acc: 72.27, Val_acc: 73.64, best_e: 42\n",
      "Epoch: 49/500, Loss: 0.740, Tr_acc: 72.79, Val_acc: 74.24, best_e: 42\n",
      "Epoch: 50/500, Loss: 0.776, Tr_acc: 72.34, Val_acc: 70.61, best_e: 42\n",
      "Epoch: 51/500, Loss: 0.757, Tr_acc: 71.30, Val_acc: 73.33, best_e: 42\n",
      "Epoch: 52/500, Loss: 0.769, Tr_acc: 73.25, Val_acc: 72.73, best_e: 42\n",
      "Epoch: 53/500, Loss: 0.745, Tr_acc: 72.73, Val_acc: 70.61, best_e: 42\n",
      "Epoch: 54/500, Loss: 0.746, Tr_acc: 71.36, Val_acc: 72.73, best_e: 42\n",
      "Epoch: 55/500, Loss: 0.745, Tr_acc: 72.53, Val_acc: 74.24, best_e: 42\n",
      "Epoch: 56/500, Loss: 0.740, Tr_acc: 72.86, Val_acc: 70.30, best_e: 42\n",
      "Epoch: 57/500, Loss: 0.742, Tr_acc: 70.97, Val_acc: 70.61, best_e: 42\n",
      "Epoch: 58/500, Loss: 0.722, Tr_acc: 70.78, Val_acc: 71.52, best_e: 42\n",
      "Epoch: 59/500, Loss: 0.732, Tr_acc: 71.82, Val_acc: 73.33, best_e: 42\n",
      "Epoch: 60/500, Loss: 0.713, Tr_acc: 73.31, Val_acc: 72.73, best_e: 42\n",
      "Epoch: 61/500, Loss: 0.723, Tr_acc: 73.25, Val_acc: 70.30, best_e: 42\n",
      "Epoch: 62/500, Loss: 0.719, Tr_acc: 70.39, Val_acc: 72.42, best_e: 42\n",
      "Epoch: 63/500, Loss: 0.717, Tr_acc: 71.75, Val_acc: 74.24, best_e: 42\n",
      "Epoch: 64/500, Loss: 0.713, Tr_acc: 72.92, Val_acc: 73.03, best_e: 42\n",
      "Epoch: 65/500, Loss: 0.711, Tr_acc: 72.60, Val_acc: 70.91, best_e: 42\n",
      "Epoch: 66/500, Loss: 0.705, Tr_acc: 71.75, Val_acc: 72.73, best_e: 42\n",
      "Epoch: 67/500, Loss: 0.703, Tr_acc: 72.79, Val_acc: 73.64, best_e: 42\n",
      "Epoch: 68/500, Loss: 0.702, Tr_acc: 73.51, Val_acc: 73.94, best_e: 42\n",
      "Epoch: 69/500, Loss: 0.692, Tr_acc: 73.64, Val_acc: 73.03, best_e: 42\n",
      "Epoch: 70/500, Loss: 0.695, Tr_acc: 73.44, Val_acc: 73.94, best_e: 42\n",
      "Epoch: 71/500, Loss: 0.690, Tr_acc: 72.86, Val_acc: 74.24, best_e: 42\n",
      "Epoch: 72/500, Loss: 0.693, Tr_acc: 73.05, Val_acc: 74.55, best_e: 42\n",
      "Epoch: 73/500, Loss: 0.686, Tr_acc: 73.25, Val_acc: 73.64, best_e: 42\n",
      "Epoch: 74/500, Loss: 0.685, Tr_acc: 73.05, Val_acc: 74.24, best_e: 42\n",
      "Epoch: 75/500, Loss: 0.681, Tr_acc: 73.12, Val_acc: 74.24, best_e: 42\n",
      "Epoch: 76/500, Loss: 0.682, Tr_acc: 73.05, Val_acc: 74.24, best_e: 42\n",
      "Epoch: 77/500, Loss: 0.678, Tr_acc: 73.51, Val_acc: 74.24, best_e: 42\n",
      "Epoch: 78/500, Loss: 0.677, Tr_acc: 73.51, Val_acc: 73.64, best_e: 42\n",
      "Epoch: 79/500, Loss: 0.675, Tr_acc: 73.51, Val_acc: 73.03, best_e: 42\n",
      "Epoch: 80/500, Loss: 0.674, Tr_acc: 72.86, Val_acc: 74.24, best_e: 42\n",
      "Epoch: 81/500, Loss: 0.672, Tr_acc: 73.70, Val_acc: 74.24, best_e: 42\n",
      "Epoch: 82/500, Loss: 0.671, Tr_acc: 73.70, Val_acc: 74.24, best_e: 42\n",
      "Epoch: 83/500, Loss: 0.670, Tr_acc: 73.70, Val_acc: 73.64, best_e: 42\n",
      "Epoch: 84/500, Loss: 0.668, Tr_acc: 73.51, Val_acc: 73.94, best_e: 42\n",
      "Epoch: 85/500, Loss: 0.666, Tr_acc: 73.64, Val_acc: 73.94, best_e: 42\n",
      "Epoch: 86/500, Loss: 0.665, Tr_acc: 73.77, Val_acc: 73.64, best_e: 42\n",
      "Epoch: 87/500, Loss: 0.665, Tr_acc: 73.57, Val_acc: 73.33, best_e: 42\n",
      "Epoch: 88/500, Loss: 0.663, Tr_acc: 73.57, Val_acc: 73.94, best_e: 42\n",
      "Epoch: 89/500, Loss: 0.663, Tr_acc: 73.77, Val_acc: 74.24, best_e: 42\n",
      "Epoch: 90/500, Loss: 0.660, Tr_acc: 73.77, Val_acc: 73.64, best_e: 42\n",
      "Epoch: 91/500, Loss: 0.660, Tr_acc: 73.57, Val_acc: 73.64, best_e: 42\n",
      "Epoch: 92/500, Loss: 0.659, Tr_acc: 73.57, Val_acc: 74.55, best_e: 42\n",
      "Epoch: 93/500, Loss: 0.657, Tr_acc: 73.38, Val_acc: 74.85, best_e: 93\n",
      "Epoch: 94/500, Loss: 0.657, Tr_acc: 73.38, Val_acc: 73.64, best_e: 93\n",
      "Epoch: 95/500, Loss: 0.656, Tr_acc: 73.57, Val_acc: 73.94, best_e: 93\n",
      "Epoch: 96/500, Loss: 0.655, Tr_acc: 73.77, Val_acc: 73.33, best_e: 93\n",
      "Epoch: 97/500, Loss: 0.654, Tr_acc: 73.57, Val_acc: 73.64, best_e: 93\n",
      "Epoch: 98/500, Loss: 0.653, Tr_acc: 73.57, Val_acc: 74.24, best_e: 93\n",
      "Epoch: 99/500, Loss: 0.652, Tr_acc: 73.77, Val_acc: 73.94, best_e: 93\n",
      "Epoch: 100/500, Loss: 0.652, Tr_acc: 73.77, Val_acc: 73.33, best_e: 93\n",
      "Epoch: 101/500, Loss: 0.652, Tr_acc: 73.44, Val_acc: 73.94, best_e: 93\n",
      "Epoch: 102/500, Loss: 0.652, Tr_acc: 73.77, Val_acc: 73.94, best_e: 93\n",
      "Epoch: 103/500, Loss: 0.650, Tr_acc: 73.05, Val_acc: 73.33, best_e: 93\n",
      "Epoch: 104/500, Loss: 0.649, Tr_acc: 73.44, Val_acc: 73.33, best_e: 93\n",
      "Epoch: 105/500, Loss: 0.649, Tr_acc: 73.57, Val_acc: 74.55, best_e: 93\n",
      "Epoch: 106/500, Loss: 0.647, Tr_acc: 73.38, Val_acc: 73.33, best_e: 93\n",
      "Epoch: 107/500, Loss: 0.647, Tr_acc: 73.44, Val_acc: 73.64, best_e: 93\n",
      "Epoch: 108/500, Loss: 0.647, Tr_acc: 73.77, Val_acc: 73.94, best_e: 93\n",
      "Epoch: 109/500, Loss: 0.648, Tr_acc: 73.77, Val_acc: 72.42, best_e: 93\n",
      "Epoch: 110/500, Loss: 0.656, Tr_acc: 72.79, Val_acc: 71.82, best_e: 93\n",
      "Epoch: 111/500, Loss: 0.674, Tr_acc: 71.36, Val_acc: 71.21, best_e: 93\n",
      "Epoch: 112/500, Loss: 0.685, Tr_acc: 71.04, Val_acc: 73.33, best_e: 93\n",
      "Epoch: 113/500, Loss: 0.665, Tr_acc: 72.86, Val_acc: 73.94, best_e: 93\n",
      "Epoch: 114/500, Loss: 0.655, Tr_acc: 73.77, Val_acc: 70.00, best_e: 93\n",
      "Epoch: 115/500, Loss: 0.662, Tr_acc: 71.04, Val_acc: 72.73, best_e: 93\n",
      "Epoch: 116/500, Loss: 0.671, Tr_acc: 71.62, Val_acc: 73.94, best_e: 93\n",
      "Epoch: 117/500, Loss: 0.657, Tr_acc: 73.77, Val_acc: 73.03, best_e: 93\n",
      "Epoch: 118/500, Loss: 0.657, Tr_acc: 73.31, Val_acc: 73.03, best_e: 93\n",
      "Epoch: 119/500, Loss: 0.666, Tr_acc: 72.86, Val_acc: 75.15, best_e: 119\n",
      "Epoch: 120/500, Loss: 0.649, Tr_acc: 72.79, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 121/500, Loss: 0.654, Tr_acc: 72.99, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 122/500, Loss: 0.655, Tr_acc: 72.86, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 123/500, Loss: 0.643, Tr_acc: 73.77, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 124/500, Loss: 0.649, Tr_acc: 73.77, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 125/500, Loss: 0.646, Tr_acc: 72.27, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 126/500, Loss: 0.642, Tr_acc: 73.57, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 127/500, Loss: 0.649, Tr_acc: 73.31, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 128/500, Loss: 0.638, Tr_acc: 73.05, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 129/500, Loss: 0.643, Tr_acc: 73.18, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 130/500, Loss: 0.643, Tr_acc: 73.77, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 131/500, Loss: 0.642, Tr_acc: 73.77, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 132/500, Loss: 0.641, Tr_acc: 73.25, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 133/500, Loss: 0.638, Tr_acc: 72.99, Val_acc: 74.55, best_e: 119\n",
      "Epoch: 134/500, Loss: 0.637, Tr_acc: 73.38, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 135/500, Loss: 0.637, Tr_acc: 73.12, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 136/500, Loss: 0.634, Tr_acc: 73.57, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 137/500, Loss: 0.634, Tr_acc: 73.57, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 138/500, Loss: 0.633, Tr_acc: 73.38, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 139/500, Loss: 0.631, Tr_acc: 73.18, Val_acc: 74.55, best_e: 119\n",
      "Epoch: 140/500, Loss: 0.632, Tr_acc: 73.38, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 141/500, Loss: 0.629, Tr_acc: 73.70, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 142/500, Loss: 0.629, Tr_acc: 73.25, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 143/500, Loss: 0.629, Tr_acc: 73.77, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 144/500, Loss: 0.628, Tr_acc: 73.38, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 145/500, Loss: 0.627, Tr_acc: 72.79, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 146/500, Loss: 0.627, Tr_acc: 73.77, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 147/500, Loss: 0.625, Tr_acc: 73.77, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 148/500, Loss: 0.626, Tr_acc: 73.51, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 149/500, Loss: 0.625, Tr_acc: 73.77, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 150/500, Loss: 0.624, Tr_acc: 73.77, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 151/500, Loss: 0.624, Tr_acc: 73.51, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 152/500, Loss: 0.623, Tr_acc: 73.77, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 153/500, Loss: 0.623, Tr_acc: 73.77, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 154/500, Loss: 0.623, Tr_acc: 73.57, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 155/500, Loss: 0.621, Tr_acc: 73.77, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 156/500, Loss: 0.621, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 157/500, Loss: 0.621, Tr_acc: 73.57, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 158/500, Loss: 0.620, Tr_acc: 73.77, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 159/500, Loss: 0.620, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 160/500, Loss: 0.620, Tr_acc: 73.57, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 161/500, Loss: 0.619, Tr_acc: 73.77, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 162/500, Loss: 0.618, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 163/500, Loss: 0.618, Tr_acc: 73.57, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 164/500, Loss: 0.618, Tr_acc: 73.77, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 165/500, Loss: 0.617, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 166/500, Loss: 0.617, Tr_acc: 73.57, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 167/500, Loss: 0.617, Tr_acc: 73.77, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 168/500, Loss: 0.616, Tr_acc: 73.57, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 169/500, Loss: 0.616, Tr_acc: 73.44, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 170/500, Loss: 0.616, Tr_acc: 73.77, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 171/500, Loss: 0.616, Tr_acc: 73.57, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 172/500, Loss: 0.617, Tr_acc: 73.77, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 173/500, Loss: 0.618, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 174/500, Loss: 0.622, Tr_acc: 73.44, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 175/500, Loss: 0.621, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 176/500, Loss: 0.619, Tr_acc: 73.70, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 177/500, Loss: 0.618, Tr_acc: 72.86, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 178/500, Loss: 0.616, Tr_acc: 73.77, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 179/500, Loss: 0.613, Tr_acc: 73.70, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 180/500, Loss: 0.615, Tr_acc: 73.44, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 181/500, Loss: 0.617, Tr_acc: 73.77, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 182/500, Loss: 0.617, Tr_acc: 72.86, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 183/500, Loss: 0.619, Tr_acc: 73.77, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 184/500, Loss: 0.618, Tr_acc: 73.77, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 185/500, Loss: 0.613, Tr_acc: 73.70, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 186/500, Loss: 0.612, Tr_acc: 73.44, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 187/500, Loss: 0.615, Tr_acc: 73.77, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 188/500, Loss: 0.617, Tr_acc: 73.77, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 189/500, Loss: 0.616, Tr_acc: 73.05, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 190/500, Loss: 0.613, Tr_acc: 73.70, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 191/500, Loss: 0.612, Tr_acc: 73.44, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 192/500, Loss: 0.612, Tr_acc: 73.18, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 193/500, Loss: 0.614, Tr_acc: 73.70, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 194/500, Loss: 0.613, Tr_acc: 73.44, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 195/500, Loss: 0.610, Tr_acc: 73.51, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 196/500, Loss: 0.609, Tr_acc: 73.77, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 197/500, Loss: 0.611, Tr_acc: 73.25, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 198/500, Loss: 0.611, Tr_acc: 73.70, Val_acc: 74.55, best_e: 119\n",
      "Epoch: 199/500, Loss: 0.609, Tr_acc: 73.38, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 200/500, Loss: 0.608, Tr_acc: 73.57, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 201/500, Loss: 0.608, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 202/500, Loss: 0.609, Tr_acc: 73.57, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 203/500, Loss: 0.608, Tr_acc: 73.70, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 204/500, Loss: 0.607, Tr_acc: 73.51, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 205/500, Loss: 0.606, Tr_acc: 73.77, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 206/500, Loss: 0.606, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 207/500, Loss: 0.607, Tr_acc: 73.57, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 208/500, Loss: 0.606, Tr_acc: 73.70, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 209/500, Loss: 0.606, Tr_acc: 73.18, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 210/500, Loss: 0.605, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 211/500, Loss: 0.605, Tr_acc: 73.70, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 212/500, Loss: 0.606, Tr_acc: 73.57, Val_acc: 74.55, best_e: 119\n",
      "Epoch: 213/500, Loss: 0.608, Tr_acc: 73.38, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 214/500, Loss: 0.611, Tr_acc: 73.51, Val_acc: 74.55, best_e: 119\n",
      "Epoch: 215/500, Loss: 0.614, Tr_acc: 73.38, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 216/500, Loss: 0.611, Tr_acc: 73.57, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 217/500, Loss: 0.609, Tr_acc: 73.05, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 218/500, Loss: 0.605, Tr_acc: 73.57, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 219/500, Loss: 0.603, Tr_acc: 73.77, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 220/500, Loss: 0.603, Tr_acc: 73.77, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 221/500, Loss: 0.604, Tr_acc: 73.51, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 222/500, Loss: 0.606, Tr_acc: 73.05, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 223/500, Loss: 0.605, Tr_acc: 73.51, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 224/500, Loss: 0.604, Tr_acc: 73.44, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 225/500, Loss: 0.603, Tr_acc: 73.57, Val_acc: 74.55, best_e: 119\n",
      "Epoch: 226/500, Loss: 0.602, Tr_acc: 73.38, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 227/500, Loss: 0.601, Tr_acc: 73.77, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 228/500, Loss: 0.601, Tr_acc: 73.51, Val_acc: 74.55, best_e: 119\n",
      "Epoch: 229/500, Loss: 0.601, Tr_acc: 73.38, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 230/500, Loss: 0.601, Tr_acc: 73.57, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 231/500, Loss: 0.604, Tr_acc: 73.38, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 232/500, Loss: 0.612, Tr_acc: 73.57, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 233/500, Loss: 0.612, Tr_acc: 73.31, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 234/500, Loss: 0.612, Tr_acc: 72.40, Val_acc: 71.21, best_e: 119\n",
      "Epoch: 235/500, Loss: 0.614, Tr_acc: 71.62, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 236/500, Loss: 0.608, Tr_acc: 73.57, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 237/500, Loss: 0.606, Tr_acc: 72.73, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 238/500, Loss: 0.612, Tr_acc: 73.25, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 239/500, Loss: 0.607, Tr_acc: 73.70, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 240/500, Loss: 0.607, Tr_acc: 73.18, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 241/500, Loss: 0.604, Tr_acc: 73.44, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 242/500, Loss: 0.604, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 243/500, Loss: 0.603, Tr_acc: 73.57, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 244/500, Loss: 0.604, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 245/500, Loss: 0.603, Tr_acc: 73.57, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 246/500, Loss: 0.601, Tr_acc: 73.77, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 247/500, Loss: 0.601, Tr_acc: 73.70, Val_acc: 74.85, best_e: 119\n",
      "Epoch: 248/500, Loss: 0.602, Tr_acc: 73.38, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 249/500, Loss: 0.601, Tr_acc: 73.77, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 250/500, Loss: 0.601, Tr_acc: 73.25, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 251/500, Loss: 0.601, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 252/500, Loss: 0.599, Tr_acc: 73.57, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 253/500, Loss: 0.601, Tr_acc: 73.70, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 254/500, Loss: 0.599, Tr_acc: 73.57, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 255/500, Loss: 0.598, Tr_acc: 72.86, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 256/500, Loss: 0.599, Tr_acc: 73.51, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 257/500, Loss: 0.597, Tr_acc: 73.57, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 258/500, Loss: 0.596, Tr_acc: 73.57, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 259/500, Loss: 0.597, Tr_acc: 73.51, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 260/500, Loss: 0.596, Tr_acc: 73.25, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 261/500, Loss: 0.595, Tr_acc: 73.57, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 262/500, Loss: 0.595, Tr_acc: 73.57, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 263/500, Loss: 0.594, Tr_acc: 73.38, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 264/500, Loss: 0.595, Tr_acc: 73.57, Val_acc: 71.82, best_e: 119\n",
      "Epoch: 265/500, Loss: 0.597, Tr_acc: 73.05, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 266/500, Loss: 0.598, Tr_acc: 73.12, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 267/500, Loss: 0.600, Tr_acc: 73.38, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 268/500, Loss: 0.603, Tr_acc: 72.79, Val_acc: 71.82, best_e: 119\n",
      "Epoch: 269/500, Loss: 0.611, Tr_acc: 72.60, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 270/500, Loss: 0.616, Tr_acc: 72.92, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 271/500, Loss: 0.602, Tr_acc: 73.57, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 272/500, Loss: 0.596, Tr_acc: 73.57, Val_acc: 74.55, best_e: 119\n",
      "Epoch: 273/500, Loss: 0.602, Tr_acc: 73.25, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 274/500, Loss: 0.607, Tr_acc: 73.57, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 275/500, Loss: 0.603, Tr_acc: 73.05, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 276/500, Loss: 0.597, Tr_acc: 73.18, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 277/500, Loss: 0.600, Tr_acc: 73.51, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 278/500, Loss: 0.606, Tr_acc: 73.05, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 279/500, Loss: 0.598, Tr_acc: 73.57, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 280/500, Loss: 0.603, Tr_acc: 72.66, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 281/500, Loss: 0.608, Tr_acc: 72.86, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 282/500, Loss: 0.602, Tr_acc: 73.77, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 283/500, Loss: 0.598, Tr_acc: 73.05, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 284/500, Loss: 0.603, Tr_acc: 72.86, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 285/500, Loss: 0.601, Tr_acc: 73.77, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 286/500, Loss: 0.599, Tr_acc: 73.51, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 287/500, Loss: 0.605, Tr_acc: 72.92, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 288/500, Loss: 0.598, Tr_acc: 73.18, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 289/500, Loss: 0.600, Tr_acc: 73.57, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 290/500, Loss: 0.596, Tr_acc: 73.77, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 291/500, Loss: 0.598, Tr_acc: 72.14, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 292/500, Loss: 0.594, Tr_acc: 73.57, Val_acc: 74.55, best_e: 119\n",
      "Epoch: 293/500, Loss: 0.596, Tr_acc: 73.38, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 294/500, Loss: 0.593, Tr_acc: 73.57, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 295/500, Loss: 0.594, Tr_acc: 73.44, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 296/500, Loss: 0.591, Tr_acc: 73.77, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 297/500, Loss: 0.593, Tr_acc: 73.51, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 298/500, Loss: 0.594, Tr_acc: 73.44, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 299/500, Loss: 0.592, Tr_acc: 73.57, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 300/500, Loss: 0.592, Tr_acc: 73.18, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 301/500, Loss: 0.591, Tr_acc: 73.77, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 302/500, Loss: 0.592, Tr_acc: 73.57, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 303/500, Loss: 0.590, Tr_acc: 73.44, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 304/500, Loss: 0.590, Tr_acc: 73.70, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 305/500, Loss: 0.589, Tr_acc: 73.57, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 306/500, Loss: 0.589, Tr_acc: 73.44, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 307/500, Loss: 0.589, Tr_acc: 73.51, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 308/500, Loss: 0.589, Tr_acc: 73.77, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 309/500, Loss: 0.588, Tr_acc: 73.70, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 310/500, Loss: 0.590, Tr_acc: 73.57, Val_acc: 74.85, best_e: 119\n",
      "Epoch: 311/500, Loss: 0.590, Tr_acc: 73.38, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 312/500, Loss: 0.588, Tr_acc: 73.57, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 313/500, Loss: 0.588, Tr_acc: 73.77, Val_acc: 74.55, best_e: 119\n",
      "Epoch: 314/500, Loss: 0.590, Tr_acc: 73.38, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 315/500, Loss: 0.589, Tr_acc: 73.57, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 316/500, Loss: 0.589, Tr_acc: 73.38, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 317/500, Loss: 0.591, Tr_acc: 73.64, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 318/500, Loss: 0.591, Tr_acc: 73.57, Val_acc: 74.55, best_e: 119\n",
      "Epoch: 319/500, Loss: 0.592, Tr_acc: 72.53, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 320/500, Loss: 0.592, Tr_acc: 73.77, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 321/500, Loss: 0.589, Tr_acc: 73.51, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 322/500, Loss: 0.590, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 323/500, Loss: 0.591, Tr_acc: 73.57, Val_acc: 74.55, best_e: 119\n",
      "Epoch: 324/500, Loss: 0.591, Tr_acc: 73.18, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 325/500, Loss: 0.590, Tr_acc: 73.18, Val_acc: 74.85, best_e: 119\n",
      "Epoch: 326/500, Loss: 0.587, Tr_acc: 73.38, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 327/500, Loss: 0.588, Tr_acc: 73.57, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 328/500, Loss: 0.587, Tr_acc: 73.44, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 329/500, Loss: 0.587, Tr_acc: 73.57, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 330/500, Loss: 0.589, Tr_acc: 72.60, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 331/500, Loss: 0.587, Tr_acc: 73.57, Val_acc: 72.12, best_e: 119\n",
      "Epoch: 332/500, Loss: 0.587, Tr_acc: 73.38, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 333/500, Loss: 0.586, Tr_acc: 73.18, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 334/500, Loss: 0.587, Tr_acc: 73.57, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 335/500, Loss: 0.586, Tr_acc: 73.51, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 336/500, Loss: 0.587, Tr_acc: 73.57, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 337/500, Loss: 0.587, Tr_acc: 73.12, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 338/500, Loss: 0.587, Tr_acc: 73.57, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 339/500, Loss: 0.587, Tr_acc: 73.51, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 340/500, Loss: 0.590, Tr_acc: 73.18, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 341/500, Loss: 0.590, Tr_acc: 73.57, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 342/500, Loss: 0.587, Tr_acc: 73.18, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 343/500, Loss: 0.589, Tr_acc: 73.57, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 344/500, Loss: 0.588, Tr_acc: 73.38, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 345/500, Loss: 0.595, Tr_acc: 73.57, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 346/500, Loss: 0.596, Tr_acc: 71.56, Val_acc: 70.30, best_e: 119\n",
      "Epoch: 347/500, Loss: 0.600, Tr_acc: 71.62, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 348/500, Loss: 0.595, Tr_acc: 73.51, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 349/500, Loss: 0.592, Tr_acc: 71.88, Val_acc: 71.82, best_e: 119\n",
      "Epoch: 350/500, Loss: 0.593, Tr_acc: 72.60, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 351/500, Loss: 0.594, Tr_acc: 72.79, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 352/500, Loss: 0.603, Tr_acc: 72.73, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 353/500, Loss: 0.594, Tr_acc: 72.86, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 354/500, Loss: 0.596, Tr_acc: 73.44, Val_acc: 72.12, best_e: 119\n",
      "Epoch: 355/500, Loss: 0.598, Tr_acc: 73.05, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 356/500, Loss: 0.595, Tr_acc: 72.86, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 357/500, Loss: 0.590, Tr_acc: 73.57, Val_acc: 70.91, best_e: 119\n",
      "Epoch: 358/500, Loss: 0.599, Tr_acc: 71.95, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 359/500, Loss: 0.603, Tr_acc: 72.86, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 360/500, Loss: 0.596, Tr_acc: 73.57, Val_acc: 71.82, best_e: 119\n",
      "Epoch: 361/500, Loss: 0.595, Tr_acc: 72.27, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 362/500, Loss: 0.592, Tr_acc: 73.05, Val_acc: 74.55, best_e: 119\n",
      "Epoch: 363/500, Loss: 0.591, Tr_acc: 73.38, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 364/500, Loss: 0.594, Tr_acc: 72.66, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 365/500, Loss: 0.606, Tr_acc: 73.12, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 366/500, Loss: 0.600, Tr_acc: 72.86, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 367/500, Loss: 0.597, Tr_acc: 72.66, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 368/500, Loss: 0.596, Tr_acc: 73.51, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 369/500, Loss: 0.600, Tr_acc: 72.86, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 370/500, Loss: 0.590, Tr_acc: 73.57, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 371/500, Loss: 0.593, Tr_acc: 73.57, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 372/500, Loss: 0.593, Tr_acc: 72.79, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 373/500, Loss: 0.586, Tr_acc: 73.18, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 374/500, Loss: 0.589, Tr_acc: 73.57, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 375/500, Loss: 0.593, Tr_acc: 73.18, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 376/500, Loss: 0.590, Tr_acc: 73.57, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 377/500, Loss: 0.587, Tr_acc: 73.57, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 378/500, Loss: 0.591, Tr_acc: 72.79, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 379/500, Loss: 0.583, Tr_acc: 73.18, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 380/500, Loss: 0.588, Tr_acc: 73.57, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 381/500, Loss: 0.586, Tr_acc: 72.86, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 382/500, Loss: 0.586, Tr_acc: 73.57, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 383/500, Loss: 0.583, Tr_acc: 73.57, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 384/500, Loss: 0.587, Tr_acc: 73.51, Val_acc: 71.82, best_e: 119\n",
      "Epoch: 385/500, Loss: 0.594, Tr_acc: 71.23, Val_acc: 71.82, best_e: 119\n",
      "Epoch: 386/500, Loss: 0.611, Tr_acc: 72.14, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 387/500, Loss: 0.606, Tr_acc: 72.79, Val_acc: 69.70, best_e: 119\n",
      "Epoch: 388/500, Loss: 0.616, Tr_acc: 71.30, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 389/500, Loss: 0.593, Tr_acc: 73.70, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 390/500, Loss: 0.612, Tr_acc: 73.18, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 391/500, Loss: 0.593, Tr_acc: 73.05, Val_acc: 70.91, best_e: 119\n",
      "Epoch: 392/500, Loss: 0.601, Tr_acc: 71.75, Val_acc: 72.12, best_e: 119\n",
      "Epoch: 393/500, Loss: 0.597, Tr_acc: 73.05, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 394/500, Loss: 0.592, Tr_acc: 72.79, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 395/500, Loss: 0.593, Tr_acc: 73.51, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 396/500, Loss: 0.591, Tr_acc: 73.44, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 397/500, Loss: 0.592, Tr_acc: 72.99, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 398/500, Loss: 0.591, Tr_acc: 73.51, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 399/500, Loss: 0.591, Tr_acc: 73.77, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 400/500, Loss: 0.588, Tr_acc: 72.86, Val_acc: 72.12, best_e: 119\n",
      "Epoch: 401/500, Loss: 0.589, Tr_acc: 73.05, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 402/500, Loss: 0.590, Tr_acc: 73.38, Val_acc: 74.85, best_e: 119\n",
      "Epoch: 403/500, Loss: 0.591, Tr_acc: 73.38, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 404/500, Loss: 0.588, Tr_acc: 73.51, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 405/500, Loss: 0.589, Tr_acc: 73.51, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 406/500, Loss: 0.589, Tr_acc: 73.57, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 407/500, Loss: 0.585, Tr_acc: 73.57, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 408/500, Loss: 0.588, Tr_acc: 72.86, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 409/500, Loss: 0.589, Tr_acc: 73.25, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 410/500, Loss: 0.587, Tr_acc: 73.77, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 411/500, Loss: 0.587, Tr_acc: 73.57, Val_acc: 71.52, best_e: 119\n",
      "Epoch: 412/500, Loss: 0.591, Tr_acc: 72.53, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 413/500, Loss: 0.589, Tr_acc: 73.38, Val_acc: 74.85, best_e: 119\n",
      "Epoch: 414/500, Loss: 0.587, Tr_acc: 73.38, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 415/500, Loss: 0.589, Tr_acc: 72.60, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 416/500, Loss: 0.586, Tr_acc: 73.77, Val_acc: 74.85, best_e: 119\n",
      "Epoch: 417/500, Loss: 0.590, Tr_acc: 73.38, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 418/500, Loss: 0.585, Tr_acc: 73.25, Val_acc: 71.52, best_e: 119\n",
      "Epoch: 419/500, Loss: 0.589, Tr_acc: 72.40, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 420/500, Loss: 0.590, Tr_acc: 73.18, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 421/500, Loss: 0.584, Tr_acc: 73.25, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 422/500, Loss: 0.588, Tr_acc: 73.51, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 423/500, Loss: 0.590, Tr_acc: 73.05, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 424/500, Loss: 0.588, Tr_acc: 73.18, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 425/500, Loss: 0.587, Tr_acc: 73.57, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 426/500, Loss: 0.589, Tr_acc: 73.51, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 427/500, Loss: 0.581, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 428/500, Loss: 0.585, Tr_acc: 73.57, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 429/500, Loss: 0.582, Tr_acc: 73.57, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 430/500, Loss: 0.583, Tr_acc: 73.77, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 431/500, Loss: 0.581, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 432/500, Loss: 0.580, Tr_acc: 73.57, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 433/500, Loss: 0.582, Tr_acc: 73.57, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 434/500, Loss: 0.578, Tr_acc: 73.57, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 435/500, Loss: 0.580, Tr_acc: 73.57, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 436/500, Loss: 0.578, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 437/500, Loss: 0.579, Tr_acc: 73.57, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 438/500, Loss: 0.578, Tr_acc: 73.05, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 439/500, Loss: 0.578, Tr_acc: 73.77, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 440/500, Loss: 0.577, Tr_acc: 73.57, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 441/500, Loss: 0.577, Tr_acc: 73.44, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 442/500, Loss: 0.577, Tr_acc: 73.57, Val_acc: 74.85, best_e: 119\n",
      "Epoch: 443/500, Loss: 0.577, Tr_acc: 73.38, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 444/500, Loss: 0.576, Tr_acc: 73.44, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 445/500, Loss: 0.576, Tr_acc: 73.77, Val_acc: 74.85, best_e: 119\n",
      "Epoch: 446/500, Loss: 0.576, Tr_acc: 73.38, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 447/500, Loss: 0.576, Tr_acc: 73.57, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 448/500, Loss: 0.576, Tr_acc: 73.77, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 449/500, Loss: 0.576, Tr_acc: 73.57, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 450/500, Loss: 0.576, Tr_acc: 73.77, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 451/500, Loss: 0.576, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 452/500, Loss: 0.575, Tr_acc: 73.57, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 453/500, Loss: 0.575, Tr_acc: 73.38, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 454/500, Loss: 0.575, Tr_acc: 73.57, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 455/500, Loss: 0.575, Tr_acc: 73.57, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 456/500, Loss: 0.575, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 457/500, Loss: 0.575, Tr_acc: 73.57, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 458/500, Loss: 0.576, Tr_acc: 73.77, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 459/500, Loss: 0.577, Tr_acc: 73.51, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 460/500, Loss: 0.579, Tr_acc: 72.86, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 461/500, Loss: 0.581, Tr_acc: 73.57, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 462/500, Loss: 0.583, Tr_acc: 72.21, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 463/500, Loss: 0.585, Tr_acc: 73.57, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 464/500, Loss: 0.585, Tr_acc: 72.21, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 465/500, Loss: 0.582, Tr_acc: 73.57, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 466/500, Loss: 0.577, Tr_acc: 72.86, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 467/500, Loss: 0.574, Tr_acc: 73.57, Val_acc: 74.24, best_e: 119\n",
      "Epoch: 468/500, Loss: 0.574, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 469/500, Loss: 0.575, Tr_acc: 73.44, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 470/500, Loss: 0.576, Tr_acc: 73.57, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 471/500, Loss: 0.577, Tr_acc: 73.18, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 472/500, Loss: 0.575, Tr_acc: 73.57, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 473/500, Loss: 0.574, Tr_acc: 73.77, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 474/500, Loss: 0.573, Tr_acc: 73.77, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 475/500, Loss: 0.574, Tr_acc: 73.57, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 476/500, Loss: 0.575, Tr_acc: 73.05, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 477/500, Loss: 0.574, Tr_acc: 73.51, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 478/500, Loss: 0.574, Tr_acc: 73.44, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 479/500, Loss: 0.573, Tr_acc: 73.38, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 480/500, Loss: 0.573, Tr_acc: 73.57, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 481/500, Loss: 0.574, Tr_acc: 73.57, Val_acc: 72.73, best_e: 119\n",
      "Epoch: 482/500, Loss: 0.574, Tr_acc: 73.57, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 483/500, Loss: 0.575, Tr_acc: 73.25, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 484/500, Loss: 0.575, Tr_acc: 73.57, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 485/500, Loss: 0.575, Tr_acc: 73.38, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 486/500, Loss: 0.575, Tr_acc: 73.57, Val_acc: 71.82, best_e: 119\n",
      "Epoch: 487/500, Loss: 0.577, Tr_acc: 73.31, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 488/500, Loss: 0.579, Tr_acc: 73.25, Val_acc: 72.12, best_e: 119\n",
      "Epoch: 489/500, Loss: 0.579, Tr_acc: 73.31, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 490/500, Loss: 0.577, Tr_acc: 73.25, Val_acc: 72.12, best_e: 119\n",
      "Epoch: 491/500, Loss: 0.575, Tr_acc: 73.38, Val_acc: 73.03, best_e: 119\n",
      "Epoch: 492/500, Loss: 0.574, Tr_acc: 73.12, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 493/500, Loss: 0.575, Tr_acc: 73.38, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 494/500, Loss: 0.575, Tr_acc: 73.51, Val_acc: 73.33, best_e: 119\n",
      "Epoch: 495/500, Loss: 0.573, Tr_acc: 73.57, Val_acc: 73.94, best_e: 119\n",
      "Epoch: 496/500, Loss: 0.572, Tr_acc: 73.77, Val_acc: 73.64, best_e: 119\n",
      "Epoch: 497/500, Loss: 0.572, Tr_acc: 73.70, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 498/500, Loss: 0.573, Tr_acc: 73.38, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 499/500, Loss: 0.574, Tr_acc: 73.51, Val_acc: 72.42, best_e: 119\n",
      "Epoch: 500/500, Loss: 0.573, Tr_acc: 73.38, Val_acc: 73.03, best_e: 119\n",
      "Test accuracy: 68.23262704308432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdum/PycharmProjects/URENA_MELENDEZ_PL/logic_explained_networks/lens/models/base.py:348: UserWarning: Loaded model does not have time or explanations. They need to be recalculated but time will only consider rule extraction time.\n",
      "  warnings.warn(\"Loaded model does not have time or explanations. \"\n"
     ]
    }
   ],
   "source": [
    "train_len(dataset, target_class=22)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
